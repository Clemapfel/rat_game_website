<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title >crisp on Clemens Cords&#39; Homepage</title>
    <link>http://clemens-cords.com/categories/crisp/</link>
    <description>Recent content in crisp on Clemens Cords&#39; Homepage</description>
    <image>
      <title>crisp on Clemens Cords&#39; Homepage</title>
      <link>http://clemens-cords.com/categories/crisp/</link>
      <url>http://clemens-cords.com/rat_icon.png</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.68.3)</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 06 Mar 2022 19:49:27 UT</lastBuildDate>
    <atom:link href="http://clemens-cords.com/categories/crisp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning Aided Differentiating of Chicken from Duck Eggs using Simulated Data</title>
      <link>http://clemens-cords.com/post/crisp_deep_learning/</link>
      <pubDate>Tue, 05 Oct 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_deep_learning/</guid>
      <description>(This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.hpp&amp;gt;#include &amp;lt;classification/fully_connected_neural_networks.hpp&amp;gt;#include &amp;lt;classification/convolutional_neural_networks.hpp&amp;gt; // all of the above collected in: #include &amp;lt;classification.hpp&amp;gt;Table of Contents  Introduction 1.1 Features, Class Membership
 Bayes Statistical Classifier
2.1 Training the Bayes Classifier
2.2 Using the Bayes Classifier for Identification
 Neural Networks
3.1 Architecture
3.2 Weights and Biases
3.3 Creating/Loading a Network
3.4 Training the Network
3.5 Using the Network for Identification
  1. Introduction We learned how to extract part of an image into a segment, then compute unique descriptors that describe the selected region&amp;rsquo;s shape and texture. Now, it is finally time to use these descriptors to solve one of the most common image processing applications: classification.
To classify an object means to divide the population it is from into a number of classes {C_0, C_1, ..., C_m-1}, then assign a label to any one specific object. This label represents what class it belongs to. For the sake of simplicity, for the rest of this tutorial, we assume that an object can only belong to exactly one class. While this assumption is not necessary for any of crisps deep learning features, it will make things easier to follow later.
1.1 Features &amp;amp; Class Membership For one specific region or object, class membership is given as a m*1 matrix M:
   class index i M(i, 0)     0 {0, 1}   1 {0, 1}   &amp;hellip; &amp;hellip;   m-1 {0, 1}    Where if M(4, 0) == 1 then the object is classified into class C4. Because an object can only belong to one class at a time, M.sum() == 1 (as all values except for the single 1 are 0 and an object has to belong to at least one class).
In order to be able to classify an object we need to describe it somehow, we do this using a n*1 matrix called a feature vector. Each row of this matrix N corresponds to one feature:
   feature index N(i, 0)     0 [-1, 1]   1 [-1, 1]   &amp;hellip; &amp;hellip;   n-1 [-1, 1]    When classifying multiple objects from the same population, all objects in that population have to have a feature vector of the same size and meaning, that is for each object, each row means the same thing. All that changes is the actual values of the features. As we saw in the table above, it is necessary to normalize the value of the feature into the range [-1, 1]. In the literature, features are often normalized into [0, 1] but because this is a subset of [-1, 1] anyway, either approach works in crisp.
A training data set in crisp is a set of feature vectors and their corresponding, desired classifications. In practice, this set is represented by two matrices:
  FeatureMatrix_t is a n*x matrix where n the number of features and x the number of samples in the population. Each row is one feature, each column is one sample. All its values should be normalized into `[-1, 1]
  ClassificationMatrix_t is a m*x matrix where m is the number of classes and x the number of sample. Each row is one class, each column is one sample. All elements of a row should be either 1 or 0
  The number of columns, x, has to be identical for both matrices. If one object has its feature vector as the column of the feature matrix at position i, the iths column of the classification matrix corresponds to that same object.
It may be instructive to consider a practical example. Consider the following data set:
   sample class egg weight (g) yolk to white ratio     0 Chicken 52 31%   1 Chicken 58 34%   2 Chicken 67 32%   3 Chicken 62 37%   4 Chicken 59 32%   5 Chicken 65 29%         6 Duck 61 36%   7 Duck 63 35%   8 Duck 69 39%   9 Duck 60 38%   10 Duck 71 37%   11 Duck 75 34%    Here we have a set of 12 eggs from both chickens and ducks. Each egg sample has 3 data point: whether it was from a chicken or duck, it&amp;rsquo;s weight, and it&amp;rsquo;s yolk-to-white-ratio. We want to create a classifier can decide whether a new, unknown egg we hand it is from a chicken or duck.
Before we can use crisp for classification, we need to arrange the data in a way that crisp understands.
We have m = 2 classes and n = 2 features. We are trying to predict the &amp;ldquo;class&amp;rdquo; property of each egg, so we create a matrix with 2 rows, each column of the matrix has a single 1 depending on whether the sample was from a chicken or duck.
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck Each egg sample gets a 2x1 feature vector that is 1 in the first row if it is from a chicken, or 1 in the second row if it is from a duck.
We transform the feature values in a similar way. Each column corresponds to one sample (egg), we arbitrarily decide that weight will be in row 0, yolk-ratio in row 1. Recall that features need to be normalized, so we divide both feature values by 100:
auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yok ratio We call the first matrix class membership matrix and the second a feature matrix.
2. Bayes Statistical Classifier The simplest classifier in crisp is crisp::BayesClassifier. This class has two template parameters:
template&amp;lt;size_t FeatureN, size_t ClassN&amp;gt; class BayesClassifier {  FeatureN is the number of features (n in our notation) ClassN is the number of classes (m in our notation)  Both need to be available at compile time.
The classifier works by estimating the type of distribution, mean and variance from a given population. When we hand it a sample x to identify the classifier will return a score that is directly proportional to, in our chicken-or-duck example, the following probabilities:
p( x | chicken) // is our current egg x from a chicken p( x | duck) // is our current egg x from a duck The return values of the classifier are not actual probabilities (they can be outside the range [0, 1]), however the order and relative value of the actual probabilities are preserved. This means we can compare and quantify them, just like we would probabilities.
2.1 Training the Bayes Classifier To estimate the values needed for classification, the bayes classifier needs to observe the training set population. We retrieve the feature- and classification matrix from earlier, then call BayesClassifier::train:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk  auto bayes = BayesClassifier&amp;lt;2, 2&amp;gt;(); // 2 features, 2 classes bayes.train(feature, classes); That&amp;rsquo;s all, unlike deep learning procedures later in this section, the bayes classifier only needs to iterate through the training set once to achieve the maximum possible classification performance based on the evidence.
2.2 Identification using the Bayes Classifier Sticking to our chicken-or-duck example, let&amp;rsquo;s say we just came home and found 3 eggs around our hypothetical farm and we really want to know whether it was the chickens or the ducks that hid them. We measure their size and content:
   egg weight yolk ratio     0 51 29%   1 67 37%   2 70 38%    We arrange these three samples just like we did with our training datas feature matrix:
auto to_identify = Eigen::MatrixXf(2, 3); // 0 1 2 // egg to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio We can then classify them using:
auto result = bayes.identify(to_identify); std::cout &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl; 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 We get a bunch of numbers, let&amp;rsquo;s try to understand what they mean. Firstly, each column index corresponds to their respective egg sample:
// 0 1 2 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 The numbers are scores representing the relative probability that the sample in column i is from class j where j is the row index. In our training data set, we had chickens as j = 0 and ducks as j = 1, so:
// 0 1 2 0.667373 0.500235 0.0936609 // ~p(egg | chicken) 0.332627 0.499765 0.906339 // ~p(egg | duck) Where ~p(x | y) is a score that&amp;rsquo;s directly proportional to the probability p(x | y).
We decide which egg came from which species by simply comparing the probabilities, choosing the class membership with the higher score.
Technically we would assign egg 0 and 1 as chicken,3 as duck, however the result for egg 1 is so ambiguous that it is basically a coin toss. To quantify this ambiguity, we assign each result a confidence score. One of the simplest confidence score is the absolute distance between the values:
auto to_confidence = [](float a, float b) { assert(a &amp;gt;= 0 and a &amp;lt;= 1 and b &amp;gt;= 0 and b &amp;lt;= 1); return abs(a-b); }; Because our classification output is always in [0, 1], the higher the distance between the two probabilities, the more certain we can be that we did not make a classification error.
We summarize our results as such:
   egg ~p(chicken) ~p(duck) classification confidence     0 0.667 0.332 chicken 0.33 (medium)   1 0.5 0.499 chicken 0.001 (very low)   2 0.09 0.90 duck 0.84 (high)    3 Neural Networks While the Bayes Classifier is useful and can lead to quite decent results with very little computational cost, some classification processes require a more flexible approach. Since the 1970s, the state-of-the-art way of solving classification problems in a semi-supervised way has been neural networks. This tutorial will not cover how they work in detail, instead we will learn just enough fundamentals to understand how to use crisp::NeuralNetwork and how to interpret its results.
3.1 Architecture A neural network has l layers. Each layer L_i has a number of neurons #L_i.
 L_0 is called the input layer. This layer has the same number of neurons as the feature vector has rows: #L_0 = n L_l is called the output layer. This layer as the same number of neurons as the number of possible classes: #L_l = m L_i for i in {1, 2, ..., l-1} are called hidden layers  Where n the number of features, m the number of classes (c.f. Section 1).
We call the number of layers and the number of neurons in each layer a network&amp;rsquo;s architecture. We define the architecture in crisp using the template arguments of crisp::NeuralNetwork:
template&amp;lt;size_t... NeuronsPerLayer&amp;gt; class NeuralNetwork; auto nn = NeuralNetwork&amp;lt;n, #L_1, #L_2, ..., m&amp;gt;(); Where n, m, #L_i as defined above. For example:
// binary feature vector: n = 2 // two possible classes: m = 2 // one hidden layer with 3 neurons: #L_1 = 3 auto nn = NeuralNetwork&amp;lt;2, 3, 2&amp;gt;(); // 20 features, 2 hidden layers with 3 neurons each, 2 classes auto nn = NeuralNetwork&amp;lt;20, 3, 3, 2&amp;gt;(); // 2 features, 5 layers with 10 neurons each, 19 classes auto nn = NeuralNetwork&amp;lt;2, 10, 10, 10, 10, 10, 19&amp;gt;(); A network&amp;rsquo;s architecture governs its structure. For the same problem, different architectures can result in vastly different results. Furthermore, the computational complexity (in terms of runtime performance) will of course increase with the total number of neurons. In praxis choosing the correct architecture is somewhat of an art, while heuristics exist in the literature it is often important to experiment.
3.2 Weights and Biases Each neuron has a number of weights and one bias. The number of weights is equal to the number of neurons in the previous layer, for example if we&amp;rsquo;re in layer L_2, all our neurons will have #L_1 weights and a single bias. The weights and biases govern a neuron&amp;rsquo;s output, thus (along with architecture) they completely define a network&amp;rsquo;s performance in terms of classification.
3.3 Creating a Network We can construct a fresh, untrained network like so:
#include &amp;lt;classification/fully_connected_neural_network.hpp&amp;gt; // in main.cpp auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2&amp;gt;(); This initializes all weights to 1 &#43; eps and all biases to 0 &#43; eps, where eps ∈ [-0.25, &#43;0.25] (random, uniformly distributed). Once our network is trained, we will want to export it to the disk to avoid having to retrain it every time the application launches. We can convert a network into a string like so:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto exported = nn.as_string(); // save exported to file We can then simply save the string to a file or otherwise store it. At a later point, we can then load the neural network from the disk using:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto file = std::ifstream(/*...*/ &#43; &amp;#34;/crisp/docs/feature_classification/chicken_or_duck_nn_2_4_4_2.txt&amp;#34;); std::stringstream buffer; buffer &amp;lt;&amp;lt; file.rdbuf(); std::string str = buffer.str(); nn.from_string(str); Here we&amp;rsquo;re opening a file stream, moving its content into a buffer stringstream and then create a plain string from said stringstream. The neural network can then parse the string it generated on a previous occasion and load the weights and biases. This is somewhat clumsy owing to C&#43;&#43;s file interface so feel free to save the string generated using as_string() as you see fit.
Note that for loading to be successful, the exported network and the network we&amp;rsquo;re trying to import have to have identical architectures. Trying to import the exported string of a network with a different architecture will trigger an exception. It is therefore good practice to state the network&amp;rsquo;s architecture in the filename (as in the above example).
3.4 Training the Network Recall our egg example from section 1:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk We will use this data to train our network. Unlike the bayes classifier, neural networks will have to complete many training cycles. Each cycle of going through the entire training data set once is called an epoch.
We can conduct a single epoch like so:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(/*...*/); nn.train(features, classes); Neural Networks work in a way that is guaranteed to converge towards a set of optimal class boundaries (functions that divide the feature space into the given classes) with enough epochs. Here, &amp;ldquo;optimal&amp;rdquo; means, that we&amp;rsquo;re minimizing the mean square error. We can directly compute the error of the current network state like so:
nn.compute_mean_squared_error(feature, classes); Here, we are not computing the error between feature and classes, rather, the neural network generates a classification matrix from feature, then measures the error between this result and classes.
The errors for the first few epochs with our egg data and the architecture from the above example:
   epoch maximum MSE     1 2.42762   2 2.08903   3 0.778278   4 0.675251    We see that the error is already decreasing rapidly. Depending on the architecture and problem, the mean square error may actually increase at the beginning, but then slowly stabilize and go back down as more epochs are completed.
If we want to only train for a certain number of epochs, we use:
for (size_t n_epochs = 0; n_epochs &amp;lt; 10000; &#43;&#43;n_epochs) nn.train(features, classes); If we have no idea how many epochs are needed to get to an acceptable classification error, we can instead use NeuralNetwork::train_until. It has 3 arguments: the feature matrix, the classification matrix and a mean square error threshold. If the network observes the current classification error to be stable and below that threshold, the function exits and returns the number of epochs it took:
auto n_epochs = nn.train_until(features, classes, 0.15); std::cout &amp;lt;&amp;lt; n_epochs &amp;lt;&amp;lt; std::endl; The number of epochs is only useful for monitoring and can often be discarded. For our egg training data, it took 1059191 many epochs to get below the threshold of 0.15. This is quite a lot, one parameter that governs the convergence behavior is called the learning constant (often called μ or α). This is a value in [0, 1] that we can optionally hand to the network on construction or by setting it later using set_learning_constant:
float alpha = 0.001; auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(alpha); // or nn.set_learning_constant(alpha); Usually values will be between 0.0001 and 0.1, however just like the network&amp;rsquo;s architecture, finding a good learning constant will require a lot of experimentation. For badly chosen values, the network can even begin to diverge, meaning the error will increase.
Now that our network is trained, we should export it, so we don&amp;rsquo;t have to do another 1059191 epochs. The weights and biases for the above example with alpha = 0.001 were:
1 0 7.30575 -1.03484 2.03974 -5.31141 5.80577 1.88958 6.59601 4.74493 -0.485592 1.42384 1.80752 2.70784 5.16637 3.31374 4.56878 -0.904903 1.88335 1.32096 1.80904 0.728481 5.68054 -0.569291 2.96311 1.78811 2.837 -0.384504 1.49958 6.98229 0.633096 1.17761 0.2897 -1.09701 -1.95467 3.00161 3.54201 -0.634866 -1.76444 -2.18135 1.18332 -0.43307 1.34111 0.527556 The first number will always be 1, this is the single and only weight of the input layer. It is always followed by a 0 which is the bias of the input layer. All following number correspond to the weights and biases of the subsequent layers.
3.5 Using the Network for Classification Now that our network is well-trained and the mean squared error is low, we should compute its classification error. Unlike the MSE, the classification error is defined as the percentage of false-positive or false negatives when classifying a data set for which the actual correct classifications are known.
We first load the network into memory using the string we generated earlier, then we classify the entire feature matrix from the training set:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(); nn.from_string(/*...*/); std::cout &amp;lt;&amp;lt; classes &amp;lt;&amp;lt; std::endl; std::cout &amp;lt;&amp;lt; nn.identify(features) &amp;lt;&amp;lt; std::endl; 0.99638 0.993424 0.988063 0.856026 0.998413 0.999656 -0.227162 0.343421 0.000525594 -0.426658 -0.00993148 -0.00718939 -0.0112014 0.434531 0.715863 0.272235 -0.189113 0.0802148 0.891286 0.900159 0.924364 0.830156 0.925139 0.924666 Like with the bayes classifier, each of the rows correspond to one class (chicken or duck), each column corresponds to one sample (egg). We classify an egg as chicken if the value in row 0 is higher, as a duck if the value in row 1 is higher.
We get the following result, where E(x) is the expected value of x as designated in the training sets classification matrix M
   sample Chicken E(Chicken) Duck E(Duck) correct     0 0.99638 1 -0.0112014 0 yes   1 0.993424 1 0.434531 0 yes   2 0.988063 1 0.715863 0 yes   3 0.856026 1 0.272235 0 yes   4 0.998413 1 -0.189113 0 yes   5 0.999656 1 0.0802148 0 yes   6 -0.227162 0 0.891286 1 yes   7 0.343421 0 0.900159 1 yes   8 0.000525594 0 0.924364 1 yes   9 -0.426658 0 0.830156 1 yes   10 -0.00993148 0 0.925139 1 yes   11 -0.00718939 0 0.924666 1 yes    We see that the neural network was able to classify all samples correctly. The least confident sample from this population was egg 2, while the most confidently classified was egg 5, where confidence is defined as in section 2.
Now that we know our neural network works decently well, we can use it to identify the three unknown eggs:
auto to_identify = Eigen::MatrixXf(2, 3); to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio  auto results = nn.identify(to_identify); // 0 1 2  -0.021921 0.8932 -0.0119139 // chicken  0.926603 0.918492 0.925525 // duck Just like with bayes classifier, eggs 0 and 2 were identified confidently. Egg 1 was previously undetermined as the bayes classifier assigned both options the same score, our neural network however leans more towards duck, if only by a difference of 0.02. This result is still not very confident, but it&amp;rsquo;s at least possible to choose one or the other.
]]></content:encoded>
    </item>
    <item>
      <title>Describing a Regions Shape and Texture with Descriptors intended for Deep Learning Applications</title>
      <link>http://clemens-cords.com/post/crisp_feature_extraction/</link>
      <pubDate>Mon, 27 Sep 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_feature_extraction/</guid>
      <description>(This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region
 Region Boundary
2.1 Definition
2.2 Boundary Polygon
 Boundary Signature
3.1 Vertex Polygon
3.2 Slope Chain Code Signature
3.3 Radial Distance Signature
3.4 Complex Coordinate Signature
3.5 Farthest Point Signature
 Whole Region Descriptors
4.1 Area, Perimeter, Compactness
4.2 Centroid
4.3 Axis Aligned Bounding Box
4.4 Major and Minor Axis
4.5 Eccentricity
4.6 Circularity
4.7 Holes
4.8 N-ths Moment Invariant
 Texture Descriptors
5.1 Intensity Histogram
5.2 Maximum Intensity Response
5.3 Mean, Variance
5.4 N-ths Pearson Standardized, Centralized Moment
5.5 Average Entropy
5.6 Co-Occurrence Matrix
5.7 Intensity Correlation
5.8 Homogeneity
5.9 Directed Entropy
5.10 Contrast
  1. Introduction In the tutorial on segmentation, we discussed how to extract part of an image. Now, we will find out what to actually do with said segment. Recall that, in crisp, an image segment is a set of pixel coordinates Vector2ui:
using ImageSegment = std::set&amp;lt;Vector2ui, /*...*/&amp;gt;; crisp::ImageSegment holds no information about anything other than the pixels coordinates. This allows for a certain degree of generality, we can use a segment as if it were part of many images. The only restraint on those images is, that they have to be at least the size of the segment.
We can apply any function only to a segment of an image, like so:
auto lambda_operator = []&amp;lt;typename Image_t&amp;gt;(size_t x, size_t y, const Image_t&amp;amp; image) -&amp;gt; typename Image_t::Value_t { auto&amp;amp; value = image(x, y); // some transformation  return value; }; ImageSegment segment = /*...*/; auto image = /*...*/; for (const auto position : segment) image(position.x(), position.y()) = lambda_operator(position.x(), position.y(), image); Here, we first define a templated lambda. This function takes the pixel coordinate and the corresponding image as arguments, reads the pixel&amp;rsquo;s value, transforms it in some way, then assigns it back to the image.
While this works well when the entire image is available, sometimes we really don&amp;rsquo;t need the rest of the image. For just this purpose, crisp defines crisp::ImageRegion:
template&amp;lt;typename Image_t&amp;gt; class ImageRegion { using Value_t = Image_t::Value_t; public: /* ... */ private: struct Element { Vector2ui _position; Value_t _value; float _intensity } std::set&amp;lt;Element, /*...*/&amp;gt; _elements; } We see that instead of just pixel coordinates, ImageSegment holds a set of ÌmageSegment::Elements. Each element has 3 members: the original pixel coordinate _position, the original value of the corresponding pixel in the image _value, and _intensity which is the mean of all planes of _value. We will use _intensity extensively in the texture descriptor chapter, but for now, it&amp;rsquo;s enough to remember that ImageSegment holds only pixel coordinates while ImageRegion holds those coordinates as well as deep-copies of the pixels values.
We construct an ImageRegion from an image and an ImageSegment like so:
// in main auto image = /*...*/; // crisp::Image&amp;lt;T, N&amp;gt; for any T, N auto segment = // some segmentation algorithm that returns crisp::ImageSegment  auto region = ImageRegion(); region.create_from(segment, image); // or equivalently: auto region = ImageRegion(segment, image); Once create_from is called, all pixel intensities are copied, so we are free to deallocate the original image or change it without the values in the image region being affected. If we were to change the pixel values in the region while the original image is still in memory, it would be unaffected.
crisp offers a shortcut to convert an entire image into one big region:
auto region = ImageRegion(image); // or auto region = ImageRegion(); region.create_from(image); This comes in handy whenever we want functionality only available to ImageRegion but have no need to first segment the image.
1.1 An Example To better illustrate the entire process of loading an image, segmenting it, then extracting a region, consider this image of the &amp;ldquo;pepper brush&amp;rdquo; as provided by gimp Wanting to extract the region that has the pepper, we observe the background to be very dark. A simple manual thresholding operation is guaranteed to extract the correct region. We first load the image as color, then convert it to grayscale using Image::get_value_plane(size_t) (remember that the HSV &amp;ldquo;value&amp;rdquo; component is equal to the mean of all RGB color components):
#include &amp;lt;system/image_io.hpp&amp;gt;#include &amp;lt;image/grayscale_image.hpp&amp;gt;#include &amp;lt;segmentation.hpp&amp;gt;using namespace crisp; // in main.cpp auto as_color = load_color_image(/*...*/ &#43; &amp;#34;/crisp/docs/feature_extraction/pepper.png&amp;#34;); auto as_grayscale = as_color.get_value_plane(); auto thresholded = Segmentation::manual_threshold(as_grayscale, 0.01f); We then want to decompose the binary image into connected segments. After decomposition, the segments will be ordered according to their respective left-most, top-most pixels coordinate. The pixel at (0, 0) is black and there are only two segments (the pepper in white, and the background in black) thus we expect the pepper to be the second segment extracted:
auto segments = decomponse_into_connected_segments(thresholded); auto pepper_segment = segments.at(1); We can now construct the resulting region using the pepper segment and the original color images values:
auto pepper = ImageRegion(pepper_segment, image); This is why we loaded the image as color but thresholded the grayscale image. Constructing the region with the colored pixel values means no information is lost.
2. Region Boundary Mathematically, crisps regions are closed, simply connected regions. This basically means:
 all boundary elements are part of the region all elements are 4-connected  ImageRegion will throw an exception if the segment handed to it is not 4-connected. We can assure it is either by using decompose_into_connected_segments (for more information about this, visit the segmentation tutorial) or we can use decompose_into_regions(const ImageSegment&amp;amp;, const Image_t&amp;amp;) -&amp;gt; std::vector&amp;lt;ImageRegion&amp;gt; which is provided to automatically split a segment into 4-connected sub-segments and then constructs a region from each.
Now that we assured that our region is indeed 4-connected, we can concern ourselves with, in terms of feature recognition, one of the most important properties of a region: it&amp;rsquo;s boundary. In crisp boundaries have the following properties:
(a visual, less math-y exploration of these concepts will follow)
Let B = {b_0, b_1, b_2, ..., b_m} be the set of boundary points, then:
 i) for each b_i there exists a b_i-1, b_i&#43;1 in B such that b_i-1 is 8-connected to b_i, b_i&#43;1 is 8-connected to b_i and b_i-1 is not 8-connected to b_i&#43;1 ii) b_0 is 8-connected to b_m iii) the set B is minimal in terms of cardinality, that is if we were to remove any b_i in B, property i) or ii) would be violated  Because definitions can be hard to conceptualize, let&amp;rsquo;s consider a purely visual example to illustrate these concepts:
2.1 8-Connectivity and Minimal Cardinality Property i) and ii) mean the boundary is an unbroken chain of 8-connected pixels and that the boundary forms a path such that one can jump from b_0 to b_1, b_1 to b_2, etc. up to b_m-1 to b_m (the last point) and then, crucially, from b_m back to b_0 completing the circle.
Let&amp;rsquo;s again consider the region of our pepper:
A simple way of tracing its boundary would be to highlight all pixels that have at least one neighbor that is not in the region (black, in our case).
This is a boundary that fulfills condition i) and ii), however inspecting the boundary closely we notice many redundant points:
If we were to ask a human to remove as many points as possible without compromising conditions i), ii), we would get the following boundary (where necessary pixels are highlighted in magenta, redundant pixels in cyan)
This is what condition iii)s minimality represents, we want all pixels to be non-redundant. This vastly increases performance, as for our pepper example we go from 3854 pixels for our trivial boundary to only 472 pixels using a minimal boundary:
crisps proprietary boundary tracing algorithm assures that the computed boundary is always minimal. After creating the region, we can access it at any point (with no performance overhead) using get_boundary(). The pixels are ordered according to condition i) where the first pixel b_0 is the left-most, top-most pixel and any following pixels b_i : i &amp;gt; 0 are enumerated in counter clock-wise direction.
2.2 Boundary Polygon We can even further reduce the number of elements in the boundary by treating it as a polygon that has vertices and straight, non-intersecting lines connecting exactly two of the vertices in order of enumeration. Consider this part of our pepper boundary:
We note multiple straight lines, each of these lines can be represented by just two pixels at the start and beginning of the line, shown in green rgb(0, 1, 0) here:
Using this approach, we reduce the number of boundary points from 472 to only 193, without loosing any information. The information is retained by the fact that the polygon vertices are ordered in counter-clockwise direction, this way we know exactly where to draw the straight line to the next point if we wanted to reconstruct the full boundary.
Now that we reduced the entire information contained in the region in the shape of a pepper to just 193 pixels, we may think we are done but thanks to more math we can reduce it even further, while increasing the representations&amp;rsquo; generality.
3. Boundary Signatures A signature is a mathematical transform of the boundary points of a shape that aims to, in some way, make the description of the boundary more widely applicable. When referring to the signature in this section, we are referring to the transform of the boundary polygons vertex points, as these are the smallest set of points that still represent the region boundary with no loss of information.
One of the ways to make a signature applicable to more than just one image is making it independent of rotation, a signature that accomplishes this describes not only our upright pepper but all possible rotation of it at the same time. Another form of generality is scale invariance, meaning that the signature describes our pepper at scale 1 and the same pepper scaled by any factor &amp;gt; 0. Lastly, independence of translation means that it does not matter if we were to translate all points of the signature by a constant (x, y), the signature represents all of those peppers just the same. crisp offers a multitude of signatures that may or may not be invariant in multiple of the aspects described above.
3.1 Vertex Polygon          scale not invariant   rotation not invariant   translation not invariant    This is the simplest signature, as already mentioned, it reduces the boundary to the vertices of it&amp;rsquo;s polygon. It is neither scale, translationally, nor rotationally invariant, it is however the basis for all other signatures. We can access it using:
auto polygon_signature = pepper.get_boundary_polygon(); 3.2 Slope Chain Code Signature          scale invariant   rotation not invariant   translation invariant    We can generate the slope-chain-signature by iterating through all boundary polygon vertices, storing the angle of the line that connects our current polygon vertex to the next (recall that the vertices are ordered counter-clockwise). This makes it invariant to both translation and scale, however rotation would alter all the angles values, so it is not invariant in this aspect. One way to make it rotationally invariant is to instead save the delta of successive angles. crisps slope_chain_code_signature() does not do this automatically, it is however a trivial operation.
The angles are stored in radians in the same order as their vertices.
3.3 Radial Distance Signature          scale not invariant   rotation invariant   translation invariant    The radial distance signature is the distance of each vertex from the region&amp;rsquo;s centroid. The centroid of a region in crisp is defined as the mean of all boundary coordinates, and can be intuitively thought of as the center of mass of a hole-less region if all pixels have the same weight. Because we are measuring the absolute distance, this signature is not invariant to scale. We can generate it using std::vector&amp;lt;float&amp;gt; ImageRegin::radial_distance_signature() const.
3.4 Complex Coordinate Signature          scale invariant   rotation invariant   translation invariant    (as proposed by El-Ghazal, Basir, Belkasim (2007))
This signature transforms each point in the boundary polygon into a complex number, for a point (x, y) the signature of the point is the complex number x &#43; i*y where i is the imaginary constant. The x-coordinate is treated as the real part, the y-coordinate is treated as the imaginary part. While this signature itself is neither invariant to scale nor rotation, we can now fourier-transform the complex numbers and store the corresponding coefficients. This achieves scale, rotational and translational invariance.
We can access the raw complex coordinates using ImageRegion::complex_coordinate_signature(). We can then use crisp::FourierTransform to transform them into their fourier descriptors.
3.5 Farthest Point Signature          scale not invariant   rotation invariant   translation invariant    (as proposed also by El-Ghazal, Basir, Belkasim (2009))
This signature computes, for each boundary point, the maximum distance to any other boundary point. When used for fourier descriptors, it performs better than other boundaries mentioned here [1] and achieves translational and rotational invariance even before fourier transformation. We can generate it using ImageRegion::farthest_point_signature.
[1] (Y. Hu, Z. Li, (2013): available here
4. Whole Region Descriptors Signatures are a transform of a region&amp;rsquo;s boundary points (the vertices of its polygon, to be precise). This is useful in unique representing a regions&amp;rsquo; boundary, but it doesn&amp;rsquo;t describe the shape of it in any way. To compare two boundaries, we would have to come up with a distance measure that compares the signatures, which can be quite hard. Instead, we can rely on the field of mathematical topology to give us many, much simpler to compute properties of a boundary. While only one of these will not unique identify a region&amp;rsquo;s shape, using multiple descriptors along with a signature can lead to great results.
4.1 Area &amp;amp; Perimeter, Compactness One of the easiest descriptors are area, the number of pixels in a region, and perimeter, the length of the region&amp;rsquo;s boundary. Perimeter only takes into account the outermost boundary, increasing the number of holes in a region will decrease its area but leave its perimeter unchanged.
Area and perimeter are usually not very useful unless they are normalized, for example by quantifying the area&amp;rsquo;s compactness, which is equal to the square of the perimeter divided by the area. A region that has no holes will have maximum compactness, while a region with the same boundary, but many or very big holes has a lower compactness. We can access area, perimeter and compactness using:
float get_perimeter() const; float get_area() const; float get_compactness() const; The values for compactness are usually in [0, 2], for edge cases the value may be outside this interval, however.
4.2 Centroid A region&amp;rsquo;s centroid, in crisp, is defined as the mean of the coordinate values of its boundary points (note that all boundary points are weighted here, not just the boundary polygons vertices). In the literature, the centroid is sometimes defined as the average of all points in a region, so it is important to remember that crisp only uses the boundary: Adding holes to a region while leaving its boundary unchanged does not alter the position of the region&amp;rsquo;s centroid.
We can access a region&amp;rsquo;s centroid at any time using get_centroid().
Where the centroid is highlighted using a red rgb(1, 0, 0) cross in the above picture.
4.3 AABB The axis aligned bounding box (AABB) of a region is the smallest rectangle whose sides align with the x- and y-axis that completely encloses the region. We can access it like so:
std::vector&amp;lt;Vector2ui, 4&amp;gt; aabb = pepper.get_axis_aligned_bounding_box(); Where the resulting vertices of the box are in the following order: top-left, top-right, bottom-right, bottom-left.
The value of the vertices of the rectangle are relative to the top-left corner of the image the region is from, translating them to the origin is a trivial operation.
Where the AABB is shown in white. Note that the boundary intersects with the AABB, this is because regions in crisp are closed regions.
4.4 Major &amp;amp; Minor Axis The major and minor axis of a region are formally defined as the major- and minor axis of the ellipses described by the eigenvectors multiplied with their respective eigenvalues of the positional covariance matrix of the boundary of the region. It&amp;rsquo;s not important to understand what this means, we can think of the major and minor axis as the &amp;ldquo;orientation&amp;rdquo; of the data spread of a region shape, where the major axis is along the highest variance (in terms of spacial position), the minor axis is perpendicular to it and both axis&amp;rsquo; intersect with the centroid. We access the minor and major access using:
const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_major_axis() const; const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_minor_axis() const; Each of the axis is given as two point. Visualizing the axes properly is difficult, as they are represented with sub-pixel precision. By scaling the image of the pepper, we can get an idea of what they look like:
Where magenta is the major, green the minor axis and the ellipses modeled by them shown in yellow. We can translate the ellipses so it&amp;rsquo;s axes align with the coordinate systems axes. The ratio of the resulting axes gives us another scale-invariant, rotationally-invariant and translationally-invariant region descriptor.
The major- and minor axis are useful for many other things, for example aligning regions to each other or for registration.
4.5 Eccentricity Using the minor and major axis we can compute the region&amp;rsquo;s eccentricity, which quantifies how &amp;ldquo;conical&amp;rdquo; the region is. If the eccentricity is 0 the shape modeled by the major and minor axis is a perfect circle, the closer to 1 the eccentricity is, the more elliptical the region.
We can access the eccentricity using ImageRegion::get_eccentricity().
4.6 Circularity While eccentricity measures how closely the shape of a region approximates a non-circular ellipse, circularity quantifies how closely the shape approximates a perfect circle. Regions, which tend to have smooth features and not many sharp edges, tend to have high circularity towards 1, while angular shapes or shapes with a high eccentricity tend to have a circularity closer to 0.
We can access circularity using ImageRegion::get_circularity().
4.7 Holes While already mentioned, it may be instructional to define what a hole is formally. A hole is an area of pixels who are not part of the region, whose boundary is entirely enclosed by the region. Intuitively this means, if you image the region as land and everything else as water, a hole would be a lake inside the region with no connection to the &amp;ldquo;ocean&amp;rdquo; that is surrounding the region.
When boundary tracing, crisp implicitly computes the boundaries of each hole and, thus, also the number of holes. We can access either with no overhead using:
size_t get_n_holes() const; const std::vector&amp;lt;std::vector&amp;lt;Vector2ui&amp;gt;&amp;gt; get_hole_boundaries() const; Where the hole boundaries are ordered corresponding to their respective top-most, left-most pixel coordinate.
4.8 Moment Invariants We&amp;rsquo;ve seen earlier that somehow representing a region&amp;rsquo;s unique shape in a way that is invariant to scale, translation and rotation is highly valuable. A very powerful way to do this is using the n-ths moment invariant. While some of them have a conceptual meaning, for beginners it is best to just think of them as properties that may not be useful to humans but do represent the shape of a region uniquely. crisp offers the first 7 moment invariants, also known as Hu Moment Invariant (Hu, 1962), accessed using ImageRegion::get_nths_moment_invariant(size_t n) where n in {1, 2, 3, &amp;hellip;, 7}.
The following table summarizes the response of a moment invariant to translation, scale, rotation and mirroring. &amp;ldquo;Unchanged&amp;rdquo; means the value of the moment does not change when recomputed after the operation.
   N translation scale rotation mirroring     1 unchanged unchanged unchanged unchanged   2 unchanged unchanged unchanged unchanged   3 unchanged unchanged unchanged unchanged   4 unchanged unchanged unchanged unchanged   5 unchanged does change unchanged unchanged   6 unchanged does change unchanged does change   7 unchanged does change slight change changes sign    We note that all first 4 moment invariants are completely independent of translation, scale, rotation and mirroring of the region and are thus highly valuable in representing a region.
5. Texture Descriptors So far, our descriptors dealt with the region&amp;rsquo;s boundary, shape or the values taken directly from the original image. In this section, we will instead deal with the region&amp;rsquo;s texture. This construct has not agreed on definition, in crisp texture refers to the distribution of intensity values in the region. Where the intensity of a pixel is the mean over all planes of that pixel (stored as _intensitiy in ImageRegion::Element, if you recall). An easier way to express quantifying texture in crisp is, that we&amp;rsquo;re converting our region to grayscale, then construct a histogram using those grayscale value and use statistical techniques to describe the distribution modeled by the histogram.
5.1 Intensity Histogram To get a rough idea of what the distribution of a region&amp;rsquo;s intensity looks like, ImageRegion offers get_intensity_histogram(). In this histogram, the intensity values are quantized into 256 intensities. This is not the case internally, however, the loss of detail in the histogram is not representative of the values computed for texture descriptors.
auto hist = pepper.get_intensity_histogram(); auto hist_img = hist.as_image(); We note that the histogram has many blank spots, which means that not all intensities were represented. The histogram exhibits multiple large spikes around the 0.5 region, these correspond to many of the constant-colored regions of the pepper. Lastly, we note that a lot of intensities were seemingly only represented once as this would account for the long tail of the distribution, the left tail extends all the way to 0 while the right tail stops at about 0.6.
5.2 Maximum Response The maximum response is the probability of the intensity with the highest number of observations occurring. The closer to 1 this value is, the more likely is it that the region has only very few shades in intensity.
We access it using get_maximum_intensity_probability() which for the pepper region returns 0.57. This is relatively high, which makes sense, because most of the pepper is the same shade of green. The high probability is represented by the huge spike in the histogram.
5.3 Mean, Variance Two of the most basic descriptors of a data set (a set of intensities in our case) are mean and variance. We can access them using:
auto mean = pepper.get_mean(); auto stddev = sqrt(pepper.get_variance()); Our pepper region&amp;rsquo;s texture has a mean of 0.45 (which corresponds to the red line in the histogram) and a variance of 0.015, which is very low. This is expected as, again, most of the pepper is shades of green that do not vary greatly when converted to grayscale.
5.4 n-ths Pearson Standardized Moment around the Mean In statistics, a distribution of random variables can be quantified using statistical moments. Each moment has an order called n. The first four moments have a human interpretable meaning:
 the 0ths standardized moment is always 1 the 1st standardized moment is the mean difference between the mean and itself, which is always 0 the 2nd standardized moment is the mean ratio of the variance to itself, which is always 1 the 3rd standardized moment is called skewness, which is a measure of how much a distribution leans to one side the 4th standardized moment is called kurtosis, which is a measure of how long, which tail of the distribution is.  Higher order moments may not have immediate use in human interpretation, but can be very useful for machine-learning application. All moments for n &amp;lt; 7 are used commonly. We can access the n-ths moment using get_nths_moment(size_t n).
The 3rd and 4th moment can furthermore be accessed directly using get_skewness() and get_kurtosis(). To put these values into context, let&amp;rsquo;s again inspect our intensity histogram:
Because of the large number of singleton intensity occurrences towards 0, the distribution overall leans to the left. This is reflected in the skewness which is -2.20965. Negative values generally mean the distribution leans left, right for positive values. The kurtosis of the pepper&amp;rsquo;s texture is 8.74199 which is very high. This again was expected because both tails are very long and thin, resulting from the high number of singletons.
5.5 Average Entropy The average entropy is a measure of how ordered the set of intensities is. We can compute its value using get_average_entropy() which returns 0.769 for the pepper region. crisp normalizes the values into [0, 1] so ~0.77 is relatively high. This is expected, the pepper is mostly green and has large regions of constant intensity, so we would expect it to be highly ordered.
5.6 Co-Occurrence Matrix To quantify texture in a specified direction, we can construct what is called the co-occurrence matrix. The co-occurrence matrix is a matrix of size 256x256. It counts for each intensity pair i_a, i_b the number of times two pixels a, b that are next to each other in a specified direction have the corresponding intensities i_a, i_b. A short example: if the co-occurrence matrix for the &amp;ldquo;right&amp;rdquo; direction has a value of 6 in the row 120 and column 98 then in the image there are 6 pairs of pixels a, b such that a has intensity 120 and b who is directly right of a has intensity 98 Recall that the intensities are quantized and projected from [0, 1] (float) to [0, 256] (int) to keep the size of the co-occurrence matrix manageable.
When constructing the co-occurrence matrix, we need to first supply a direction. Let a = (x, y) and b = (x &#43; i, y &#43; j) then the values of the crisp::CoOccurrenceDirection enum have the following meaning:
Direction a b ------------------------------------- PLUS_MINUS_ZERO (x,y) (x, y-1) PLUS_45 (x,y) (x&#43;1, y-1) PLUS_90 (x,y) (x&#43;1, y) PLUS_125 (x,y) (x&#43;1, y&#43;1) PLUS_MINUS_180 (x,y) (x, y&#43;1) MINUS_125 (x,y) (x-1, y&#43;1) MINUS_90 (x,y) (x-1, y) MINUS_45 (x,y) (x-1, y-1) In crisp we can render the matrix like so:
auto co_occurrence_matrix = pepper.get_co_occurrence_matrix(CoOccurrenceDirection::PLUS_90); auto as_image = GrayScaleImage(co_occurrence_matrix); // bind or save to disk The above image was log-scaled for clarity. Firstly, we note that most of the cells are left black. This points to the pepper&amp;rsquo;s texture only having a relative small amount of pair-wise different intensity pairs, which, looking at the original image, is indeed the case. Secondly, we note that most of the high-occurrence pairs (those with a lighter pixel color in the above image) are clustered around the matrix&amp;rsquo; trace. This is evidence of high uniformity, as an occurrence along the trace means, that the intensity pair {i_a, i_b} that occurred had identical values i_a == i_b. Recall that these observations only make sense in respect to the direction of the co-occurrence matrix, which in our case is &#43;90° (from left to right).
We can numerically quantify the distribution of intensity value pair occurrences using the following descriptors:
5.7 Intensity Correlation Intensity Correlation measures how correlated the intensities in each intensity value pair are. Similar to the pearson correlation coefficient it has a value in [-1, 1] where -1 means a strong negative correlation, &#43;1 means a strong positive correlation (either of which usually mean there is a regularity to the pattern) and 0 means no correlation. A pattern that has 0 correlation could be random noise or of constant intensity.
We can compute the mean intensity correlation using get_intensity_correlation(CoOccurrenceDirection). In our example for the left-to-right (&#43;90°) direction, this returns 0.4868. This means the texture exhibits above average positive correlation in the left-to-right direction. Looking at the image of the pepper closely, we, note that the lighter spots coming from lighting are more present on the right side while the left side is more in the shadow. This explains the positive (increasing) intensity correlation when looking at the texture from left to right.
5.8 Homogeneity In the co-occurrence matrix, the diagonal represents occurrences of intensity pair i_a, i_b where i_a = i_b, occurrences where two pixels in the specified direction have the same intensity. Homogeneity quantifies, how many of the intensity pairs are located near this diagonal. The higher the homogeneity, the more common pixel neighbors with the same or similar intensity are.
We can access the value using get_homogeneity(CoOccurrenceDirection), it returns a float in [0,1]. The pepper region exhibits a homogeneity of 0.625. This may be slightly lower than expected considering the pepper is all green, however remember that we are quantifying texture, not color. The intensity (lightness) of the shades of green do vary quite a bit, even though the hue does not. Nonetheless, 0.625 would be considered far above average, so we would call the pepper a fairly homogenically textured region.
5.9 Entropy Similar to the average entropy, we can also compute the entropy of the co-occurrence matrix. This can be thought of as a descriptor of how ordered the occurrence-pair distribution in that direction is. Using get_entropy(CoOccurrenceDirection) we compute a value of 0.36 (again normalized into [0, 1]).
5.10 Contrast Contrast measures the difference in value between co-occurring pixels. A white pixel next to a black pixel (or vice-versa) would have maximum contrast, while two pixels of identical color would have 0 contrast. We can compute the mean contrast in the specified direction using get_contrast(CoOccurrenceDirection), as already mentioned, its values are in [0, 1].
Our pepper has a contrast of 0.0002 which is extremely low, again this is expected, the shades of green transition into each other smoothly, as there are no big jumps in intensity. Large parts of the pepper have constant regions where neighboring pixels have the same intensity.
]]></content:encoded>
    </item>
    <item>
      <title>[WIP] A fast, robust Boundary Tracing and Enumeration Algorithm</title>
      <link>http://clemens-cords.com/post/boundary_tracing_algorithm/</link>
      <pubDate>Thu, 29 Jul 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/boundary_tracing_algorithm/</guid>
      <description>Abstract Boundaries are central to many field in image processing and having a way to numerically represent them is the only way to build libraries of them for later feature detection. Here I present the algorithm used by crisp::ImageRegion that extracts from a region consisting of a set of 4 (8)-connected pixel coordinates all it&amp;rsquo;s outer boundary points and enumerates them in a deterministic manner. The region has to fullfill no further constraints.</description>
      <category domain="http://clemens-cords.com/categories/programming">Programming</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[Abstract Boundaries are central to many field in image processing and having a way to numerically represent them is the only way to build libraries of them for later feature detection. Here I present the algorithm used by crisp::ImageRegion that extracts from a region consisting of a set of 4 (8)-connected pixel coordinates all it&amp;rsquo;s outer boundary points and enumerates them in a deterministic manner. The region has to fullfill no further constraints. The algorithm runs in amortized o(x &#43; k) where x is the number of pixels in the region and k are the number of boundary pixels. The advantage of the method presented here is not only a robustness to common edge cases making user-input minimal but that both the freeman-chain codes and the minimum-vertex boundary polygon can be extracted at the end of the algorithm in o(1).
[(skip to source code)](#The Algorithm)
Introduction Boundary tracing it such an elemental task in image processing because boundaries are one of the best features to classify and many other features descriptors work as a function of boundary. Crisp for example uses the fourier transform of a boundary converted onto the complex plane (such that each vertices x position is the real, y position the complex part respectively) for boundary simplification and when I went to consult the literature I was frustrated that many of the common algorithms don&amp;rsquo;t fullfill the following two demands I somewhat arbitrarily set myself:
  i) the algorithm should work on all closed regions, no exceptions Common assumptions algorithms make are &amp;ldquo;let the region be simply connected&amp;rdquo; or &amp;ldquo;let the region boundary be non-overlapping&amp;rdquo;. I want crisps algorithm to be able to handle all of these with not further user interaction, indeed the only assertion crisp makes is that the region is closed and connected which for image-processing purposes is an assumption that&amp;rsquo;s made for all common segmentation algorithms anyway. If the region is not connected you can simply decompose them into connected segments, then the boundary of the original non-connected region is the union of the boundary of all segments
  ii) the resulting boundary should have the minimum number of points necessary to represent the region with no loss of detail To build a library of boundaries you need to transform them and that transformation should be as computationally as possible and crisp achieves this by returning the boundary polygon, a set of ordeted vertices that when connected are identical to the set of boundary points.
  iii) the resulting order of boundary points should be circular, consistent and predictable it is not difficult to isolate the set of boundary points from a region, however we want those points to have a strong order that makes sense to both humans and computers. crisp wants the vertex to orderd in the counter-clockwise with the position x-axis advancing to the east and the positive y-axis advancing south in 2d space. This order being consistent means for two sets such that the intersection of the two is empty, the algorithm should return two boundary point sequences that are identical. Circularity means that if we index the points with i = 0 &amp;hellip; N then point p_0 needs to be connected to p_1 and p_N, more on this constraint below
  These three conditions need to be fullfilled at the same time of course. I&amp;rsquo;ve used quite a few terms without properly defining them so far so let&amp;rsquo;s get that out of the way
Definitions A image region M of an Image I is defined as a finite set of pixels that is a subset of an image of size m*n such that the following conditions hold:
 for any two pixel x in M there is a different pixel x_n such that x and x_n are 8-connected. M is a subset of I  A boundary can be intuitively described as the outer most pixels of the region that are still a subset of it. Each pixel in the boundary can be approached both from inside M and from the space around M. Formally a boundary K = {p_0, p_1, &amp;hellip;, p_n} is a set of pixels such that
 A: K is a subset of M, this makes M a closed region B: for any pixel p_i in K it holds that p_i-1 is 8-connected to p_i and p_i is 8-connected to p_i&#43;1. p_0 is 8-connected to p_n, this makes the boundary circular C: any pixel y in K has less than 8 neighbors in M, this means in image space y has at least 1 pixel that is not part of the region D: K is not the boundary of a hole (TODO: define hole)  A boundary K is minimal if there exists no pixel p_i such that K - p_i (the set difference) is still 8-connected and fullfills condition B, see above
A boundary polygon P = {v_0, v_1, &amp;hellip;, v_n} is a set of vertices such that if you draw a 1-pixel thick line from v_0 to v_1, v_1 to v_2, &amp;hellip;, v_m-1 to v_m in image space then the set of pixels covered by these lines is identical to the boundary K.
On minimality, it&amp;rsquo;s best to show visually what it means to be minimal, of course we want our set to be as small as possible for performance reason but visually it means that some pixels with only 1 non-M neighbor should be ommitted as such:
 Figure 1 An 8-connected region (A), a boundary (B) and the minimal boundary (C)  As state in the definition if we remove any boundary (white) pixel in C we would either loose detail from the original region or make it so there&amp;rsquo;s an unconnected gap in the sequence of boundary points.
The Algorithm Now that we know what we&amp;rsquo;re trying to do let&amp;rsquo;s first get the actual algorithm out of the way. An step-by-step explanation will follow below
// in BinaryImage image; // image that is true if the pixel is part of a closed region, false otherwise  // out std::vector&amp;lt;Vector2ui&amp;gt; boundary; std::vector&amp;lt;Vector2ui&amp;gt; boundary_polygon; size_t n_holes; // ### STEP 1: pre-process the image bool segment_color = true; // white  // fill all 1-pixel holes for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) == segment_color) continue; // count 4-connected neighbors that are part of the region  size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } if (n &amp;gt;= 3) image(x, y) = segment_color; } } // prune all 1-pixel lines std::vector&amp;lt;Vecto2ui&amp;gt; pruned; while (true) { n_changed = 0; for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) != segment_color) continue; size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } if (n == 1) { image(x, y) = not segment_color; n_changed&#43;&#43;; } } } if (n_changed == 0) break; } // ### STEP 2: extract the segment struct PixelCoordCompare { bool operator()(const Vector2ui&amp;amp; a, const Vector2ui&amp;amp; b) const { return a.y() != b.y() ? a.y() &amp;lt; b.y() : a.x() &amp;lt; b.x(); } }; using PixelSet = std::set&amp;lt;Vector2ui, detail::PixelCoordCompare&amp;gt;; std::vector&amp;lt;PixelSet&amp;gt; segments = crisp::decompose_into_connected_segments(image, {true}); PixelSet segment = segments.at(0); // ### STEP 3: isolate all possible boundary points  PixelSet strong_pixels, // 100% part of b oundary  weak_pixels; // may be needed for continuing in edge cases  for (auto&amp;amp; px : segment) { size_t n_unconnected = 0; for (long i = -1; i &amp;lt;= &#43;1; &#43;&#43;i) { for (long j = -1; j &amp;lt;= &#43;1; &#43;&#43;j) { if (not (i == 0 and j == 0) and segment.find(Vector2ui(px.x() &#43; i, px.y() &#43; j)) == segment.end()) n_unconnected&#43;&#43;; } } if (n_unconnected &amp;gt; 1) strong_pixels.insert(px); else if (n_unconnected == 1) weak_pixels.insert(px); } // ### STEP 4: define the direction function  auto translate_in_direction = [&amp;amp;](Vector2ui point, uint8_t direction) -&amp;gt; Vector2ui { direction = direction % 8; int x_offset, y_offset; switch (direction) { case 0: // WEST  x_offset = -1; y_offset = 0; break; case 1: // SOUTH WEST  x_offset = -1; y_offset = &#43;1; break; case 2: // SOUTH  x_offset = 0; y_offset = &#43;1; break; case 3: // SOUTH EAST  x_offset = &#43;1; y_offset = &#43;1; break; case 4: // EAST  x_offset = &#43;1; y_offset = 0; break; case 5: // NORTH EAST  x_offset = &#43;1; y_offset = -1; break; case 6: // NORTH  x_offset = 0; y_offset = -1; break; case 7: // NORTH_WEST  x_offset = -1; y_offset = -1; break; } return Vector2ui(point.x() &#43; x_offset, point.y() &#43; y_offset); }; // ### STEP 5: Initialize the Tracing  // output, each index holds 1 boundary object where the first one is the outer boundary and every subsequent one is the outline of a hole std::vector&amp;lt;std::vector&amp;lt;Vector2ui&amp;gt;&amp;gt; boundaries_out; std::vector&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt; directions_out; while (strong_pixels.size() &amp;gt; 0) { boundaries_out.emplace_back(); directions_out.emplace_back(); auto&amp;amp; boundary = boundaries_out.back(); auto&amp;amp; direction = directions_out.back(); auto top_left = *strong_pixels.begin(); boundary.push_back(top_left); strong_pixels.erase(top_left); direction.push_back(0); size_t current_i = 0; // ### STEP 6 trace and push once tracing complete  do { // current pixel  auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); // was a candidate found  bool found = false; // check strong candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // check for convergence by looping back to the starting pixel  if (to_check == top_left) finished_maybe = true; if (strong_pixels.find(to_check) != strong_pixels.end()) { // push new pixel from set to boundary  boundary.push_back(to_check); direction.push_back(dir); strong_pixels.erase(to_check); found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { current_i = boundary.size() - 1; continue; } // if no strong candidate was found even though we looped back, we are back at the start  // c.f. explanation below  else if (finished_maybe) break; // if no strong candidate was found, check weak candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; if (weak_pixels.find(to_check) != weak_pixels.end()) { boundary.push_back(to_check); direction.push_back(dir); weak_pixels.erase(to_check); found = true; break; } } if (found) { current_i = boundary.size() - 1; continue; } // if neither weak nor strong candidate found, start traceback  else current_i--; } while (current_i != 0); // ### STEP 7: reduce to non-redundant polygon vertices outer_boundary = boundaries_out.at(0); outer_boundary_directions = directions_out.at(0); size_t n_holes = boundaries_out.size() - 1; // all boundary at position &amp;gt; 0 are those of holes  // define function to detect if two vertices in a sequence are colinear auto turn_type = [&amp;amp;](size_t i_a, size_t i_b) -&amp;gt; int { auto point_a = outer_boundary.at(i_a), point_b = outer_boundary.at(i_b); // warp point from traceback  if (abs(int(point_a.x()) - int(point_b.x())) &amp;gt; 1 or abs(int(point_a.y()) - int(point_b.y())) &amp;gt; 1) return 0; auto dir_a = outer_boundary_directions.at(i_a), dir_b = outer_boundary_directions.at(i_b); if (dir_b &amp;gt; dir_a or (dir_a == 7 and dir_b == 0)) return -1; // left-hand turn  else if (dir_b &amp;lt; dir_a or (dir_a == 0 and dir_b == 7)) return &#43;1; // right-hand turn  else return 0; // colinear }; auto boundary_polygon_out = std::vector&amp;lt;Vector2ui&amp;gt;(); Vector2f mean_pos = Vector2f(0, 0); // discard all colinear vertices but preserve order for (size_t i = 0; i &amp;lt; _boundary.size() - 1; &#43;&#43;i) if (turn_type(i, i&#43;1) != 0) boundary_polygon_out.push_back(_boundary.at(i)); return boundary_polygon_out; Step 1: Pre Processing We first need to pre-process the image, 1-pixel holes and gaps and 1-pixel thick lines will cause problems later on so we need to remove them. Often these features happen purely because of noise and even if you would need them to be part of the actual region because you&amp;rsquo;re working on that small a resolution you can simply scale the image by a factor of 2 and the automated pre-processing step will leave them untouched.
To fill all 1-pixel holes we iterate through the region M (here represented as a binary image) and for each pixel count the number of 4-connected neighboring pixels that are also in M. If at least 3 of them are we found a hole or gap that needs to be patch
// fill all 1-pixel holes for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { // only iterate through region  if (image(x, y) == segment_color) continue; size_t n = 0; // count for connected neighbors  for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } // make that pixel part of the region by setting it to the specified color (black)  if (n &amp;gt;= 3) image(x, y) = segment_color; } }  Figure 2: Left: Segmented region corrupted by noise, Right: Region after filling all 1-pixel holes (newly assigned pixels shown in purple)  We now need to prune all 1-pixel thick lines. We do this recursively (there&amp;rsquo;s more optimal ways to do this that have a far less performance overhead but for the sake of simplicity I will demonstrate the recursive method here. Optimally you would use a subtract a morphological hit-or-miss-transform that detects the lines from the original image) by &amp;ldquo;eating away&amp;rdquo; at the end of each line and repeat until no more lines are left:
// prune all 1-pixel lines while (true) { // count number of changed pixels  n_changed = 0; for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) != segment_color) continue; // count number of 4-connected neighbors in M  size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } // if it only has 1 neighbor it is the end of a line, prune it  if (n == 1) { image(x, y) = not segment_color; n_changed&#43;&#43;; } } } // if no lines changed, we converged  if (n_changed == 0) break; }  Figure 3: Left: Region after filling all 1-pixel holes, Right: Pruned region, removed pixels shown in purple with increased lightness as the number of iterations increases  Note that we leave a &amp;ldquo;stump&amp;rdquo; of size 1-pixel, if this throws of later processing feels free to first erode then dilate the area with a 2x2 structuring element. Morphological processing in general can make this process faster and cleaner but for now we will do it recursively like this.
Now that our region as an image is primed we can transform it into a region as a mathematical set of coordinates.
Step 2: Extract the Region We first define a new typedef PixelSet, this is a set that takes a custom comparator such that the pixel coordinates in the set are ordered left-to-right, top-to-bottom which means the first pixel is the pixel with the smalle x- and y- coordinate, if two pixels share a x-coordinate, the pixel with the lower y-coordinate comes first. Remember that our coordinate systems x-axis extends to right of the image, the y-axis to the bottom so a higher y-value means the pixel is lower down.
struct PixelCoordCompare { bool operator()(const Vector2ui&amp;amp; a, const Vector2ui&amp;amp; b) const { return a.y() != b.y() ? a.y() &amp;lt; b.y() : a.x() &amp;lt; b.x(); } }; using PixelSet = std::set&amp;lt;Vector2ui, detail::PixelCoordCompare&amp;gt;; std::vector&amp;lt;PixelSet&amp;gt; segments = crisp::decompose_into_connected_segments(image, {true}); PixelSet segment = segments.at(0); The actual extraction of the set is left to another crisp function, to learn more about it you can check the documentation HERE TODO. It extracts all 4-connected segments, let&amp;rsquo;s assume our segment is the only one in the image so the only set in the vector is the set of all pixel coordinates M.
Step 3: Isolate all Boundary Pixels A boundary pixel by definition is a pixel that has a neighbor in image space that is not part of M. Much like in our pre-processing steps we can simply iterate through the set and count the number of pixels with less than 8 neighbors. We furthermore devide these into weak and strong boundary candidates. A weak candidate is a pixel with exactly 1 non-M neighbour, a strong candidate is a pixel with 2 or more non-M neighbours. The reason for this seperation will come into play shortly
PixelSet strong_pixels, weak_pixels; for (auto&amp;amp; px : segment) { // count non-M neighbours  size_t n_unconnected = 0; for (long i = -1; i &amp;lt;= &#43;1; &#43;&#43;i) { for (long j = -1; j &amp;lt;= &#43;1; &#43;&#43;j) { if (not (i == 0 and j == 0) and segment.find(Vector2ui(px.x() &#43; i, px.y() &#43; j)) == segment.end()) n_unconnected&#43;&#43;; } } // 2 or more -&amp;gt; strong  if (n_unconnected &amp;gt; 1) strong_pixels.insert(px); // exactly 1 -&amp;gt; weak  else if (n_unconnected == 1) weak_pixels.insert(px); }  Figure 4: Left: Region from step 3, Right: Region with strong boundary candidates highlighted in green, weak candidates highlighted in red.  Step 5: Define the direction function One more thing before we can finally start tracing the boundary, we need to define a function that defines the direction of two pixels in the boundary. Let p_i, p_i&#43;1 be two pixels in K, then we can visualize the following directions as &amp;ldquo;to travel from p_i to p_i&#43;1 we need to go one pixel x&amp;rdquo; where x is the direction. So for x = &amp;ldquo;south west&amp;rdquo;, we need to travel one pixel to the bottom and one pixel to the right in image space. We get 8 direction like this and we assign each direction a number: 0 = west, 1 = south west, 2 = south and so on in clockwise direction (the boundary is traced in counterclockwise direction while this direction mapping function assigns values clockwise). We then end up with 7 = north west after which we loop back to 0 = west. To do ths looping we take the modulo of the direction before returning the appropriate next pixel.
auto translate_in_direction = [&amp;amp;](Vector2ui point, uint8_t direction) -&amp;gt; Vector2ui { direction = direction % 8; int x_offset, y_offset; switch (direction) { case 0: // WEST: p_i&#43;1 is right of p_i  x_offset = -1; y_offset = 0; break; case 1: // SOUTH WEST: p_i is  x_offset = -1; y_offset = &#43;1; break; case 2: // SOUTH  x_offset = 0; y_offset = &#43;1; break; case 3: // SOUTH EAST  x_offset = &#43;1; y_offset = &#43;1; break; case 4: // EAST  x_offset = &#43;1; y_offset = 0; break; case 5: // NORTH EAST  x_offset = &#43;1; y_offset = -1; break; case 6: // NORTH  x_offset = 0; y_offset = -1; break; case 7: // NORTH_WEST  x_offset = -1; y_offset = -1; break; } return Vector2ui(point.x() &#43; x_offset, point.y() &#43; y_offset); }; It is important to really understand this function, for tracing to work properly we need to know what direction we just went to get from p_i-1 to p_i, let&amp;rsquo;s say &amp;ldquo;south&amp;rdquo; (which is the value 2) then when looking for the next pixel around p_i we need to start looking first at south - 1 = south west, if no match was found look south next, south east afterwards, etc.. Doing it like this will assure that our tracing can&amp;rsquo;t get stuck in loops.
Step 6: Tracing We first need to initialize the algorithm, we open a vector that holds confirmed boundary points and a vector of directions that holds the corresponding direction. So when the boundary point is at position j in the vector then the direction vector will have the direction travele from p_j-1 to p_j at position j.
(the following algorithm is missing certain parts of the full algorithm above, this is for ease of explaining, we will get to what other things we need to do later).
auto&amp;amp; boundary = std::vector&amp;lt;Vector2ui&amp;gt;(); auto&amp;amp; direction = std::vector&amp;lt;uin8_t&amp;gt;(); auto top_left = *strong_pixels.begin(); boundary.push_back(top_left); strong_pixels.erase(top_left); direction.push_back(0); We also initialize the first boundary point which is the top-most left-most strong pixel. Because our custom PixelSet orders points in just this way, the first pixel in the set is also the top-most, left-most one. We then initialize the directions with 0 (west). We can now start tracing.
size_t current_i = 0; do { // current pixel and direction  auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); // boolean to track if a proper candidate was found yet  bool found = false; // check strong candidates  // we start at the last direction -1 (counterclockwise before the other)  // and go through all possible directions, here counted with n  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { // we check the next pixel as directed by the direction function mentioned above  auto to_check = translate_in_direction(current, dir); // skip if out of bounds  if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // this will be important for convergene, ignore it for now  //if (to_check == top_left)  // finished_maybe = true;  //if a strong pixel was found  if (strong_pixels.find(to_check) != strong_pixels.end()) { // add it to confirmed boundary points  boundary.push_back(to_check); // add it corresponding direction  direction.push_back(dir); // scrub it from the set of possible boundary points  strong_pixels.erase(to_check); // say that we found a candidate  found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { // reset current_i to the pixel last pushed  current_i = boundary.size() - 1; // and skip the rest of the loop  continue; } (...) Again, some of the loop was left out for clarity for now. We start at the top left and then check if any of the pixels in it&amp;rsquo;s 8-connected neighborhood are also in M. If yes, we found a boundary pixel and can push it and now search the 8-connected neighborhood around it. As mentioned above keeping track of the direction is important to enforce a strong order, if we arrived via direction d at pixel p_i (from p_i-1) then we should start checking pixels at direction d-1 next. d-1 is left of d because the directions are numerate clockwise. If we don&amp;rsquo;t find a pixel there, try the next direction and so on.
Now you might think we are done. We just go through all the pixels and since all strong pixels are linked, it&amp;rsquo;ll just finish, right? Well not quite. Consider our region from earlier:
 Figure 6: State of the tracing algorithm after 44 iterations. The compass wheel on the top shows the directions where ``mint = east``, ``red = west``, ``pink = north``, ``cyan = south``, etc.. The corresponding direction for each point p_i is shown in color. The yellow point is the starting point. On the right the boundary so far is highlighted in white.  We&amp;rsquo;re stuck! We&amp;rsquo;ve only been using strong pixels so far and we&amp;rsquo;ve gotten to a point where there&amp;rsquo;s no other strong pixel in the 8-neighborhood. We kept track of the weak pixels for just this occassion, because:
(the previous code is reposts here for convenience)
do { auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); bool found = false; // check strong candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // check for convergence by looping back to the starting pixel  if (to_check == top_left) finished_maybe = true; if (strong_pixels.find(to_check) != strong_pixels.end()) { // push new pixel from set to boundary  boundary.push_back(to_check); direction.push_back(dir); strong_pixels.erase(to_check); found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { current_i = boundary.size() - 1; continue; } // if no strong candidate was found, check weak candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { // same thing as with strong candidates, just with the weak_pixels set  auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; if (weak_pixels.find(to_check) != weak_pixels.end()) { boundary.push_back(to_check); direction.push_back(dir); weak_pixels.erase(to_check); found = true; break; } } // if we found a weak pixel we can continue  if (found) { current_i = boundary.size() - 1; continue; } } while (current_i != 0); We now do just the same thing for weak pixels but we want to prioritize the strong ones always but now that we would be stuck without them we fall back and add 1 weak pixel:
 Figure 6: State of the tracing algorithm after 44 iterations. The compass wheel on the top shows the directions where ``mint = east``, ``red = west``, ``pink = north``, ``cyan = south``, etc.. The corresponding direction for each point p_i is shown in color. The yellow point is the starting point. On the right the boundary so far is highlighted in white.  ]]></content:encoded>
    </item>
    <item>
      <title>[WIP] Choosing initial Cluster Centers for use in RGB Image Segmentation using k-means clustering</title>
      <link>http://clemens-cords.com/post/k-means_heuristic/</link>
      <pubDate>Thu, 29 Jul 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/k-means_heuristic/</guid>
      <description>Abstract TODO
The Algorithm First we initialize the cluster centers (how exactly this is done will be explained later)
using namespace crisp; struct Cluster { Color color_sum; size_t n; Color mean_color; } std::vector&amp;lt;Cluster&amp;gt; clusters; for (size_t i = 0; i &amp;lt; n_clusters; &#43;&#43;i) { Color color = // heuristically choosen, see below  clusters.push_back(Cluster{color, 1, color}); } Here n is the number of pixels currently in the cluster, color_sum is the sum of the rgb color and mean_color is the current color assigned to the entire cluster.</description>
      <category domain="http://clemens-cords.com/categories/programming">Programming</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[Abstract TODO
The Algorithm First we initialize the cluster centers (how exactly this is done will be explained later)
using namespace crisp; struct Cluster { Color color_sum; size_t n; Color mean_color; } std::vector&amp;lt;Cluster&amp;gt; clusters; for (size_t i = 0; i &amp;lt; n_clusters; &#43;&#43;i) { Color color = // heuristically choosen, see below  clusters.push_back(Cluster{color, 1, color}); } Here n is the number of pixels currently in the cluster, color_sum is the sum of the rgb color and mean_color is the current color assigned to the entire cluster.
We then specify a distance measure, for this variant the euclidian distance in rgb space is sufficient. Quantizing the possible values (remember, our Colors are floating point vectors which each element in the range [0, 1]) aids in performance
auto distance = [](Color a, Color b) -&amp;gt; int { int score = abs(int(a.red() * 255) - int(b.red() * 255)) &#43; abs(int(a.green() * 255) - int(b.green() * 255)) &#43; abs(int(a.blue() * 255) - int(b.blue() * 255)); return score; } We quantize the color components to 8-bit and then compute the euclidian distance ommitting the square root and square operations for improved performance as no normalization is necessary. Note that this function will return values in the range of [0, 3*255].
The algorithm is initialized by first allocating the result image. To save on performance we use the image itself as a way to keep track of which pixel is in which clusters. Since the result image is an RGB image we arbitrarily declare the .red() component to be the cluster index (range {0, 1, 2, &amp;hellip;, n_clusters - 1}) while all other components are set to 0. While color components can only be in the range [0, 1] for display, crisp allows you to freely set the values while the images are still in memory. The range is only enforced when binding an image for rendering.
ColorImage out; // .r is cluster index out.create(image.get_size().x(), image.get_size().y(), Color(-1, 0, 0)); Now for the algorithm proper, we can think of it in two parts, the first part is the initial iteration. Here we assign each pixel to the cluster nearest to it:
for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { auto&amp;amp; pixel = image(x, y); // find cluster with minimum distance  int min_distance = std::numeric_limits&amp;lt;int&amp;gt;::max(); size_t min_cluster_i = -1; for (size_t i = 0; i &amp;lt; clusters.size(); &#43;&#43;i) { // compute mean color of all cluster pixels  auto current_distance = distance(pixel, clusters.at(i).mean_color); if (current_distance &amp;lt; min_distance) { min_distance = current_distance; min_cluster_i = i; } } // assign pixel to cluster, .red() component is cluster index  pixel.red() = min_cluster_i; } We could end the algorithm right now and we would have a decent segmentation however this relies entirely on the heuristically chosen cluster centers and there is no guruantee the clusters would trend towards optimality. We achieve a decent approximation of this behavior (achieving proper optimality is NP-hard) by iteratively checking each pixel again and if there is a cluster that&amp;rsquo;s a better fit for it, we reassign it. After each iteration, we recompute the current mean cluster color meaning the clusters will change a little each iteration until convergence is achieved.
size_t n_changed = -1; // number of pixels that changed cluster this iteration  while (n_changed &amp;gt; 0) { n_changed = 0; for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { auto&amp;amp; pixel = image(x, y); // find cluster with minimum distance  int min_distance = std::numeric_limits&amp;lt;int&amp;gt;::max(); size_t min_cluster_i = -1; for (size_t i = 0; i &amp;lt; clusters.size(); &#43;&#43;i) { // compute mean color of all cluster pixels  auto current_distance = distance(pixel, clusters.at(i).mean_color); if (current_distance &amp;lt; min_distance) { min_distance = current_distance; min_cluster_i = i; } } // get cluster pixel is currently assigned to  int old_i = pixel.red(); // reassign  if (old_i != min_cluster_i) { auto&amp;amp; old_cluster = clusters.at(old_i); auto&amp;amp; new_cluster = clusters.at(new_i); old_cluster.n -= 1; old_cluster.color_sum -= pixel; new_cluster.n &#43;= 1; new_cluster.color_sum &#43;= pixel; } // else do nothing  } // update clusters  for (auto&amp;amp; cluster : clusters) cluster.mean_color = cluster.color_sum / cluster.n; } It can be shown [TODO] that if ties (that is two clusters have the exact same distance to a single pixel) are resolved arbitrarily but each pixel is only assigned to one cluster this algorithm will always converge.
[TODO: convergence with images after each cluster]
The Heuristic In proper statistics it is common to run the algorithm multiple times with different number of cluster centers. Some even randomize the cluster centers completely and the solution is achieved by testing convergence between different entire runs rather than iterations within the same run. In image segmentation this is not available to us, usually the viewer will a) know how many principal color clusters there are in an image and b) will not have the memory nor time to do multiple executions. This means we basically need to nail it on the first try.
]]></content:encoded>
    </item>
  </channel>
</rss>
