<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title >deep learning on Clemens Cords&#39; Homepage</title>
    <link>http://clemens-cords.com/categories/deep-learning/</link>
    <description>Recent content in deep learning on Clemens Cords&#39; Homepage</description>
    <image>
      <title>deep learning on Clemens Cords&#39; Homepage</title>
      <link>http://clemens-cords.com/categories/deep-learning/</link>
      <url>http://clemens-cords.com/rat_icon.png</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.68.3)</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 06 Mar 2022 19:49:27 UT</lastBuildDate>
    <atom:link href="http://clemens-cords.com/categories/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning Aided Differentiating of Chicken from Duck Eggs using Simulated Data</title>
      <link>http://clemens-cords.com/post/crisp_deep_learning/</link>
      <pubDate>Tue, 05 Oct 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_deep_learning/</guid>
      <description>(This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.hpp&amp;gt;#include &amp;lt;classification/fully_connected_neural_networks.hpp&amp;gt;#include &amp;lt;classification/convolutional_neural_networks.hpp&amp;gt; // all of the above collected in: #include &amp;lt;classification.hpp&amp;gt;Table of Contents  Introduction 1.1 Features, Class Membership
 Bayes Statistical Classifier
2.1 Training the Bayes Classifier
2.2 Using the Bayes Classifier for Identification
 Neural Networks
3.1 Architecture
3.2 Weights and Biases
3.3 Creating/Loading a Network
3.4 Training the Network
3.5 Using the Network for Identification
  1. Introduction We learned how to extract part of an image into a segment, then compute unique descriptors that describe the selected region&amp;rsquo;s shape and texture. Now, it is finally time to use these descriptors to solve one of the most common image processing applications: classification.
To classify an object means to divide the population it is from into a number of classes {C_0, C_1, ..., C_m-1}, then assign a label to any one specific object. This label represents what class it belongs to. For the sake of simplicity, for the rest of this tutorial, we assume that an object can only belong to exactly one class. While this assumption is not necessary for any of crisps deep learning features, it will make things easier to follow later.
1.1 Features &amp;amp; Class Membership For one specific region or object, class membership is given as a m*1 matrix M:
   class index i M(i, 0)     0 {0, 1}   1 {0, 1}   &amp;hellip; &amp;hellip;   m-1 {0, 1}    Where if M(4, 0) == 1 then the object is classified into class C4. Because an object can only belong to one class at a time, M.sum() == 1 (as all values except for the single 1 are 0 and an object has to belong to at least one class).
In order to be able to classify an object we need to describe it somehow, we do this using a n*1 matrix called a feature vector. Each row of this matrix N corresponds to one feature:
   feature index N(i, 0)     0 [-1, 1]   1 [-1, 1]   &amp;hellip; &amp;hellip;   n-1 [-1, 1]    When classifying multiple objects from the same population, all objects in that population have to have a feature vector of the same size and meaning, that is for each object, each row means the same thing. All that changes is the actual values of the features. As we saw in the table above, it is necessary to normalize the value of the feature into the range [-1, 1]. In the literature, features are often normalized into [0, 1] but because this is a subset of [-1, 1] anyway, either approach works in crisp.
A training data set in crisp is a set of feature vectors and their corresponding, desired classifications. In practice, this set is represented by two matrices:
  FeatureMatrix_t is a n*x matrix where n the number of features and x the number of samples in the population. Each row is one feature, each column is one sample. All its values should be normalized into `[-1, 1]
  ClassificationMatrix_t is a m*x matrix where m is the number of classes and x the number of sample. Each row is one class, each column is one sample. All elements of a row should be either 1 or 0
  The number of columns, x, has to be identical for both matrices. If one object has its feature vector as the column of the feature matrix at position i, the iths column of the classification matrix corresponds to that same object.
It may be instructive to consider a practical example. Consider the following data set:
   sample class egg weight (g) yolk to white ratio     0 Chicken 52 31%   1 Chicken 58 34%   2 Chicken 67 32%   3 Chicken 62 37%   4 Chicken 59 32%   5 Chicken 65 29%         6 Duck 61 36%   7 Duck 63 35%   8 Duck 69 39%   9 Duck 60 38%   10 Duck 71 37%   11 Duck 75 34%    Here we have a set of 12 eggs from both chickens and ducks. Each egg sample has 3 data point: whether it was from a chicken or duck, it&amp;rsquo;s weight, and it&amp;rsquo;s yolk-to-white-ratio. We want to create a classifier can decide whether a new, unknown egg we hand it is from a chicken or duck.
Before we can use crisp for classification, we need to arrange the data in a way that crisp understands.
We have m = 2 classes and n = 2 features. We are trying to predict the &amp;ldquo;class&amp;rdquo; property of each egg, so we create a matrix with 2 rows, each column of the matrix has a single 1 depending on whether the sample was from a chicken or duck.
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck Each egg sample gets a 2x1 feature vector that is 1 in the first row if it is from a chicken, or 1 in the second row if it is from a duck.
We transform the feature values in a similar way. Each column corresponds to one sample (egg), we arbitrarily decide that weight will be in row 0, yolk-ratio in row 1. Recall that features need to be normalized, so we divide both feature values by 100:
auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yok ratio We call the first matrix class membership matrix and the second a feature matrix.
2. Bayes Statistical Classifier The simplest classifier in crisp is crisp::BayesClassifier. This class has two template parameters:
template&amp;lt;size_t FeatureN, size_t ClassN&amp;gt; class BayesClassifier {  FeatureN is the number of features (n in our notation) ClassN is the number of classes (m in our notation)  Both need to be available at compile time.
The classifier works by estimating the type of distribution, mean and variance from a given population. When we hand it a sample x to identify the classifier will return a score that is directly proportional to, in our chicken-or-duck example, the following probabilities:
p( x | chicken) // is our current egg x from a chicken p( x | duck) // is our current egg x from a duck The return values of the classifier are not actual probabilities (they can be outside the range [0, 1]), however the order and relative value of the actual probabilities are preserved. This means we can compare and quantify them, just like we would probabilities.
2.1 Training the Bayes Classifier To estimate the values needed for classification, the bayes classifier needs to observe the training set population. We retrieve the feature- and classification matrix from earlier, then call BayesClassifier::train:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk  auto bayes = BayesClassifier&amp;lt;2, 2&amp;gt;(); // 2 features, 2 classes bayes.train(feature, classes); That&amp;rsquo;s all, unlike deep learning procedures later in this section, the bayes classifier only needs to iterate through the training set once to achieve the maximum possible classification performance based on the evidence.
2.2 Identification using the Bayes Classifier Sticking to our chicken-or-duck example, let&amp;rsquo;s say we just came home and found 3 eggs around our hypothetical farm and we really want to know whether it was the chickens or the ducks that hid them. We measure their size and content:
   egg weight yolk ratio     0 51 29%   1 67 37%   2 70 38%    We arrange these three samples just like we did with our training datas feature matrix:
auto to_identify = Eigen::MatrixXf(2, 3); // 0 1 2 // egg to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio We can then classify them using:
auto result = bayes.identify(to_identify); std::cout &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl; 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 We get a bunch of numbers, let&amp;rsquo;s try to understand what they mean. Firstly, each column index corresponds to their respective egg sample:
// 0 1 2 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 The numbers are scores representing the relative probability that the sample in column i is from class j where j is the row index. In our training data set, we had chickens as j = 0 and ducks as j = 1, so:
// 0 1 2 0.667373 0.500235 0.0936609 // ~p(egg | chicken) 0.332627 0.499765 0.906339 // ~p(egg | duck) Where ~p(x | y) is a score that&amp;rsquo;s directly proportional to the probability p(x | y).
We decide which egg came from which species by simply comparing the probabilities, choosing the class membership with the higher score.
Technically we would assign egg 0 and 1 as chicken,3 as duck, however the result for egg 1 is so ambiguous that it is basically a coin toss. To quantify this ambiguity, we assign each result a confidence score. One of the simplest confidence score is the absolute distance between the values:
auto to_confidence = [](float a, float b) { assert(a &amp;gt;= 0 and a &amp;lt;= 1 and b &amp;gt;= 0 and b &amp;lt;= 1); return abs(a-b); }; Because our classification output is always in [0, 1], the higher the distance between the two probabilities, the more certain we can be that we did not make a classification error.
We summarize our results as such:
   egg ~p(chicken) ~p(duck) classification confidence     0 0.667 0.332 chicken 0.33 (medium)   1 0.5 0.499 chicken 0.001 (very low)   2 0.09 0.90 duck 0.84 (high)    3 Neural Networks While the Bayes Classifier is useful and can lead to quite decent results with very little computational cost, some classification processes require a more flexible approach. Since the 1970s, the state-of-the-art way of solving classification problems in a semi-supervised way has been neural networks. This tutorial will not cover how they work in detail, instead we will learn just enough fundamentals to understand how to use crisp::NeuralNetwork and how to interpret its results.
3.1 Architecture A neural network has l layers. Each layer L_i has a number of neurons #L_i.
 L_0 is called the input layer. This layer has the same number of neurons as the feature vector has rows: #L_0 = n L_l is called the output layer. This layer as the same number of neurons as the number of possible classes: #L_l = m L_i for i in {1, 2, ..., l-1} are called hidden layers  Where n the number of features, m the number of classes (c.f. Section 1).
We call the number of layers and the number of neurons in each layer a network&amp;rsquo;s architecture. We define the architecture in crisp using the template arguments of crisp::NeuralNetwork:
template&amp;lt;size_t... NeuronsPerLayer&amp;gt; class NeuralNetwork; auto nn = NeuralNetwork&amp;lt;n, #L_1, #L_2, ..., m&amp;gt;(); Where n, m, #L_i as defined above. For example:
// binary feature vector: n = 2 // two possible classes: m = 2 // one hidden layer with 3 neurons: #L_1 = 3 auto nn = NeuralNetwork&amp;lt;2, 3, 2&amp;gt;(); // 20 features, 2 hidden layers with 3 neurons each, 2 classes auto nn = NeuralNetwork&amp;lt;20, 3, 3, 2&amp;gt;(); // 2 features, 5 layers with 10 neurons each, 19 classes auto nn = NeuralNetwork&amp;lt;2, 10, 10, 10, 10, 10, 19&amp;gt;(); A network&amp;rsquo;s architecture governs its structure. For the same problem, different architectures can result in vastly different results. Furthermore, the computational complexity (in terms of runtime performance) will of course increase with the total number of neurons. In praxis choosing the correct architecture is somewhat of an art, while heuristics exist in the literature it is often important to experiment.
3.2 Weights and Biases Each neuron has a number of weights and one bias. The number of weights is equal to the number of neurons in the previous layer, for example if we&amp;rsquo;re in layer L_2, all our neurons will have #L_1 weights and a single bias. The weights and biases govern a neuron&amp;rsquo;s output, thus (along with architecture) they completely define a network&amp;rsquo;s performance in terms of classification.
3.3 Creating a Network We can construct a fresh, untrained network like so:
#include &amp;lt;classification/fully_connected_neural_network.hpp&amp;gt; // in main.cpp auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2&amp;gt;(); This initializes all weights to 1 &#43; eps and all biases to 0 &#43; eps, where eps ∈ [-0.25, &#43;0.25] (random, uniformly distributed). Once our network is trained, we will want to export it to the disk to avoid having to retrain it every time the application launches. We can convert a network into a string like so:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto exported = nn.as_string(); // save exported to file We can then simply save the string to a file or otherwise store it. At a later point, we can then load the neural network from the disk using:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto file = std::ifstream(/*...*/ &#43; &amp;#34;/crisp/docs/feature_classification/chicken_or_duck_nn_2_4_4_2.txt&amp;#34;); std::stringstream buffer; buffer &amp;lt;&amp;lt; file.rdbuf(); std::string str = buffer.str(); nn.from_string(str); Here we&amp;rsquo;re opening a file stream, moving its content into a buffer stringstream and then create a plain string from said stringstream. The neural network can then parse the string it generated on a previous occasion and load the weights and biases. This is somewhat clumsy owing to C&#43;&#43;s file interface so feel free to save the string generated using as_string() as you see fit.
Note that for loading to be successful, the exported network and the network we&amp;rsquo;re trying to import have to have identical architectures. Trying to import the exported string of a network with a different architecture will trigger an exception. It is therefore good practice to state the network&amp;rsquo;s architecture in the filename (as in the above example).
3.4 Training the Network Recall our egg example from section 1:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk We will use this data to train our network. Unlike the bayes classifier, neural networks will have to complete many training cycles. Each cycle of going through the entire training data set once is called an epoch.
We can conduct a single epoch like so:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(/*...*/); nn.train(features, classes); Neural Networks work in a way that is guaranteed to converge towards a set of optimal class boundaries (functions that divide the feature space into the given classes) with enough epochs. Here, &amp;ldquo;optimal&amp;rdquo; means, that we&amp;rsquo;re minimizing the mean square error. We can directly compute the error of the current network state like so:
nn.compute_mean_squared_error(feature, classes); Here, we are not computing the error between feature and classes, rather, the neural network generates a classification matrix from feature, then measures the error between this result and classes.
The errors for the first few epochs with our egg data and the architecture from the above example:
   epoch maximum MSE     1 2.42762   2 2.08903   3 0.778278   4 0.675251    We see that the error is already decreasing rapidly. Depending on the architecture and problem, the mean square error may actually increase at the beginning, but then slowly stabilize and go back down as more epochs are completed.
If we want to only train for a certain number of epochs, we use:
for (size_t n_epochs = 0; n_epochs &amp;lt; 10000; &#43;&#43;n_epochs) nn.train(features, classes); If we have no idea how many epochs are needed to get to an acceptable classification error, we can instead use NeuralNetwork::train_until. It has 3 arguments: the feature matrix, the classification matrix and a mean square error threshold. If the network observes the current classification error to be stable and below that threshold, the function exits and returns the number of epochs it took:
auto n_epochs = nn.train_until(features, classes, 0.15); std::cout &amp;lt;&amp;lt; n_epochs &amp;lt;&amp;lt; std::endl; The number of epochs is only useful for monitoring and can often be discarded. For our egg training data, it took 1059191 many epochs to get below the threshold of 0.15. This is quite a lot, one parameter that governs the convergence behavior is called the learning constant (often called μ or α). This is a value in [0, 1] that we can optionally hand to the network on construction or by setting it later using set_learning_constant:
float alpha = 0.001; auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(alpha); // or nn.set_learning_constant(alpha); Usually values will be between 0.0001 and 0.1, however just like the network&amp;rsquo;s architecture, finding a good learning constant will require a lot of experimentation. For badly chosen values, the network can even begin to diverge, meaning the error will increase.
Now that our network is trained, we should export it, so we don&amp;rsquo;t have to do another 1059191 epochs. The weights and biases for the above example with alpha = 0.001 were:
1 0 7.30575 -1.03484 2.03974 -5.31141 5.80577 1.88958 6.59601 4.74493 -0.485592 1.42384 1.80752 2.70784 5.16637 3.31374 4.56878 -0.904903 1.88335 1.32096 1.80904 0.728481 5.68054 -0.569291 2.96311 1.78811 2.837 -0.384504 1.49958 6.98229 0.633096 1.17761 0.2897 -1.09701 -1.95467 3.00161 3.54201 -0.634866 -1.76444 -2.18135 1.18332 -0.43307 1.34111 0.527556 The first number will always be 1, this is the single and only weight of the input layer. It is always followed by a 0 which is the bias of the input layer. All following number correspond to the weights and biases of the subsequent layers.
3.5 Using the Network for Classification Now that our network is well-trained and the mean squared error is low, we should compute its classification error. Unlike the MSE, the classification error is defined as the percentage of false-positive or false negatives when classifying a data set for which the actual correct classifications are known.
We first load the network into memory using the string we generated earlier, then we classify the entire feature matrix from the training set:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(); nn.from_string(/*...*/); std::cout &amp;lt;&amp;lt; classes &amp;lt;&amp;lt; std::endl; std::cout &amp;lt;&amp;lt; nn.identify(features) &amp;lt;&amp;lt; std::endl; 0.99638 0.993424 0.988063 0.856026 0.998413 0.999656 -0.227162 0.343421 0.000525594 -0.426658 -0.00993148 -0.00718939 -0.0112014 0.434531 0.715863 0.272235 -0.189113 0.0802148 0.891286 0.900159 0.924364 0.830156 0.925139 0.924666 Like with the bayes classifier, each of the rows correspond to one class (chicken or duck), each column corresponds to one sample (egg). We classify an egg as chicken if the value in row 0 is higher, as a duck if the value in row 1 is higher.
We get the following result, where E(x) is the expected value of x as designated in the training sets classification matrix M
   sample Chicken E(Chicken) Duck E(Duck) correct     0 0.99638 1 -0.0112014 0 yes   1 0.993424 1 0.434531 0 yes   2 0.988063 1 0.715863 0 yes   3 0.856026 1 0.272235 0 yes   4 0.998413 1 -0.189113 0 yes   5 0.999656 1 0.0802148 0 yes   6 -0.227162 0 0.891286 1 yes   7 0.343421 0 0.900159 1 yes   8 0.000525594 0 0.924364 1 yes   9 -0.426658 0 0.830156 1 yes   10 -0.00993148 0 0.925139 1 yes   11 -0.00718939 0 0.924666 1 yes    We see that the neural network was able to classify all samples correctly. The least confident sample from this population was egg 2, while the most confidently classified was egg 5, where confidence is defined as in section 2.
Now that we know our neural network works decently well, we can use it to identify the three unknown eggs:
auto to_identify = Eigen::MatrixXf(2, 3); to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio  auto results = nn.identify(to_identify); // 0 1 2  -0.021921 0.8932 -0.0119139 // chicken  0.926603 0.918492 0.925525 // duck Just like with bayes classifier, eggs 0 and 2 were identified confidently. Egg 1 was previously undetermined as the bayes classifier assigned both options the same score, our neural network however leans more towards duck, if only by a difference of 0.02. This result is still not very confident, but it&amp;rsquo;s at least possible to choose one or the other.
]]></content:encoded>
    </item>
    <item>
      <title>Describing a Regions Shape and Texture with Descriptors intended for Deep Learning Applications</title>
      <link>http://clemens-cords.com/post/crisp_feature_extraction/</link>
      <pubDate>Mon, 27 Sep 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_feature_extraction/</guid>
      <description>(This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region
 Region Boundary
2.1 Definition
2.2 Boundary Polygon
 Boundary Signature
3.1 Vertex Polygon
3.2 Slope Chain Code Signature
3.3 Radial Distance Signature
3.4 Complex Coordinate Signature
3.5 Farthest Point Signature
 Whole Region Descriptors
4.1 Area, Perimeter, Compactness
4.2 Centroid
4.3 Axis Aligned Bounding Box
4.4 Major and Minor Axis
4.5 Eccentricity
4.6 Circularity
4.7 Holes
4.8 N-ths Moment Invariant
 Texture Descriptors
5.1 Intensity Histogram
5.2 Maximum Intensity Response
5.3 Mean, Variance
5.4 N-ths Pearson Standardized, Centralized Moment
5.5 Average Entropy
5.6 Co-Occurrence Matrix
5.7 Intensity Correlation
5.8 Homogeneity
5.9 Directed Entropy
5.10 Contrast
  1. Introduction In the tutorial on segmentation, we discussed how to extract part of an image. Now, we will find out what to actually do with said segment. Recall that, in crisp, an image segment is a set of pixel coordinates Vector2ui:
using ImageSegment = std::set&amp;lt;Vector2ui, /*...*/&amp;gt;; crisp::ImageSegment holds no information about anything other than the pixels coordinates. This allows for a certain degree of generality, we can use a segment as if it were part of many images. The only restraint on those images is, that they have to be at least the size of the segment.
We can apply any function only to a segment of an image, like so:
auto lambda_operator = []&amp;lt;typename Image_t&amp;gt;(size_t x, size_t y, const Image_t&amp;amp; image) -&amp;gt; typename Image_t::Value_t { auto&amp;amp; value = image(x, y); // some transformation  return value; }; ImageSegment segment = /*...*/; auto image = /*...*/; for (const auto position : segment) image(position.x(), position.y()) = lambda_operator(position.x(), position.y(), image); Here, we first define a templated lambda. This function takes the pixel coordinate and the corresponding image as arguments, reads the pixel&amp;rsquo;s value, transforms it in some way, then assigns it back to the image.
While this works well when the entire image is available, sometimes we really don&amp;rsquo;t need the rest of the image. For just this purpose, crisp defines crisp::ImageRegion:
template&amp;lt;typename Image_t&amp;gt; class ImageRegion { using Value_t = Image_t::Value_t; public: /* ... */ private: struct Element { Vector2ui _position; Value_t _value; float _intensity } std::set&amp;lt;Element, /*...*/&amp;gt; _elements; } We see that instead of just pixel coordinates, ImageSegment holds a set of ÌmageSegment::Elements. Each element has 3 members: the original pixel coordinate _position, the original value of the corresponding pixel in the image _value, and _intensity which is the mean of all planes of _value. We will use _intensity extensively in the texture descriptor chapter, but for now, it&amp;rsquo;s enough to remember that ImageSegment holds only pixel coordinates while ImageRegion holds those coordinates as well as deep-copies of the pixels values.
We construct an ImageRegion from an image and an ImageSegment like so:
// in main auto image = /*...*/; // crisp::Image&amp;lt;T, N&amp;gt; for any T, N auto segment = // some segmentation algorithm that returns crisp::ImageSegment  auto region = ImageRegion(); region.create_from(segment, image); // or equivalently: auto region = ImageRegion(segment, image); Once create_from is called, all pixel intensities are copied, so we are free to deallocate the original image or change it without the values in the image region being affected. If we were to change the pixel values in the region while the original image is still in memory, it would be unaffected.
crisp offers a shortcut to convert an entire image into one big region:
auto region = ImageRegion(image); // or auto region = ImageRegion(); region.create_from(image); This comes in handy whenever we want functionality only available to ImageRegion but have no need to first segment the image.
1.1 An Example To better illustrate the entire process of loading an image, segmenting it, then extracting a region, consider this image of the &amp;ldquo;pepper brush&amp;rdquo; as provided by gimp Wanting to extract the region that has the pepper, we observe the background to be very dark. A simple manual thresholding operation is guaranteed to extract the correct region. We first load the image as color, then convert it to grayscale using Image::get_value_plane(size_t) (remember that the HSV &amp;ldquo;value&amp;rdquo; component is equal to the mean of all RGB color components):
#include &amp;lt;system/image_io.hpp&amp;gt;#include &amp;lt;image/grayscale_image.hpp&amp;gt;#include &amp;lt;segmentation.hpp&amp;gt;using namespace crisp; // in main.cpp auto as_color = load_color_image(/*...*/ &#43; &amp;#34;/crisp/docs/feature_extraction/pepper.png&amp;#34;); auto as_grayscale = as_color.get_value_plane(); auto thresholded = Segmentation::manual_threshold(as_grayscale, 0.01f); We then want to decompose the binary image into connected segments. After decomposition, the segments will be ordered according to their respective left-most, top-most pixels coordinate. The pixel at (0, 0) is black and there are only two segments (the pepper in white, and the background in black) thus we expect the pepper to be the second segment extracted:
auto segments = decomponse_into_connected_segments(thresholded); auto pepper_segment = segments.at(1); We can now construct the resulting region using the pepper segment and the original color images values:
auto pepper = ImageRegion(pepper_segment, image); This is why we loaded the image as color but thresholded the grayscale image. Constructing the region with the colored pixel values means no information is lost.
2. Region Boundary Mathematically, crisps regions are closed, simply connected regions. This basically means:
 all boundary elements are part of the region all elements are 4-connected  ImageRegion will throw an exception if the segment handed to it is not 4-connected. We can assure it is either by using decompose_into_connected_segments (for more information about this, visit the segmentation tutorial) or we can use decompose_into_regions(const ImageSegment&amp;amp;, const Image_t&amp;amp;) -&amp;gt; std::vector&amp;lt;ImageRegion&amp;gt; which is provided to automatically split a segment into 4-connected sub-segments and then constructs a region from each.
Now that we assured that our region is indeed 4-connected, we can concern ourselves with, in terms of feature recognition, one of the most important properties of a region: it&amp;rsquo;s boundary. In crisp boundaries have the following properties:
(a visual, less math-y exploration of these concepts will follow)
Let B = {b_0, b_1, b_2, ..., b_m} be the set of boundary points, then:
 i) for each b_i there exists a b_i-1, b_i&#43;1 in B such that b_i-1 is 8-connected to b_i, b_i&#43;1 is 8-connected to b_i and b_i-1 is not 8-connected to b_i&#43;1 ii) b_0 is 8-connected to b_m iii) the set B is minimal in terms of cardinality, that is if we were to remove any b_i in B, property i) or ii) would be violated  Because definitions can be hard to conceptualize, let&amp;rsquo;s consider a purely visual example to illustrate these concepts:
2.1 8-Connectivity and Minimal Cardinality Property i) and ii) mean the boundary is an unbroken chain of 8-connected pixels and that the boundary forms a path such that one can jump from b_0 to b_1, b_1 to b_2, etc. up to b_m-1 to b_m (the last point) and then, crucially, from b_m back to b_0 completing the circle.
Let&amp;rsquo;s again consider the region of our pepper:
A simple way of tracing its boundary would be to highlight all pixels that have at least one neighbor that is not in the region (black, in our case).
This is a boundary that fulfills condition i) and ii), however inspecting the boundary closely we notice many redundant points:
If we were to ask a human to remove as many points as possible without compromising conditions i), ii), we would get the following boundary (where necessary pixels are highlighted in magenta, redundant pixels in cyan)
This is what condition iii)s minimality represents, we want all pixels to be non-redundant. This vastly increases performance, as for our pepper example we go from 3854 pixels for our trivial boundary to only 472 pixels using a minimal boundary:
crisps proprietary boundary tracing algorithm assures that the computed boundary is always minimal. After creating the region, we can access it at any point (with no performance overhead) using get_boundary(). The pixels are ordered according to condition i) where the first pixel b_0 is the left-most, top-most pixel and any following pixels b_i : i &amp;gt; 0 are enumerated in counter clock-wise direction.
2.2 Boundary Polygon We can even further reduce the number of elements in the boundary by treating it as a polygon that has vertices and straight, non-intersecting lines connecting exactly two of the vertices in order of enumeration. Consider this part of our pepper boundary:
We note multiple straight lines, each of these lines can be represented by just two pixels at the start and beginning of the line, shown in green rgb(0, 1, 0) here:
Using this approach, we reduce the number of boundary points from 472 to only 193, without loosing any information. The information is retained by the fact that the polygon vertices are ordered in counter-clockwise direction, this way we know exactly where to draw the straight line to the next point if we wanted to reconstruct the full boundary.
Now that we reduced the entire information contained in the region in the shape of a pepper to just 193 pixels, we may think we are done but thanks to more math we can reduce it even further, while increasing the representations&amp;rsquo; generality.
3. Boundary Signatures A signature is a mathematical transform of the boundary points of a shape that aims to, in some way, make the description of the boundary more widely applicable. When referring to the signature in this section, we are referring to the transform of the boundary polygons vertex points, as these are the smallest set of points that still represent the region boundary with no loss of information.
One of the ways to make a signature applicable to more than just one image is making it independent of rotation, a signature that accomplishes this describes not only our upright pepper but all possible rotation of it at the same time. Another form of generality is scale invariance, meaning that the signature describes our pepper at scale 1 and the same pepper scaled by any factor &amp;gt; 0. Lastly, independence of translation means that it does not matter if we were to translate all points of the signature by a constant (x, y), the signature represents all of those peppers just the same. crisp offers a multitude of signatures that may or may not be invariant in multiple of the aspects described above.
3.1 Vertex Polygon          scale not invariant   rotation not invariant   translation not invariant    This is the simplest signature, as already mentioned, it reduces the boundary to the vertices of it&amp;rsquo;s polygon. It is neither scale, translationally, nor rotationally invariant, it is however the basis for all other signatures. We can access it using:
auto polygon_signature = pepper.get_boundary_polygon(); 3.2 Slope Chain Code Signature          scale invariant   rotation not invariant   translation invariant    We can generate the slope-chain-signature by iterating through all boundary polygon vertices, storing the angle of the line that connects our current polygon vertex to the next (recall that the vertices are ordered counter-clockwise). This makes it invariant to both translation and scale, however rotation would alter all the angles values, so it is not invariant in this aspect. One way to make it rotationally invariant is to instead save the delta of successive angles. crisps slope_chain_code_signature() does not do this automatically, it is however a trivial operation.
The angles are stored in radians in the same order as their vertices.
3.3 Radial Distance Signature          scale not invariant   rotation invariant   translation invariant    The radial distance signature is the distance of each vertex from the region&amp;rsquo;s centroid. The centroid of a region in crisp is defined as the mean of all boundary coordinates, and can be intuitively thought of as the center of mass of a hole-less region if all pixels have the same weight. Because we are measuring the absolute distance, this signature is not invariant to scale. We can generate it using std::vector&amp;lt;float&amp;gt; ImageRegin::radial_distance_signature() const.
3.4 Complex Coordinate Signature          scale invariant   rotation invariant   translation invariant    (as proposed by El-Ghazal, Basir, Belkasim (2007))
This signature transforms each point in the boundary polygon into a complex number, for a point (x, y) the signature of the point is the complex number x &#43; i*y where i is the imaginary constant. The x-coordinate is treated as the real part, the y-coordinate is treated as the imaginary part. While this signature itself is neither invariant to scale nor rotation, we can now fourier-transform the complex numbers and store the corresponding coefficients. This achieves scale, rotational and translational invariance.
We can access the raw complex coordinates using ImageRegion::complex_coordinate_signature(). We can then use crisp::FourierTransform to transform them into their fourier descriptors.
3.5 Farthest Point Signature          scale not invariant   rotation invariant   translation invariant    (as proposed also by El-Ghazal, Basir, Belkasim (2009))
This signature computes, for each boundary point, the maximum distance to any other boundary point. When used for fourier descriptors, it performs better than other boundaries mentioned here [1] and achieves translational and rotational invariance even before fourier transformation. We can generate it using ImageRegion::farthest_point_signature.
[1] (Y. Hu, Z. Li, (2013): available here
4. Whole Region Descriptors Signatures are a transform of a region&amp;rsquo;s boundary points (the vertices of its polygon, to be precise). This is useful in unique representing a regions&amp;rsquo; boundary, but it doesn&amp;rsquo;t describe the shape of it in any way. To compare two boundaries, we would have to come up with a distance measure that compares the signatures, which can be quite hard. Instead, we can rely on the field of mathematical topology to give us many, much simpler to compute properties of a boundary. While only one of these will not unique identify a region&amp;rsquo;s shape, using multiple descriptors along with a signature can lead to great results.
4.1 Area &amp;amp; Perimeter, Compactness One of the easiest descriptors are area, the number of pixels in a region, and perimeter, the length of the region&amp;rsquo;s boundary. Perimeter only takes into account the outermost boundary, increasing the number of holes in a region will decrease its area but leave its perimeter unchanged.
Area and perimeter are usually not very useful unless they are normalized, for example by quantifying the area&amp;rsquo;s compactness, which is equal to the square of the perimeter divided by the area. A region that has no holes will have maximum compactness, while a region with the same boundary, but many or very big holes has a lower compactness. We can access area, perimeter and compactness using:
float get_perimeter() const; float get_area() const; float get_compactness() const; The values for compactness are usually in [0, 2], for edge cases the value may be outside this interval, however.
4.2 Centroid A region&amp;rsquo;s centroid, in crisp, is defined as the mean of the coordinate values of its boundary points (note that all boundary points are weighted here, not just the boundary polygons vertices). In the literature, the centroid is sometimes defined as the average of all points in a region, so it is important to remember that crisp only uses the boundary: Adding holes to a region while leaving its boundary unchanged does not alter the position of the region&amp;rsquo;s centroid.
We can access a region&amp;rsquo;s centroid at any time using get_centroid().
Where the centroid is highlighted using a red rgb(1, 0, 0) cross in the above picture.
4.3 AABB The axis aligned bounding box (AABB) of a region is the smallest rectangle whose sides align with the x- and y-axis that completely encloses the region. We can access it like so:
std::vector&amp;lt;Vector2ui, 4&amp;gt; aabb = pepper.get_axis_aligned_bounding_box(); Where the resulting vertices of the box are in the following order: top-left, top-right, bottom-right, bottom-left.
The value of the vertices of the rectangle are relative to the top-left corner of the image the region is from, translating them to the origin is a trivial operation.
Where the AABB is shown in white. Note that the boundary intersects with the AABB, this is because regions in crisp are closed regions.
4.4 Major &amp;amp; Minor Axis The major and minor axis of a region are formally defined as the major- and minor axis of the ellipses described by the eigenvectors multiplied with their respective eigenvalues of the positional covariance matrix of the boundary of the region. It&amp;rsquo;s not important to understand what this means, we can think of the major and minor axis as the &amp;ldquo;orientation&amp;rdquo; of the data spread of a region shape, where the major axis is along the highest variance (in terms of spacial position), the minor axis is perpendicular to it and both axis&amp;rsquo; intersect with the centroid. We access the minor and major access using:
const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_major_axis() const; const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_minor_axis() const; Each of the axis is given as two point. Visualizing the axes properly is difficult, as they are represented with sub-pixel precision. By scaling the image of the pepper, we can get an idea of what they look like:
Where magenta is the major, green the minor axis and the ellipses modeled by them shown in yellow. We can translate the ellipses so it&amp;rsquo;s axes align with the coordinate systems axes. The ratio of the resulting axes gives us another scale-invariant, rotationally-invariant and translationally-invariant region descriptor.
The major- and minor axis are useful for many other things, for example aligning regions to each other or for registration.
4.5 Eccentricity Using the minor and major axis we can compute the region&amp;rsquo;s eccentricity, which quantifies how &amp;ldquo;conical&amp;rdquo; the region is. If the eccentricity is 0 the shape modeled by the major and minor axis is a perfect circle, the closer to 1 the eccentricity is, the more elliptical the region.
We can access the eccentricity using ImageRegion::get_eccentricity().
4.6 Circularity While eccentricity measures how closely the shape of a region approximates a non-circular ellipse, circularity quantifies how closely the shape approximates a perfect circle. Regions, which tend to have smooth features and not many sharp edges, tend to have high circularity towards 1, while angular shapes or shapes with a high eccentricity tend to have a circularity closer to 0.
We can access circularity using ImageRegion::get_circularity().
4.7 Holes While already mentioned, it may be instructional to define what a hole is formally. A hole is an area of pixels who are not part of the region, whose boundary is entirely enclosed by the region. Intuitively this means, if you image the region as land and everything else as water, a hole would be a lake inside the region with no connection to the &amp;ldquo;ocean&amp;rdquo; that is surrounding the region.
When boundary tracing, crisp implicitly computes the boundaries of each hole and, thus, also the number of holes. We can access either with no overhead using:
size_t get_n_holes() const; const std::vector&amp;lt;std::vector&amp;lt;Vector2ui&amp;gt;&amp;gt; get_hole_boundaries() const; Where the hole boundaries are ordered corresponding to their respective top-most, left-most pixel coordinate.
4.8 Moment Invariants We&amp;rsquo;ve seen earlier that somehow representing a region&amp;rsquo;s unique shape in a way that is invariant to scale, translation and rotation is highly valuable. A very powerful way to do this is using the n-ths moment invariant. While some of them have a conceptual meaning, for beginners it is best to just think of them as properties that may not be useful to humans but do represent the shape of a region uniquely. crisp offers the first 7 moment invariants, also known as Hu Moment Invariant (Hu, 1962), accessed using ImageRegion::get_nths_moment_invariant(size_t n) where n in {1, 2, 3, &amp;hellip;, 7}.
The following table summarizes the response of a moment invariant to translation, scale, rotation and mirroring. &amp;ldquo;Unchanged&amp;rdquo; means the value of the moment does not change when recomputed after the operation.
   N translation scale rotation mirroring     1 unchanged unchanged unchanged unchanged   2 unchanged unchanged unchanged unchanged   3 unchanged unchanged unchanged unchanged   4 unchanged unchanged unchanged unchanged   5 unchanged does change unchanged unchanged   6 unchanged does change unchanged does change   7 unchanged does change slight change changes sign    We note that all first 4 moment invariants are completely independent of translation, scale, rotation and mirroring of the region and are thus highly valuable in representing a region.
5. Texture Descriptors So far, our descriptors dealt with the region&amp;rsquo;s boundary, shape or the values taken directly from the original image. In this section, we will instead deal with the region&amp;rsquo;s texture. This construct has not agreed on definition, in crisp texture refers to the distribution of intensity values in the region. Where the intensity of a pixel is the mean over all planes of that pixel (stored as _intensitiy in ImageRegion::Element, if you recall). An easier way to express quantifying texture in crisp is, that we&amp;rsquo;re converting our region to grayscale, then construct a histogram using those grayscale value and use statistical techniques to describe the distribution modeled by the histogram.
5.1 Intensity Histogram To get a rough idea of what the distribution of a region&amp;rsquo;s intensity looks like, ImageRegion offers get_intensity_histogram(). In this histogram, the intensity values are quantized into 256 intensities. This is not the case internally, however, the loss of detail in the histogram is not representative of the values computed for texture descriptors.
auto hist = pepper.get_intensity_histogram(); auto hist_img = hist.as_image(); We note that the histogram has many blank spots, which means that not all intensities were represented. The histogram exhibits multiple large spikes around the 0.5 region, these correspond to many of the constant-colored regions of the pepper. Lastly, we note that a lot of intensities were seemingly only represented once as this would account for the long tail of the distribution, the left tail extends all the way to 0 while the right tail stops at about 0.6.
5.2 Maximum Response The maximum response is the probability of the intensity with the highest number of observations occurring. The closer to 1 this value is, the more likely is it that the region has only very few shades in intensity.
We access it using get_maximum_intensity_probability() which for the pepper region returns 0.57. This is relatively high, which makes sense, because most of the pepper is the same shade of green. The high probability is represented by the huge spike in the histogram.
5.3 Mean, Variance Two of the most basic descriptors of a data set (a set of intensities in our case) are mean and variance. We can access them using:
auto mean = pepper.get_mean(); auto stddev = sqrt(pepper.get_variance()); Our pepper region&amp;rsquo;s texture has a mean of 0.45 (which corresponds to the red line in the histogram) and a variance of 0.015, which is very low. This is expected as, again, most of the pepper is shades of green that do not vary greatly when converted to grayscale.
5.4 n-ths Pearson Standardized Moment around the Mean In statistics, a distribution of random variables can be quantified using statistical moments. Each moment has an order called n. The first four moments have a human interpretable meaning:
 the 0ths standardized moment is always 1 the 1st standardized moment is the mean difference between the mean and itself, which is always 0 the 2nd standardized moment is the mean ratio of the variance to itself, which is always 1 the 3rd standardized moment is called skewness, which is a measure of how much a distribution leans to one side the 4th standardized moment is called kurtosis, which is a measure of how long, which tail of the distribution is.  Higher order moments may not have immediate use in human interpretation, but can be very useful for machine-learning application. All moments for n &amp;lt; 7 are used commonly. We can access the n-ths moment using get_nths_moment(size_t n).
The 3rd and 4th moment can furthermore be accessed directly using get_skewness() and get_kurtosis(). To put these values into context, let&amp;rsquo;s again inspect our intensity histogram:
Because of the large number of singleton intensity occurrences towards 0, the distribution overall leans to the left. This is reflected in the skewness which is -2.20965. Negative values generally mean the distribution leans left, right for positive values. The kurtosis of the pepper&amp;rsquo;s texture is 8.74199 which is very high. This again was expected because both tails are very long and thin, resulting from the high number of singletons.
5.5 Average Entropy The average entropy is a measure of how ordered the set of intensities is. We can compute its value using get_average_entropy() which returns 0.769 for the pepper region. crisp normalizes the values into [0, 1] so ~0.77 is relatively high. This is expected, the pepper is mostly green and has large regions of constant intensity, so we would expect it to be highly ordered.
5.6 Co-Occurrence Matrix To quantify texture in a specified direction, we can construct what is called the co-occurrence matrix. The co-occurrence matrix is a matrix of size 256x256. It counts for each intensity pair i_a, i_b the number of times two pixels a, b that are next to each other in a specified direction have the corresponding intensities i_a, i_b. A short example: if the co-occurrence matrix for the &amp;ldquo;right&amp;rdquo; direction has a value of 6 in the row 120 and column 98 then in the image there are 6 pairs of pixels a, b such that a has intensity 120 and b who is directly right of a has intensity 98 Recall that the intensities are quantized and projected from [0, 1] (float) to [0, 256] (int) to keep the size of the co-occurrence matrix manageable.
When constructing the co-occurrence matrix, we need to first supply a direction. Let a = (x, y) and b = (x &#43; i, y &#43; j) then the values of the crisp::CoOccurrenceDirection enum have the following meaning:
Direction a b ------------------------------------- PLUS_MINUS_ZERO (x,y) (x, y-1) PLUS_45 (x,y) (x&#43;1, y-1) PLUS_90 (x,y) (x&#43;1, y) PLUS_125 (x,y) (x&#43;1, y&#43;1) PLUS_MINUS_180 (x,y) (x, y&#43;1) MINUS_125 (x,y) (x-1, y&#43;1) MINUS_90 (x,y) (x-1, y) MINUS_45 (x,y) (x-1, y-1) In crisp we can render the matrix like so:
auto co_occurrence_matrix = pepper.get_co_occurrence_matrix(CoOccurrenceDirection::PLUS_90); auto as_image = GrayScaleImage(co_occurrence_matrix); // bind or save to disk The above image was log-scaled for clarity. Firstly, we note that most of the cells are left black. This points to the pepper&amp;rsquo;s texture only having a relative small amount of pair-wise different intensity pairs, which, looking at the original image, is indeed the case. Secondly, we note that most of the high-occurrence pairs (those with a lighter pixel color in the above image) are clustered around the matrix&amp;rsquo; trace. This is evidence of high uniformity, as an occurrence along the trace means, that the intensity pair {i_a, i_b} that occurred had identical values i_a == i_b. Recall that these observations only make sense in respect to the direction of the co-occurrence matrix, which in our case is &#43;90° (from left to right).
We can numerically quantify the distribution of intensity value pair occurrences using the following descriptors:
5.7 Intensity Correlation Intensity Correlation measures how correlated the intensities in each intensity value pair are. Similar to the pearson correlation coefficient it has a value in [-1, 1] where -1 means a strong negative correlation, &#43;1 means a strong positive correlation (either of which usually mean there is a regularity to the pattern) and 0 means no correlation. A pattern that has 0 correlation could be random noise or of constant intensity.
We can compute the mean intensity correlation using get_intensity_correlation(CoOccurrenceDirection). In our example for the left-to-right (&#43;90°) direction, this returns 0.4868. This means the texture exhibits above average positive correlation in the left-to-right direction. Looking at the image of the pepper closely, we, note that the lighter spots coming from lighting are more present on the right side while the left side is more in the shadow. This explains the positive (increasing) intensity correlation when looking at the texture from left to right.
5.8 Homogeneity In the co-occurrence matrix, the diagonal represents occurrences of intensity pair i_a, i_b where i_a = i_b, occurrences where two pixels in the specified direction have the same intensity. Homogeneity quantifies, how many of the intensity pairs are located near this diagonal. The higher the homogeneity, the more common pixel neighbors with the same or similar intensity are.
We can access the value using get_homogeneity(CoOccurrenceDirection), it returns a float in [0,1]. The pepper region exhibits a homogeneity of 0.625. This may be slightly lower than expected considering the pepper is all green, however remember that we are quantifying texture, not color. The intensity (lightness) of the shades of green do vary quite a bit, even though the hue does not. Nonetheless, 0.625 would be considered far above average, so we would call the pepper a fairly homogenically textured region.
5.9 Entropy Similar to the average entropy, we can also compute the entropy of the co-occurrence matrix. This can be thought of as a descriptor of how ordered the occurrence-pair distribution in that direction is. Using get_entropy(CoOccurrenceDirection) we compute a value of 0.36 (again normalized into [0, 1]).
5.10 Contrast Contrast measures the difference in value between co-occurring pixels. A white pixel next to a black pixel (or vice-versa) would have maximum contrast, while two pixels of identical color would have 0 contrast. We can compute the mean contrast in the specified direction using get_contrast(CoOccurrenceDirection), as already mentioned, its values are in [0, 1].
Our pepper has a contrast of 0.0002 which is extremely low, again this is expected, the shades of green transition into each other smoothly, as there are no big jumps in intensity. Large parts of the pepper have constant regions where neighboring pixels have the same intensity.
]]></content:encoded>
    </item>
  </channel>
</rss>
