<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title >Posts on Clemens Cords&#39; Homepage</title>
    <link>http://clemens-cords.com/post/</link>
    <description>Recent content in Posts on Clemens Cords&#39; Homepage</description>
    <image>
      <title>Posts on Clemens Cords&#39; Homepage</title>
      <link>http://clemens-cords.com/post/</link>
      <url>http://clemens-cords.com/rat_icon.png</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.68.3)</generator>
    <language>en-US</language>
    <lastBuildDate>Sun, 06 Mar 2022 19:49:27 UT</lastBuildDate>
    <atom:link href="http://clemens-cords.com/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Deep Learning Aided Differentiating of Chicken from Duck Eggs using Simulated Data</title>
      <link>http://clemens-cords.com/post/crisp_deep_learning/</link>
      <pubDate>Tue, 05 Oct 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_deep_learning/</guid>
      <description>(This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps deep learning module that is also available here. The examples used for illustrating crisps functionality also illustrate general usage and design of both the Fully Connected Neural Networks and Bayes Classification which is why the post has been reposted here.)
  Feature Classification &amp;amp; Deep Learning Bayes Classifier, Fully Connected Neural Networks, Convolutional Neural Networks, SIFT
#include &amp;lt;classification/bayes_classifier.hpp&amp;gt;#include &amp;lt;classification/sift.hpp&amp;gt;#include &amp;lt;classification/fully_connected_neural_networks.hpp&amp;gt;#include &amp;lt;classification/convolutional_neural_networks.hpp&amp;gt; // all of the above collected in: #include &amp;lt;classification.hpp&amp;gt;Table of Contents  Introduction 1.1 Features, Class Membership
 Bayes Statistical Classifier
2.1 Training the Bayes Classifier
2.2 Using the Bayes Classifier for Identification
 Neural Networks
3.1 Architecture
3.2 Weights and Biases
3.3 Creating/Loading a Network
3.4 Training the Network
3.5 Using the Network for Identification
  1. Introduction We learned how to extract part of an image into a segment, then compute unique descriptors that describe the selected region&amp;rsquo;s shape and texture. Now, it is finally time to use these descriptors to solve one of the most common image processing applications: classification.
To classify an object means to divide the population it is from into a number of classes {C_0, C_1, ..., C_m-1}, then assign a label to any one specific object. This label represents what class it belongs to. For the sake of simplicity, for the rest of this tutorial, we assume that an object can only belong to exactly one class. While this assumption is not necessary for any of crisps deep learning features, it will make things easier to follow later.
1.1 Features &amp;amp; Class Membership For one specific region or object, class membership is given as a m*1 matrix M:
   class index i M(i, 0)     0 {0, 1}   1 {0, 1}   &amp;hellip; &amp;hellip;   m-1 {0, 1}    Where if M(4, 0) == 1 then the object is classified into class C4. Because an object can only belong to one class at a time, M.sum() == 1 (as all values except for the single 1 are 0 and an object has to belong to at least one class).
In order to be able to classify an object we need to describe it somehow, we do this using a n*1 matrix called a feature vector. Each row of this matrix N corresponds to one feature:
   feature index N(i, 0)     0 [-1, 1]   1 [-1, 1]   &amp;hellip; &amp;hellip;   n-1 [-1, 1]    When classifying multiple objects from the same population, all objects in that population have to have a feature vector of the same size and meaning, that is for each object, each row means the same thing. All that changes is the actual values of the features. As we saw in the table above, it is necessary to normalize the value of the feature into the range [-1, 1]. In the literature, features are often normalized into [0, 1] but because this is a subset of [-1, 1] anyway, either approach works in crisp.
A training data set in crisp is a set of feature vectors and their corresponding, desired classifications. In practice, this set is represented by two matrices:
  FeatureMatrix_t is a n*x matrix where n the number of features and x the number of samples in the population. Each row is one feature, each column is one sample. All its values should be normalized into `[-1, 1]
  ClassificationMatrix_t is a m*x matrix where m is the number of classes and x the number of sample. Each row is one class, each column is one sample. All elements of a row should be either 1 or 0
  The number of columns, x, has to be identical for both matrices. If one object has its feature vector as the column of the feature matrix at position i, the iths column of the classification matrix corresponds to that same object.
It may be instructive to consider a practical example. Consider the following data set:
   sample class egg weight (g) yolk to white ratio     0 Chicken 52 31%   1 Chicken 58 34%   2 Chicken 67 32%   3 Chicken 62 37%   4 Chicken 59 32%   5 Chicken 65 29%         6 Duck 61 36%   7 Duck 63 35%   8 Duck 69 39%   9 Duck 60 38%   10 Duck 71 37%   11 Duck 75 34%    Here we have a set of 12 eggs from both chickens and ducks. Each egg sample has 3 data point: whether it was from a chicken or duck, it&amp;rsquo;s weight, and it&amp;rsquo;s yolk-to-white-ratio. We want to create a classifier can decide whether a new, unknown egg we hand it is from a chicken or duck.
Before we can use crisp for classification, we need to arrange the data in a way that crisp understands.
We have m = 2 classes and n = 2 features. We are trying to predict the &amp;ldquo;class&amp;rdquo; property of each egg, so we create a matrix with 2 rows, each column of the matrix has a single 1 depending on whether the sample was from a chicken or duck.
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck Each egg sample gets a 2x1 feature vector that is 1 in the first row if it is from a chicken, or 1 in the second row if it is from a duck.
We transform the feature values in a similar way. Each column corresponds to one sample (egg), we arbitrarily decide that weight will be in row 0, yolk-ratio in row 1. Recall that features need to be normalized, so we divide both feature values by 100:
auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); // sample: 0 1 2 3 4 5 6 7 8 9 10 11 features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yok ratio We call the first matrix class membership matrix and the second a feature matrix.
2. Bayes Statistical Classifier The simplest classifier in crisp is crisp::BayesClassifier. This class has two template parameters:
template&amp;lt;size_t FeatureN, size_t ClassN&amp;gt; class BayesClassifier {  FeatureN is the number of features (n in our notation) ClassN is the number of classes (m in our notation)  Both need to be available at compile time.
The classifier works by estimating the type of distribution, mean and variance from a given population. When we hand it a sample x to identify the classifier will return a score that is directly proportional to, in our chicken-or-duck example, the following probabilities:
p( x | chicken) // is our current egg x from a chicken p( x | duck) // is our current egg x from a duck The return values of the classifier are not actual probabilities (they can be outside the range [0, 1]), however the order and relative value of the actual probabilities are preserved. This means we can compare and quantify them, just like we would probabilities.
2.1 Training the Bayes Classifier To estimate the values needed for classification, the bayes classifier needs to observe the training set population. We retrieve the feature- and classification matrix from earlier, then call BayesClassifier::train:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk  auto bayes = BayesClassifier&amp;lt;2, 2&amp;gt;(); // 2 features, 2 classes bayes.train(feature, classes); That&amp;rsquo;s all, unlike deep learning procedures later in this section, the bayes classifier only needs to iterate through the training set once to achieve the maximum possible classification performance based on the evidence.
2.2 Identification using the Bayes Classifier Sticking to our chicken-or-duck example, let&amp;rsquo;s say we just came home and found 3 eggs around our hypothetical farm and we really want to know whether it was the chickens or the ducks that hid them. We measure their size and content:
   egg weight yolk ratio     0 51 29%   1 67 37%   2 70 38%    We arrange these three samples just like we did with our training datas feature matrix:
auto to_identify = Eigen::MatrixXf(2, 3); // 0 1 2 // egg to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio We can then classify them using:
auto result = bayes.identify(to_identify); std::cout &amp;lt;&amp;lt; result &amp;lt;&amp;lt; std::endl; 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 We get a bunch of numbers, let&amp;rsquo;s try to understand what they mean. Firstly, each column index corresponds to their respective egg sample:
// 0 1 2 0.667373 0.500235 0.0936609 0.332627 0.499765 0.906339 The numbers are scores representing the relative probability that the sample in column i is from class j where j is the row index. In our training data set, we had chickens as j = 0 and ducks as j = 1, so:
// 0 1 2 0.667373 0.500235 0.0936609 // ~p(egg | chicken) 0.332627 0.499765 0.906339 // ~p(egg | duck) Where ~p(x | y) is a score that&amp;rsquo;s directly proportional to the probability p(x | y).
We decide which egg came from which species by simply comparing the probabilities, choosing the class membership with the higher score.
Technically we would assign egg 0 and 1 as chicken,3 as duck, however the result for egg 1 is so ambiguous that it is basically a coin toss. To quantify this ambiguity, we assign each result a confidence score. One of the simplest confidence score is the absolute distance between the values:
auto to_confidence = [](float a, float b) { assert(a &amp;gt;= 0 and a &amp;lt;= 1 and b &amp;gt;= 0 and b &amp;lt;= 1); return abs(a-b); }; Because our classification output is always in [0, 1], the higher the distance between the two probabilities, the more certain we can be that we did not make a classification error.
We summarize our results as such:
   egg ~p(chicken) ~p(duck) classification confidence     0 0.667 0.332 chicken 0.33 (medium)   1 0.5 0.499 chicken 0.001 (very low)   2 0.09 0.90 duck 0.84 (high)    3 Neural Networks While the Bayes Classifier is useful and can lead to quite decent results with very little computational cost, some classification processes require a more flexible approach. Since the 1970s, the state-of-the-art way of solving classification problems in a semi-supervised way has been neural networks. This tutorial will not cover how they work in detail, instead we will learn just enough fundamentals to understand how to use crisp::NeuralNetwork and how to interpret its results.
3.1 Architecture A neural network has l layers. Each layer L_i has a number of neurons #L_i.
 L_0 is called the input layer. This layer has the same number of neurons as the feature vector has rows: #L_0 = n L_l is called the output layer. This layer as the same number of neurons as the number of possible classes: #L_l = m L_i for i in {1, 2, ..., l-1} are called hidden layers  Where n the number of features, m the number of classes (c.f. Section 1).
We call the number of layers and the number of neurons in each layer a network&amp;rsquo;s architecture. We define the architecture in crisp using the template arguments of crisp::NeuralNetwork:
template&amp;lt;size_t... NeuronsPerLayer&amp;gt; class NeuralNetwork; auto nn = NeuralNetwork&amp;lt;n, #L_1, #L_2, ..., m&amp;gt;(); Where n, m, #L_i as defined above. For example:
// binary feature vector: n = 2 // two possible classes: m = 2 // one hidden layer with 3 neurons: #L_1 = 3 auto nn = NeuralNetwork&amp;lt;2, 3, 2&amp;gt;(); // 20 features, 2 hidden layers with 3 neurons each, 2 classes auto nn = NeuralNetwork&amp;lt;20, 3, 3, 2&amp;gt;(); // 2 features, 5 layers with 10 neurons each, 19 classes auto nn = NeuralNetwork&amp;lt;2, 10, 10, 10, 10, 10, 19&amp;gt;(); A network&amp;rsquo;s architecture governs its structure. For the same problem, different architectures can result in vastly different results. Furthermore, the computational complexity (in terms of runtime performance) will of course increase with the total number of neurons. In praxis choosing the correct architecture is somewhat of an art, while heuristics exist in the literature it is often important to experiment.
3.2 Weights and Biases Each neuron has a number of weights and one bias. The number of weights is equal to the number of neurons in the previous layer, for example if we&amp;rsquo;re in layer L_2, all our neurons will have #L_1 weights and a single bias. The weights and biases govern a neuron&amp;rsquo;s output, thus (along with architecture) they completely define a network&amp;rsquo;s performance in terms of classification.
3.3 Creating a Network We can construct a fresh, untrained network like so:
#include &amp;lt;classification/fully_connected_neural_network.hpp&amp;gt; // in main.cpp auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2&amp;gt;(); This initializes all weights to 1 &#43; eps and all biases to 0 &#43; eps, where eps ∈ [-0.25, &#43;0.25] (random, uniformly distributed). Once our network is trained, we will want to export it to the disk to avoid having to retrain it every time the application launches. We can convert a network into a string like so:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto exported = nn.as_string(); // save exported to file We can then simply save the string to a file or otherwise store it. At a later point, we can then load the neural network from the disk using:
auto nn = crisp::NeuralNetwork&amp;lt;2, 2, 2, 2&amp;gt;(); auto file = std::ifstream(/*...*/ &#43; &amp;#34;/crisp/docs/feature_classification/chicken_or_duck_nn_2_4_4_2.txt&amp;#34;); std::stringstream buffer; buffer &amp;lt;&amp;lt; file.rdbuf(); std::string str = buffer.str(); nn.from_string(str); Here we&amp;rsquo;re opening a file stream, moving its content into a buffer stringstream and then create a plain string from said stringstream. The neural network can then parse the string it generated on a previous occasion and load the weights and biases. This is somewhat clumsy owing to C&#43;&#43;s file interface so feel free to save the string generated using as_string() as you see fit.
Note that for loading to be successful, the exported network and the network we&amp;rsquo;re trying to import have to have identical architectures. Trying to import the exported string of a network with a different architecture will trigger an exception. It is therefore good practice to state the network&amp;rsquo;s architecture in the filename (as in the above example).
3.4 Training the Network Recall our egg example from section 1:
auto classes = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); classes &amp;lt;&amp;lt; 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, // is chicken  0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1; // is duck  auto features = Eigen::Matrix&amp;lt;2, 12&amp;gt;(); features &amp;lt;&amp;lt; .52, .58, .67, .62, .59, .65, .61, .63, .69, .60, .71, .75, // weight  .31, .34, .32, .37, .32, .29, .36, .35, .39, .38, .37, .34; // yolk We will use this data to train our network. Unlike the bayes classifier, neural networks will have to complete many training cycles. Each cycle of going through the entire training data set once is called an epoch.
We can conduct a single epoch like so:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(/*...*/); nn.train(features, classes); Neural Networks work in a way that is guaranteed to converge towards a set of optimal class boundaries (functions that divide the feature space into the given classes) with enough epochs. Here, &amp;ldquo;optimal&amp;rdquo; means, that we&amp;rsquo;re minimizing the mean square error. We can directly compute the error of the current network state like so:
nn.compute_mean_squared_error(feature, classes); Here, we are not computing the error between feature and classes, rather, the neural network generates a classification matrix from feature, then measures the error between this result and classes.
The errors for the first few epochs with our egg data and the architecture from the above example:
   epoch maximum MSE     1 2.42762   2 2.08903   3 0.778278   4 0.675251    We see that the error is already decreasing rapidly. Depending on the architecture and problem, the mean square error may actually increase at the beginning, but then slowly stabilize and go back down as more epochs are completed.
If we want to only train for a certain number of epochs, we use:
for (size_t n_epochs = 0; n_epochs &amp;lt; 10000; &#43;&#43;n_epochs) nn.train(features, classes); If we have no idea how many epochs are needed to get to an acceptable classification error, we can instead use NeuralNetwork::train_until. It has 3 arguments: the feature matrix, the classification matrix and a mean square error threshold. If the network observes the current classification error to be stable and below that threshold, the function exits and returns the number of epochs it took:
auto n_epochs = nn.train_until(features, classes, 0.15); std::cout &amp;lt;&amp;lt; n_epochs &amp;lt;&amp;lt; std::endl; The number of epochs is only useful for monitoring and can often be discarded. For our egg training data, it took 1059191 many epochs to get below the threshold of 0.15. This is quite a lot, one parameter that governs the convergence behavior is called the learning constant (often called μ or α). This is a value in [0, 1] that we can optionally hand to the network on construction or by setting it later using set_learning_constant:
float alpha = 0.001; auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(alpha); // or nn.set_learning_constant(alpha); Usually values will be between 0.0001 and 0.1, however just like the network&amp;rsquo;s architecture, finding a good learning constant will require a lot of experimentation. For badly chosen values, the network can even begin to diverge, meaning the error will increase.
Now that our network is trained, we should export it, so we don&amp;rsquo;t have to do another 1059191 epochs. The weights and biases for the above example with alpha = 0.001 were:
1 0 7.30575 -1.03484 2.03974 -5.31141 5.80577 1.88958 6.59601 4.74493 -0.485592 1.42384 1.80752 2.70784 5.16637 3.31374 4.56878 -0.904903 1.88335 1.32096 1.80904 0.728481 5.68054 -0.569291 2.96311 1.78811 2.837 -0.384504 1.49958 6.98229 0.633096 1.17761 0.2897 -1.09701 -1.95467 3.00161 3.54201 -0.634866 -1.76444 -2.18135 1.18332 -0.43307 1.34111 0.527556 The first number will always be 1, this is the single and only weight of the input layer. It is always followed by a 0 which is the bias of the input layer. All following number correspond to the weights and biases of the subsequent layers.
3.5 Using the Network for Classification Now that our network is well-trained and the mean squared error is low, we should compute its classification error. Unlike the MSE, the classification error is defined as the percentage of false-positive or false negatives when classifying a data set for which the actual correct classifications are known.
We first load the network into memory using the string we generated earlier, then we classify the entire feature matrix from the training set:
auto nn = NeuralNetwork&amp;lt;2, 4, 4, 2&amp;gt;(); nn.from_string(/*...*/); std::cout &amp;lt;&amp;lt; classes &amp;lt;&amp;lt; std::endl; std::cout &amp;lt;&amp;lt; nn.identify(features) &amp;lt;&amp;lt; std::endl; 0.99638 0.993424 0.988063 0.856026 0.998413 0.999656 -0.227162 0.343421 0.000525594 -0.426658 -0.00993148 -0.00718939 -0.0112014 0.434531 0.715863 0.272235 -0.189113 0.0802148 0.891286 0.900159 0.924364 0.830156 0.925139 0.924666 Like with the bayes classifier, each of the rows correspond to one class (chicken or duck), each column corresponds to one sample (egg). We classify an egg as chicken if the value in row 0 is higher, as a duck if the value in row 1 is higher.
We get the following result, where E(x) is the expected value of x as designated in the training sets classification matrix M
   sample Chicken E(Chicken) Duck E(Duck) correct     0 0.99638 1 -0.0112014 0 yes   1 0.993424 1 0.434531 0 yes   2 0.988063 1 0.715863 0 yes   3 0.856026 1 0.272235 0 yes   4 0.998413 1 -0.189113 0 yes   5 0.999656 1 0.0802148 0 yes   6 -0.227162 0 0.891286 1 yes   7 0.343421 0 0.900159 1 yes   8 0.000525594 0 0.924364 1 yes   9 -0.426658 0 0.830156 1 yes   10 -0.00993148 0 0.925139 1 yes   11 -0.00718939 0 0.924666 1 yes    We see that the neural network was able to classify all samples correctly. The least confident sample from this population was egg 2, while the most confidently classified was egg 5, where confidence is defined as in section 2.
Now that we know our neural network works decently well, we can use it to identify the three unknown eggs:
auto to_identify = Eigen::MatrixXf(2, 3); to_identify &amp;lt;&amp;lt; .51, .60, .70, // weight  .29, .28, .38; // yolk ratio  auto results = nn.identify(to_identify); // 0 1 2  -0.021921 0.8932 -0.0119139 // chicken  0.926603 0.918492 0.925525 // duck Just like with bayes classifier, eggs 0 and 2 were identified confidently. Egg 1 was previously undetermined as the bayes classifier assigned both options the same score, our neural network however leans more towards duck, if only by a difference of 0.02. This result is still not very confident, but it&amp;rsquo;s at least possible to choose one or the other.
]]></content:encoded>
    </item>
    <item>
      <title>Describing a Regions Shape and Texture with Descriptors intended for Deep Learning Applications</title>
      <link>http://clemens-cords.com/post/crisp_feature_extraction/</link>
      <pubDate>Mon, 27 Sep 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/crisp_feature_extraction/</guid>
      <description>(This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region</description>
      <category domain="http://clemens-cords.com/categories/deep-learning">Deep Learning</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[
 (This is a mirror of the documentation of crisps feature extraction tutorial also available [here](here. It has been reposted on this blog because it also illustrates many of the techniques available to describe both an image regions shape and texture for classification via deep learning.)
  Feature Extraction Image Regions, Region Descriptors, Boundary Tracing, Signatures, Pattern Descriptors
#include &amp;lt;image_region.hpp&amp;gt;Table of Contents  Introduction
1.1 Extracting a Region
 Region Boundary
2.1 Definition
2.2 Boundary Polygon
 Boundary Signature
3.1 Vertex Polygon
3.2 Slope Chain Code Signature
3.3 Radial Distance Signature
3.4 Complex Coordinate Signature
3.5 Farthest Point Signature
 Whole Region Descriptors
4.1 Area, Perimeter, Compactness
4.2 Centroid
4.3 Axis Aligned Bounding Box
4.4 Major and Minor Axis
4.5 Eccentricity
4.6 Circularity
4.7 Holes
4.8 N-ths Moment Invariant
 Texture Descriptors
5.1 Intensity Histogram
5.2 Maximum Intensity Response
5.3 Mean, Variance
5.4 N-ths Pearson Standardized, Centralized Moment
5.5 Average Entropy
5.6 Co-Occurrence Matrix
5.7 Intensity Correlation
5.8 Homogeneity
5.9 Directed Entropy
5.10 Contrast
  1. Introduction In the tutorial on segmentation, we discussed how to extract part of an image. Now, we will find out what to actually do with said segment. Recall that, in crisp, an image segment is a set of pixel coordinates Vector2ui:
using ImageSegment = std::set&amp;lt;Vector2ui, /*...*/&amp;gt;; crisp::ImageSegment holds no information about anything other than the pixels coordinates. This allows for a certain degree of generality, we can use a segment as if it were part of many images. The only restraint on those images is, that they have to be at least the size of the segment.
We can apply any function only to a segment of an image, like so:
auto lambda_operator = []&amp;lt;typename Image_t&amp;gt;(size_t x, size_t y, const Image_t&amp;amp; image) -&amp;gt; typename Image_t::Value_t { auto&amp;amp; value = image(x, y); // some transformation  return value; }; ImageSegment segment = /*...*/; auto image = /*...*/; for (const auto position : segment) image(position.x(), position.y()) = lambda_operator(position.x(), position.y(), image); Here, we first define a templated lambda. This function takes the pixel coordinate and the corresponding image as arguments, reads the pixel&amp;rsquo;s value, transforms it in some way, then assigns it back to the image.
While this works well when the entire image is available, sometimes we really don&amp;rsquo;t need the rest of the image. For just this purpose, crisp defines crisp::ImageRegion:
template&amp;lt;typename Image_t&amp;gt; class ImageRegion { using Value_t = Image_t::Value_t; public: /* ... */ private: struct Element { Vector2ui _position; Value_t _value; float _intensity } std::set&amp;lt;Element, /*...*/&amp;gt; _elements; } We see that instead of just pixel coordinates, ImageSegment holds a set of ÌmageSegment::Elements. Each element has 3 members: the original pixel coordinate _position, the original value of the corresponding pixel in the image _value, and _intensity which is the mean of all planes of _value. We will use _intensity extensively in the texture descriptor chapter, but for now, it&amp;rsquo;s enough to remember that ImageSegment holds only pixel coordinates while ImageRegion holds those coordinates as well as deep-copies of the pixels values.
We construct an ImageRegion from an image and an ImageSegment like so:
// in main auto image = /*...*/; // crisp::Image&amp;lt;T, N&amp;gt; for any T, N auto segment = // some segmentation algorithm that returns crisp::ImageSegment  auto region = ImageRegion(); region.create_from(segment, image); // or equivalently: auto region = ImageRegion(segment, image); Once create_from is called, all pixel intensities are copied, so we are free to deallocate the original image or change it without the values in the image region being affected. If we were to change the pixel values in the region while the original image is still in memory, it would be unaffected.
crisp offers a shortcut to convert an entire image into one big region:
auto region = ImageRegion(image); // or auto region = ImageRegion(); region.create_from(image); This comes in handy whenever we want functionality only available to ImageRegion but have no need to first segment the image.
1.1 An Example To better illustrate the entire process of loading an image, segmenting it, then extracting a region, consider this image of the &amp;ldquo;pepper brush&amp;rdquo; as provided by gimp Wanting to extract the region that has the pepper, we observe the background to be very dark. A simple manual thresholding operation is guaranteed to extract the correct region. We first load the image as color, then convert it to grayscale using Image::get_value_plane(size_t) (remember that the HSV &amp;ldquo;value&amp;rdquo; component is equal to the mean of all RGB color components):
#include &amp;lt;system/image_io.hpp&amp;gt;#include &amp;lt;image/grayscale_image.hpp&amp;gt;#include &amp;lt;segmentation.hpp&amp;gt;using namespace crisp; // in main.cpp auto as_color = load_color_image(/*...*/ &#43; &amp;#34;/crisp/docs/feature_extraction/pepper.png&amp;#34;); auto as_grayscale = as_color.get_value_plane(); auto thresholded = Segmentation::manual_threshold(as_grayscale, 0.01f); We then want to decompose the binary image into connected segments. After decomposition, the segments will be ordered according to their respective left-most, top-most pixels coordinate. The pixel at (0, 0) is black and there are only two segments (the pepper in white, and the background in black) thus we expect the pepper to be the second segment extracted:
auto segments = decomponse_into_connected_segments(thresholded); auto pepper_segment = segments.at(1); We can now construct the resulting region using the pepper segment and the original color images values:
auto pepper = ImageRegion(pepper_segment, image); This is why we loaded the image as color but thresholded the grayscale image. Constructing the region with the colored pixel values means no information is lost.
2. Region Boundary Mathematically, crisps regions are closed, simply connected regions. This basically means:
 all boundary elements are part of the region all elements are 4-connected  ImageRegion will throw an exception if the segment handed to it is not 4-connected. We can assure it is either by using decompose_into_connected_segments (for more information about this, visit the segmentation tutorial) or we can use decompose_into_regions(const ImageSegment&amp;amp;, const Image_t&amp;amp;) -&amp;gt; std::vector&amp;lt;ImageRegion&amp;gt; which is provided to automatically split a segment into 4-connected sub-segments and then constructs a region from each.
Now that we assured that our region is indeed 4-connected, we can concern ourselves with, in terms of feature recognition, one of the most important properties of a region: it&amp;rsquo;s boundary. In crisp boundaries have the following properties:
(a visual, less math-y exploration of these concepts will follow)
Let B = {b_0, b_1, b_2, ..., b_m} be the set of boundary points, then:
 i) for each b_i there exists a b_i-1, b_i&#43;1 in B such that b_i-1 is 8-connected to b_i, b_i&#43;1 is 8-connected to b_i and b_i-1 is not 8-connected to b_i&#43;1 ii) b_0 is 8-connected to b_m iii) the set B is minimal in terms of cardinality, that is if we were to remove any b_i in B, property i) or ii) would be violated  Because definitions can be hard to conceptualize, let&amp;rsquo;s consider a purely visual example to illustrate these concepts:
2.1 8-Connectivity and Minimal Cardinality Property i) and ii) mean the boundary is an unbroken chain of 8-connected pixels and that the boundary forms a path such that one can jump from b_0 to b_1, b_1 to b_2, etc. up to b_m-1 to b_m (the last point) and then, crucially, from b_m back to b_0 completing the circle.
Let&amp;rsquo;s again consider the region of our pepper:
A simple way of tracing its boundary would be to highlight all pixels that have at least one neighbor that is not in the region (black, in our case).
This is a boundary that fulfills condition i) and ii), however inspecting the boundary closely we notice many redundant points:
If we were to ask a human to remove as many points as possible without compromising conditions i), ii), we would get the following boundary (where necessary pixels are highlighted in magenta, redundant pixels in cyan)
This is what condition iii)s minimality represents, we want all pixels to be non-redundant. This vastly increases performance, as for our pepper example we go from 3854 pixels for our trivial boundary to only 472 pixels using a minimal boundary:
crisps proprietary boundary tracing algorithm assures that the computed boundary is always minimal. After creating the region, we can access it at any point (with no performance overhead) using get_boundary(). The pixels are ordered according to condition i) where the first pixel b_0 is the left-most, top-most pixel and any following pixels b_i : i &amp;gt; 0 are enumerated in counter clock-wise direction.
2.2 Boundary Polygon We can even further reduce the number of elements in the boundary by treating it as a polygon that has vertices and straight, non-intersecting lines connecting exactly two of the vertices in order of enumeration. Consider this part of our pepper boundary:
We note multiple straight lines, each of these lines can be represented by just two pixels at the start and beginning of the line, shown in green rgb(0, 1, 0) here:
Using this approach, we reduce the number of boundary points from 472 to only 193, without loosing any information. The information is retained by the fact that the polygon vertices are ordered in counter-clockwise direction, this way we know exactly where to draw the straight line to the next point if we wanted to reconstruct the full boundary.
Now that we reduced the entire information contained in the region in the shape of a pepper to just 193 pixels, we may think we are done but thanks to more math we can reduce it even further, while increasing the representations&amp;rsquo; generality.
3. Boundary Signatures A signature is a mathematical transform of the boundary points of a shape that aims to, in some way, make the description of the boundary more widely applicable. When referring to the signature in this section, we are referring to the transform of the boundary polygons vertex points, as these are the smallest set of points that still represent the region boundary with no loss of information.
One of the ways to make a signature applicable to more than just one image is making it independent of rotation, a signature that accomplishes this describes not only our upright pepper but all possible rotation of it at the same time. Another form of generality is scale invariance, meaning that the signature describes our pepper at scale 1 and the same pepper scaled by any factor &amp;gt; 0. Lastly, independence of translation means that it does not matter if we were to translate all points of the signature by a constant (x, y), the signature represents all of those peppers just the same. crisp offers a multitude of signatures that may or may not be invariant in multiple of the aspects described above.
3.1 Vertex Polygon          scale not invariant   rotation not invariant   translation not invariant    This is the simplest signature, as already mentioned, it reduces the boundary to the vertices of it&amp;rsquo;s polygon. It is neither scale, translationally, nor rotationally invariant, it is however the basis for all other signatures. We can access it using:
auto polygon_signature = pepper.get_boundary_polygon(); 3.2 Slope Chain Code Signature          scale invariant   rotation not invariant   translation invariant    We can generate the slope-chain-signature by iterating through all boundary polygon vertices, storing the angle of the line that connects our current polygon vertex to the next (recall that the vertices are ordered counter-clockwise). This makes it invariant to both translation and scale, however rotation would alter all the angles values, so it is not invariant in this aspect. One way to make it rotationally invariant is to instead save the delta of successive angles. crisps slope_chain_code_signature() does not do this automatically, it is however a trivial operation.
The angles are stored in radians in the same order as their vertices.
3.3 Radial Distance Signature          scale not invariant   rotation invariant   translation invariant    The radial distance signature is the distance of each vertex from the region&amp;rsquo;s centroid. The centroid of a region in crisp is defined as the mean of all boundary coordinates, and can be intuitively thought of as the center of mass of a hole-less region if all pixels have the same weight. Because we are measuring the absolute distance, this signature is not invariant to scale. We can generate it using std::vector&amp;lt;float&amp;gt; ImageRegin::radial_distance_signature() const.
3.4 Complex Coordinate Signature          scale invariant   rotation invariant   translation invariant    (as proposed by El-Ghazal, Basir, Belkasim (2007))
This signature transforms each point in the boundary polygon into a complex number, for a point (x, y) the signature of the point is the complex number x &#43; i*y where i is the imaginary constant. The x-coordinate is treated as the real part, the y-coordinate is treated as the imaginary part. While this signature itself is neither invariant to scale nor rotation, we can now fourier-transform the complex numbers and store the corresponding coefficients. This achieves scale, rotational and translational invariance.
We can access the raw complex coordinates using ImageRegion::complex_coordinate_signature(). We can then use crisp::FourierTransform to transform them into their fourier descriptors.
3.5 Farthest Point Signature          scale not invariant   rotation invariant   translation invariant    (as proposed also by El-Ghazal, Basir, Belkasim (2009))
This signature computes, for each boundary point, the maximum distance to any other boundary point. When used for fourier descriptors, it performs better than other boundaries mentioned here [1] and achieves translational and rotational invariance even before fourier transformation. We can generate it using ImageRegion::farthest_point_signature.
[1] (Y. Hu, Z. Li, (2013): available here
4. Whole Region Descriptors Signatures are a transform of a region&amp;rsquo;s boundary points (the vertices of its polygon, to be precise). This is useful in unique representing a regions&amp;rsquo; boundary, but it doesn&amp;rsquo;t describe the shape of it in any way. To compare two boundaries, we would have to come up with a distance measure that compares the signatures, which can be quite hard. Instead, we can rely on the field of mathematical topology to give us many, much simpler to compute properties of a boundary. While only one of these will not unique identify a region&amp;rsquo;s shape, using multiple descriptors along with a signature can lead to great results.
4.1 Area &amp;amp; Perimeter, Compactness One of the easiest descriptors are area, the number of pixels in a region, and perimeter, the length of the region&amp;rsquo;s boundary. Perimeter only takes into account the outermost boundary, increasing the number of holes in a region will decrease its area but leave its perimeter unchanged.
Area and perimeter are usually not very useful unless they are normalized, for example by quantifying the area&amp;rsquo;s compactness, which is equal to the square of the perimeter divided by the area. A region that has no holes will have maximum compactness, while a region with the same boundary, but many or very big holes has a lower compactness. We can access area, perimeter and compactness using:
float get_perimeter() const; float get_area() const; float get_compactness() const; The values for compactness are usually in [0, 2], for edge cases the value may be outside this interval, however.
4.2 Centroid A region&amp;rsquo;s centroid, in crisp, is defined as the mean of the coordinate values of its boundary points (note that all boundary points are weighted here, not just the boundary polygons vertices). In the literature, the centroid is sometimes defined as the average of all points in a region, so it is important to remember that crisp only uses the boundary: Adding holes to a region while leaving its boundary unchanged does not alter the position of the region&amp;rsquo;s centroid.
We can access a region&amp;rsquo;s centroid at any time using get_centroid().
Where the centroid is highlighted using a red rgb(1, 0, 0) cross in the above picture.
4.3 AABB The axis aligned bounding box (AABB) of a region is the smallest rectangle whose sides align with the x- and y-axis that completely encloses the region. We can access it like so:
std::vector&amp;lt;Vector2ui, 4&amp;gt; aabb = pepper.get_axis_aligned_bounding_box(); Where the resulting vertices of the box are in the following order: top-left, top-right, bottom-right, bottom-left.
The value of the vertices of the rectangle are relative to the top-left corner of the image the region is from, translating them to the origin is a trivial operation.
Where the AABB is shown in white. Note that the boundary intersects with the AABB, this is because regions in crisp are closed regions.
4.4 Major &amp;amp; Minor Axis The major and minor axis of a region are formally defined as the major- and minor axis of the ellipses described by the eigenvectors multiplied with their respective eigenvalues of the positional covariance matrix of the boundary of the region. It&amp;rsquo;s not important to understand what this means, we can think of the major and minor axis as the &amp;ldquo;orientation&amp;rdquo; of the data spread of a region shape, where the major axis is along the highest variance (in terms of spacial position), the minor axis is perpendicular to it and both axis&amp;rsquo; intersect with the centroid. We access the minor and major access using:
const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_major_axis() const; const std::pair&amp;lt;Vector2f, Vector2f&amp;gt;&amp;amp; get_minor_axis() const; Each of the axis is given as two point. Visualizing the axes properly is difficult, as they are represented with sub-pixel precision. By scaling the image of the pepper, we can get an idea of what they look like:
Where magenta is the major, green the minor axis and the ellipses modeled by them shown in yellow. We can translate the ellipses so it&amp;rsquo;s axes align with the coordinate systems axes. The ratio of the resulting axes gives us another scale-invariant, rotationally-invariant and translationally-invariant region descriptor.
The major- and minor axis are useful for many other things, for example aligning regions to each other or for registration.
4.5 Eccentricity Using the minor and major axis we can compute the region&amp;rsquo;s eccentricity, which quantifies how &amp;ldquo;conical&amp;rdquo; the region is. If the eccentricity is 0 the shape modeled by the major and minor axis is a perfect circle, the closer to 1 the eccentricity is, the more elliptical the region.
We can access the eccentricity using ImageRegion::get_eccentricity().
4.6 Circularity While eccentricity measures how closely the shape of a region approximates a non-circular ellipse, circularity quantifies how closely the shape approximates a perfect circle. Regions, which tend to have smooth features and not many sharp edges, tend to have high circularity towards 1, while angular shapes or shapes with a high eccentricity tend to have a circularity closer to 0.
We can access circularity using ImageRegion::get_circularity().
4.7 Holes While already mentioned, it may be instructional to define what a hole is formally. A hole is an area of pixels who are not part of the region, whose boundary is entirely enclosed by the region. Intuitively this means, if you image the region as land and everything else as water, a hole would be a lake inside the region with no connection to the &amp;ldquo;ocean&amp;rdquo; that is surrounding the region.
When boundary tracing, crisp implicitly computes the boundaries of each hole and, thus, also the number of holes. We can access either with no overhead using:
size_t get_n_holes() const; const std::vector&amp;lt;std::vector&amp;lt;Vector2ui&amp;gt;&amp;gt; get_hole_boundaries() const; Where the hole boundaries are ordered corresponding to their respective top-most, left-most pixel coordinate.
4.8 Moment Invariants We&amp;rsquo;ve seen earlier that somehow representing a region&amp;rsquo;s unique shape in a way that is invariant to scale, translation and rotation is highly valuable. A very powerful way to do this is using the n-ths moment invariant. While some of them have a conceptual meaning, for beginners it is best to just think of them as properties that may not be useful to humans but do represent the shape of a region uniquely. crisp offers the first 7 moment invariants, also known as Hu Moment Invariant (Hu, 1962), accessed using ImageRegion::get_nths_moment_invariant(size_t n) where n in {1, 2, 3, &amp;hellip;, 7}.
The following table summarizes the response of a moment invariant to translation, scale, rotation and mirroring. &amp;ldquo;Unchanged&amp;rdquo; means the value of the moment does not change when recomputed after the operation.
   N translation scale rotation mirroring     1 unchanged unchanged unchanged unchanged   2 unchanged unchanged unchanged unchanged   3 unchanged unchanged unchanged unchanged   4 unchanged unchanged unchanged unchanged   5 unchanged does change unchanged unchanged   6 unchanged does change unchanged does change   7 unchanged does change slight change changes sign    We note that all first 4 moment invariants are completely independent of translation, scale, rotation and mirroring of the region and are thus highly valuable in representing a region.
5. Texture Descriptors So far, our descriptors dealt with the region&amp;rsquo;s boundary, shape or the values taken directly from the original image. In this section, we will instead deal with the region&amp;rsquo;s texture. This construct has not agreed on definition, in crisp texture refers to the distribution of intensity values in the region. Where the intensity of a pixel is the mean over all planes of that pixel (stored as _intensitiy in ImageRegion::Element, if you recall). An easier way to express quantifying texture in crisp is, that we&amp;rsquo;re converting our region to grayscale, then construct a histogram using those grayscale value and use statistical techniques to describe the distribution modeled by the histogram.
5.1 Intensity Histogram To get a rough idea of what the distribution of a region&amp;rsquo;s intensity looks like, ImageRegion offers get_intensity_histogram(). In this histogram, the intensity values are quantized into 256 intensities. This is not the case internally, however, the loss of detail in the histogram is not representative of the values computed for texture descriptors.
auto hist = pepper.get_intensity_histogram(); auto hist_img = hist.as_image(); We note that the histogram has many blank spots, which means that not all intensities were represented. The histogram exhibits multiple large spikes around the 0.5 region, these correspond to many of the constant-colored regions of the pepper. Lastly, we note that a lot of intensities were seemingly only represented once as this would account for the long tail of the distribution, the left tail extends all the way to 0 while the right tail stops at about 0.6.
5.2 Maximum Response The maximum response is the probability of the intensity with the highest number of observations occurring. The closer to 1 this value is, the more likely is it that the region has only very few shades in intensity.
We access it using get_maximum_intensity_probability() which for the pepper region returns 0.57. This is relatively high, which makes sense, because most of the pepper is the same shade of green. The high probability is represented by the huge spike in the histogram.
5.3 Mean, Variance Two of the most basic descriptors of a data set (a set of intensities in our case) are mean and variance. We can access them using:
auto mean = pepper.get_mean(); auto stddev = sqrt(pepper.get_variance()); Our pepper region&amp;rsquo;s texture has a mean of 0.45 (which corresponds to the red line in the histogram) and a variance of 0.015, which is very low. This is expected as, again, most of the pepper is shades of green that do not vary greatly when converted to grayscale.
5.4 n-ths Pearson Standardized Moment around the Mean In statistics, a distribution of random variables can be quantified using statistical moments. Each moment has an order called n. The first four moments have a human interpretable meaning:
 the 0ths standardized moment is always 1 the 1st standardized moment is the mean difference between the mean and itself, which is always 0 the 2nd standardized moment is the mean ratio of the variance to itself, which is always 1 the 3rd standardized moment is called skewness, which is a measure of how much a distribution leans to one side the 4th standardized moment is called kurtosis, which is a measure of how long, which tail of the distribution is.  Higher order moments may not have immediate use in human interpretation, but can be very useful for machine-learning application. All moments for n &amp;lt; 7 are used commonly. We can access the n-ths moment using get_nths_moment(size_t n).
The 3rd and 4th moment can furthermore be accessed directly using get_skewness() and get_kurtosis(). To put these values into context, let&amp;rsquo;s again inspect our intensity histogram:
Because of the large number of singleton intensity occurrences towards 0, the distribution overall leans to the left. This is reflected in the skewness which is -2.20965. Negative values generally mean the distribution leans left, right for positive values. The kurtosis of the pepper&amp;rsquo;s texture is 8.74199 which is very high. This again was expected because both tails are very long and thin, resulting from the high number of singletons.
5.5 Average Entropy The average entropy is a measure of how ordered the set of intensities is. We can compute its value using get_average_entropy() which returns 0.769 for the pepper region. crisp normalizes the values into [0, 1] so ~0.77 is relatively high. This is expected, the pepper is mostly green and has large regions of constant intensity, so we would expect it to be highly ordered.
5.6 Co-Occurrence Matrix To quantify texture in a specified direction, we can construct what is called the co-occurrence matrix. The co-occurrence matrix is a matrix of size 256x256. It counts for each intensity pair i_a, i_b the number of times two pixels a, b that are next to each other in a specified direction have the corresponding intensities i_a, i_b. A short example: if the co-occurrence matrix for the &amp;ldquo;right&amp;rdquo; direction has a value of 6 in the row 120 and column 98 then in the image there are 6 pairs of pixels a, b such that a has intensity 120 and b who is directly right of a has intensity 98 Recall that the intensities are quantized and projected from [0, 1] (float) to [0, 256] (int) to keep the size of the co-occurrence matrix manageable.
When constructing the co-occurrence matrix, we need to first supply a direction. Let a = (x, y) and b = (x &#43; i, y &#43; j) then the values of the crisp::CoOccurrenceDirection enum have the following meaning:
Direction a b ------------------------------------- PLUS_MINUS_ZERO (x,y) (x, y-1) PLUS_45 (x,y) (x&#43;1, y-1) PLUS_90 (x,y) (x&#43;1, y) PLUS_125 (x,y) (x&#43;1, y&#43;1) PLUS_MINUS_180 (x,y) (x, y&#43;1) MINUS_125 (x,y) (x-1, y&#43;1) MINUS_90 (x,y) (x-1, y) MINUS_45 (x,y) (x-1, y-1) In crisp we can render the matrix like so:
auto co_occurrence_matrix = pepper.get_co_occurrence_matrix(CoOccurrenceDirection::PLUS_90); auto as_image = GrayScaleImage(co_occurrence_matrix); // bind or save to disk The above image was log-scaled for clarity. Firstly, we note that most of the cells are left black. This points to the pepper&amp;rsquo;s texture only having a relative small amount of pair-wise different intensity pairs, which, looking at the original image, is indeed the case. Secondly, we note that most of the high-occurrence pairs (those with a lighter pixel color in the above image) are clustered around the matrix&amp;rsquo; trace. This is evidence of high uniformity, as an occurrence along the trace means, that the intensity pair {i_a, i_b} that occurred had identical values i_a == i_b. Recall that these observations only make sense in respect to the direction of the co-occurrence matrix, which in our case is &#43;90° (from left to right).
We can numerically quantify the distribution of intensity value pair occurrences using the following descriptors:
5.7 Intensity Correlation Intensity Correlation measures how correlated the intensities in each intensity value pair are. Similar to the pearson correlation coefficient it has a value in [-1, 1] where -1 means a strong negative correlation, &#43;1 means a strong positive correlation (either of which usually mean there is a regularity to the pattern) and 0 means no correlation. A pattern that has 0 correlation could be random noise or of constant intensity.
We can compute the mean intensity correlation using get_intensity_correlation(CoOccurrenceDirection). In our example for the left-to-right (&#43;90°) direction, this returns 0.4868. This means the texture exhibits above average positive correlation in the left-to-right direction. Looking at the image of the pepper closely, we, note that the lighter spots coming from lighting are more present on the right side while the left side is more in the shadow. This explains the positive (increasing) intensity correlation when looking at the texture from left to right.
5.8 Homogeneity In the co-occurrence matrix, the diagonal represents occurrences of intensity pair i_a, i_b where i_a = i_b, occurrences where two pixels in the specified direction have the same intensity. Homogeneity quantifies, how many of the intensity pairs are located near this diagonal. The higher the homogeneity, the more common pixel neighbors with the same or similar intensity are.
We can access the value using get_homogeneity(CoOccurrenceDirection), it returns a float in [0,1]. The pepper region exhibits a homogeneity of 0.625. This may be slightly lower than expected considering the pepper is all green, however remember that we are quantifying texture, not color. The intensity (lightness) of the shades of green do vary quite a bit, even though the hue does not. Nonetheless, 0.625 would be considered far above average, so we would call the pepper a fairly homogenically textured region.
5.9 Entropy Similar to the average entropy, we can also compute the entropy of the co-occurrence matrix. This can be thought of as a descriptor of how ordered the occurrence-pair distribution in that direction is. Using get_entropy(CoOccurrenceDirection) we compute a value of 0.36 (again normalized into [0, 1]).
5.10 Contrast Contrast measures the difference in value between co-occurring pixels. A white pixel next to a black pixel (or vice-versa) would have maximum contrast, while two pixels of identical color would have 0 contrast. We can compute the mean contrast in the specified direction using get_contrast(CoOccurrenceDirection), as already mentioned, its values are in [0, 1].
Our pepper has a contrast of 0.0002 which is extremely low, again this is expected, the shades of green transition into each other smoothly, as there are no big jumps in intensity. Large parts of the pepper have constant regions where neighboring pixels have the same intensity.
]]></content:encoded>
    </item>
    <item>
      <title>[WIP] A fast, robust Boundary Tracing and Enumeration Algorithm</title>
      <link>http://clemens-cords.com/post/boundary_tracing_algorithm/</link>
      <pubDate>Thu, 29 Jul 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/boundary_tracing_algorithm/</guid>
      <description>Abstract Boundaries are central to many field in image processing and having a way to numerically represent them is the only way to build libraries of them for later feature detection. Here I present the algorithm used by crisp::ImageRegion that extracts from a region consisting of a set of 4 (8)-connected pixel coordinates all it&amp;rsquo;s outer boundary points and enumerates them in a deterministic manner. The region has to fullfill no further constraints.</description>
      <category domain="http://clemens-cords.com/categories/programming">Programming</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[Abstract Boundaries are central to many field in image processing and having a way to numerically represent them is the only way to build libraries of them for later feature detection. Here I present the algorithm used by crisp::ImageRegion that extracts from a region consisting of a set of 4 (8)-connected pixel coordinates all it&amp;rsquo;s outer boundary points and enumerates them in a deterministic manner. The region has to fullfill no further constraints. The algorithm runs in amortized o(x &#43; k) where x is the number of pixels in the region and k are the number of boundary pixels. The advantage of the method presented here is not only a robustness to common edge cases making user-input minimal but that both the freeman-chain codes and the minimum-vertex boundary polygon can be extracted at the end of the algorithm in o(1).
[(skip to source code)](#The Algorithm)
Introduction Boundary tracing it such an elemental task in image processing because boundaries are one of the best features to classify and many other features descriptors work as a function of boundary. Crisp for example uses the fourier transform of a boundary converted onto the complex plane (such that each vertices x position is the real, y position the complex part respectively) for boundary simplification and when I went to consult the literature I was frustrated that many of the common algorithms don&amp;rsquo;t fullfill the following two demands I somewhat arbitrarily set myself:
  i) the algorithm should work on all closed regions, no exceptions Common assumptions algorithms make are &amp;ldquo;let the region be simply connected&amp;rdquo; or &amp;ldquo;let the region boundary be non-overlapping&amp;rdquo;. I want crisps algorithm to be able to handle all of these with not further user interaction, indeed the only assertion crisp makes is that the region is closed and connected which for image-processing purposes is an assumption that&amp;rsquo;s made for all common segmentation algorithms anyway. If the region is not connected you can simply decompose them into connected segments, then the boundary of the original non-connected region is the union of the boundary of all segments
  ii) the resulting boundary should have the minimum number of points necessary to represent the region with no loss of detail To build a library of boundaries you need to transform them and that transformation should be as computationally as possible and crisp achieves this by returning the boundary polygon, a set of ordeted vertices that when connected are identical to the set of boundary points.
  iii) the resulting order of boundary points should be circular, consistent and predictable it is not difficult to isolate the set of boundary points from a region, however we want those points to have a strong order that makes sense to both humans and computers. crisp wants the vertex to orderd in the counter-clockwise with the position x-axis advancing to the east and the positive y-axis advancing south in 2d space. This order being consistent means for two sets such that the intersection of the two is empty, the algorithm should return two boundary point sequences that are identical. Circularity means that if we index the points with i = 0 &amp;hellip; N then point p_0 needs to be connected to p_1 and p_N, more on this constraint below
  These three conditions need to be fullfilled at the same time of course. I&amp;rsquo;ve used quite a few terms without properly defining them so far so let&amp;rsquo;s get that out of the way
Definitions A image region M of an Image I is defined as a finite set of pixels that is a subset of an image of size m*n such that the following conditions hold:
 for any two pixel x in M there is a different pixel x_n such that x and x_n are 8-connected. M is a subset of I  A boundary can be intuitively described as the outer most pixels of the region that are still a subset of it. Each pixel in the boundary can be approached both from inside M and from the space around M. Formally a boundary K = {p_0, p_1, &amp;hellip;, p_n} is a set of pixels such that
 A: K is a subset of M, this makes M a closed region B: for any pixel p_i in K it holds that p_i-1 is 8-connected to p_i and p_i is 8-connected to p_i&#43;1. p_0 is 8-connected to p_n, this makes the boundary circular C: any pixel y in K has less than 8 neighbors in M, this means in image space y has at least 1 pixel that is not part of the region D: K is not the boundary of a hole (TODO: define hole)  A boundary K is minimal if there exists no pixel p_i such that K - p_i (the set difference) is still 8-connected and fullfills condition B, see above
A boundary polygon P = {v_0, v_1, &amp;hellip;, v_n} is a set of vertices such that if you draw a 1-pixel thick line from v_0 to v_1, v_1 to v_2, &amp;hellip;, v_m-1 to v_m in image space then the set of pixels covered by these lines is identical to the boundary K.
On minimality, it&amp;rsquo;s best to show visually what it means to be minimal, of course we want our set to be as small as possible for performance reason but visually it means that some pixels with only 1 non-M neighbor should be ommitted as such:
 Figure 1 An 8-connected region (A), a boundary (B) and the minimal boundary (C)  As state in the definition if we remove any boundary (white) pixel in C we would either loose detail from the original region or make it so there&amp;rsquo;s an unconnected gap in the sequence of boundary points.
The Algorithm Now that we know what we&amp;rsquo;re trying to do let&amp;rsquo;s first get the actual algorithm out of the way. An step-by-step explanation will follow below
// in BinaryImage image; // image that is true if the pixel is part of a closed region, false otherwise  // out std::vector&amp;lt;Vector2ui&amp;gt; boundary; std::vector&amp;lt;Vector2ui&amp;gt; boundary_polygon; size_t n_holes; // ### STEP 1: pre-process the image bool segment_color = true; // white  // fill all 1-pixel holes for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) == segment_color) continue; // count 4-connected neighbors that are part of the region  size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } if (n &amp;gt;= 3) image(x, y) = segment_color; } } // prune all 1-pixel lines std::vector&amp;lt;Vecto2ui&amp;gt; pruned; while (true) { n_changed = 0; for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) != segment_color) continue; size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } if (n == 1) { image(x, y) = not segment_color; n_changed&#43;&#43;; } } } if (n_changed == 0) break; } // ### STEP 2: extract the segment struct PixelCoordCompare { bool operator()(const Vector2ui&amp;amp; a, const Vector2ui&amp;amp; b) const { return a.y() != b.y() ? a.y() &amp;lt; b.y() : a.x() &amp;lt; b.x(); } }; using PixelSet = std::set&amp;lt;Vector2ui, detail::PixelCoordCompare&amp;gt;; std::vector&amp;lt;PixelSet&amp;gt; segments = crisp::decompose_into_connected_segments(image, {true}); PixelSet segment = segments.at(0); // ### STEP 3: isolate all possible boundary points  PixelSet strong_pixels, // 100% part of b oundary  weak_pixels; // may be needed for continuing in edge cases  for (auto&amp;amp; px : segment) { size_t n_unconnected = 0; for (long i = -1; i &amp;lt;= &#43;1; &#43;&#43;i) { for (long j = -1; j &amp;lt;= &#43;1; &#43;&#43;j) { if (not (i == 0 and j == 0) and segment.find(Vector2ui(px.x() &#43; i, px.y() &#43; j)) == segment.end()) n_unconnected&#43;&#43;; } } if (n_unconnected &amp;gt; 1) strong_pixels.insert(px); else if (n_unconnected == 1) weak_pixels.insert(px); } // ### STEP 4: define the direction function  auto translate_in_direction = [&amp;amp;](Vector2ui point, uint8_t direction) -&amp;gt; Vector2ui { direction = direction % 8; int x_offset, y_offset; switch (direction) { case 0: // WEST  x_offset = -1; y_offset = 0; break; case 1: // SOUTH WEST  x_offset = -1; y_offset = &#43;1; break; case 2: // SOUTH  x_offset = 0; y_offset = &#43;1; break; case 3: // SOUTH EAST  x_offset = &#43;1; y_offset = &#43;1; break; case 4: // EAST  x_offset = &#43;1; y_offset = 0; break; case 5: // NORTH EAST  x_offset = &#43;1; y_offset = -1; break; case 6: // NORTH  x_offset = 0; y_offset = -1; break; case 7: // NORTH_WEST  x_offset = -1; y_offset = -1; break; } return Vector2ui(point.x() &#43; x_offset, point.y() &#43; y_offset); }; // ### STEP 5: Initialize the Tracing  // output, each index holds 1 boundary object where the first one is the outer boundary and every subsequent one is the outline of a hole std::vector&amp;lt;std::vector&amp;lt;Vector2ui&amp;gt;&amp;gt; boundaries_out; std::vector&amp;lt;std::vector&amp;lt;uint8_t&amp;gt;&amp;gt; directions_out; while (strong_pixels.size() &amp;gt; 0) { boundaries_out.emplace_back(); directions_out.emplace_back(); auto&amp;amp; boundary = boundaries_out.back(); auto&amp;amp; direction = directions_out.back(); auto top_left = *strong_pixels.begin(); boundary.push_back(top_left); strong_pixels.erase(top_left); direction.push_back(0); size_t current_i = 0; // ### STEP 6 trace and push once tracing complete  do { // current pixel  auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); // was a candidate found  bool found = false; // check strong candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // check for convergence by looping back to the starting pixel  if (to_check == top_left) finished_maybe = true; if (strong_pixels.find(to_check) != strong_pixels.end()) { // push new pixel from set to boundary  boundary.push_back(to_check); direction.push_back(dir); strong_pixels.erase(to_check); found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { current_i = boundary.size() - 1; continue; } // if no strong candidate was found even though we looped back, we are back at the start  // c.f. explanation below  else if (finished_maybe) break; // if no strong candidate was found, check weak candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; if (weak_pixels.find(to_check) != weak_pixels.end()) { boundary.push_back(to_check); direction.push_back(dir); weak_pixels.erase(to_check); found = true; break; } } if (found) { current_i = boundary.size() - 1; continue; } // if neither weak nor strong candidate found, start traceback  else current_i--; } while (current_i != 0); // ### STEP 7: reduce to non-redundant polygon vertices outer_boundary = boundaries_out.at(0); outer_boundary_directions = directions_out.at(0); size_t n_holes = boundaries_out.size() - 1; // all boundary at position &amp;gt; 0 are those of holes  // define function to detect if two vertices in a sequence are colinear auto turn_type = [&amp;amp;](size_t i_a, size_t i_b) -&amp;gt; int { auto point_a = outer_boundary.at(i_a), point_b = outer_boundary.at(i_b); // warp point from traceback  if (abs(int(point_a.x()) - int(point_b.x())) &amp;gt; 1 or abs(int(point_a.y()) - int(point_b.y())) &amp;gt; 1) return 0; auto dir_a = outer_boundary_directions.at(i_a), dir_b = outer_boundary_directions.at(i_b); if (dir_b &amp;gt; dir_a or (dir_a == 7 and dir_b == 0)) return -1; // left-hand turn  else if (dir_b &amp;lt; dir_a or (dir_a == 0 and dir_b == 7)) return &#43;1; // right-hand turn  else return 0; // colinear }; auto boundary_polygon_out = std::vector&amp;lt;Vector2ui&amp;gt;(); Vector2f mean_pos = Vector2f(0, 0); // discard all colinear vertices but preserve order for (size_t i = 0; i &amp;lt; _boundary.size() - 1; &#43;&#43;i) if (turn_type(i, i&#43;1) != 0) boundary_polygon_out.push_back(_boundary.at(i)); return boundary_polygon_out; Step 1: Pre Processing We first need to pre-process the image, 1-pixel holes and gaps and 1-pixel thick lines will cause problems later on so we need to remove them. Often these features happen purely because of noise and even if you would need them to be part of the actual region because you&amp;rsquo;re working on that small a resolution you can simply scale the image by a factor of 2 and the automated pre-processing step will leave them untouched.
To fill all 1-pixel holes we iterate through the region M (here represented as a binary image) and for each pixel count the number of 4-connected neighboring pixels that are also in M. If at least 3 of them are we found a hole or gap that needs to be patch
// fill all 1-pixel holes for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { // only iterate through region  if (image(x, y) == segment_color) continue; size_t n = 0; // count for connected neighbors  for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } // make that pixel part of the region by setting it to the specified color (black)  if (n &amp;gt;= 3) image(x, y) = segment_color; } }  Figure 2: Left: Segmented region corrupted by noise, Right: Region after filling all 1-pixel holes (newly assigned pixels shown in purple)  We now need to prune all 1-pixel thick lines. We do this recursively (there&amp;rsquo;s more optimal ways to do this that have a far less performance overhead but for the sake of simplicity I will demonstrate the recursive method here. Optimally you would use a subtract a morphological hit-or-miss-transform that detects the lines from the original image) by &amp;ldquo;eating away&amp;rdquo; at the end of each line and repeat until no more lines are left:
// prune all 1-pixel lines while (true) { // count number of changed pixels  n_changed = 0; for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) { for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { if (image(x, y) != segment_color) continue; // count number of 4-connected neighbors in M  size_t n = 0; for (std::pair&amp;lt;int, int&amp;gt; i_j : {{-1, 0}, {1, 0}, {0, -1}, {0, 1}}) { if (image(x &#43; i_j.first, y &#43; i_j.second) == segment_color) n&#43;&#43;; } // if it only has 1 neighbor it is the end of a line, prune it  if (n == 1) { image(x, y) = not segment_color; n_changed&#43;&#43;; } } } // if no lines changed, we converged  if (n_changed == 0) break; }  Figure 3: Left: Region after filling all 1-pixel holes, Right: Pruned region, removed pixels shown in purple with increased lightness as the number of iterations increases  Note that we leave a &amp;ldquo;stump&amp;rdquo; of size 1-pixel, if this throws of later processing feels free to first erode then dilate the area with a 2x2 structuring element. Morphological processing in general can make this process faster and cleaner but for now we will do it recursively like this.
Now that our region as an image is primed we can transform it into a region as a mathematical set of coordinates.
Step 2: Extract the Region We first define a new typedef PixelSet, this is a set that takes a custom comparator such that the pixel coordinates in the set are ordered left-to-right, top-to-bottom which means the first pixel is the pixel with the smalle x- and y- coordinate, if two pixels share a x-coordinate, the pixel with the lower y-coordinate comes first. Remember that our coordinate systems x-axis extends to right of the image, the y-axis to the bottom so a higher y-value means the pixel is lower down.
struct PixelCoordCompare { bool operator()(const Vector2ui&amp;amp; a, const Vector2ui&amp;amp; b) const { return a.y() != b.y() ? a.y() &amp;lt; b.y() : a.x() &amp;lt; b.x(); } }; using PixelSet = std::set&amp;lt;Vector2ui, detail::PixelCoordCompare&amp;gt;; std::vector&amp;lt;PixelSet&amp;gt; segments = crisp::decompose_into_connected_segments(image, {true}); PixelSet segment = segments.at(0); The actual extraction of the set is left to another crisp function, to learn more about it you can check the documentation HERE TODO. It extracts all 4-connected segments, let&amp;rsquo;s assume our segment is the only one in the image so the only set in the vector is the set of all pixel coordinates M.
Step 3: Isolate all Boundary Pixels A boundary pixel by definition is a pixel that has a neighbor in image space that is not part of M. Much like in our pre-processing steps we can simply iterate through the set and count the number of pixels with less than 8 neighbors. We furthermore devide these into weak and strong boundary candidates. A weak candidate is a pixel with exactly 1 non-M neighbour, a strong candidate is a pixel with 2 or more non-M neighbours. The reason for this seperation will come into play shortly
PixelSet strong_pixels, weak_pixels; for (auto&amp;amp; px : segment) { // count non-M neighbours  size_t n_unconnected = 0; for (long i = -1; i &amp;lt;= &#43;1; &#43;&#43;i) { for (long j = -1; j &amp;lt;= &#43;1; &#43;&#43;j) { if (not (i == 0 and j == 0) and segment.find(Vector2ui(px.x() &#43; i, px.y() &#43; j)) == segment.end()) n_unconnected&#43;&#43;; } } // 2 or more -&amp;gt; strong  if (n_unconnected &amp;gt; 1) strong_pixels.insert(px); // exactly 1 -&amp;gt; weak  else if (n_unconnected == 1) weak_pixels.insert(px); }  Figure 4: Left: Region from step 3, Right: Region with strong boundary candidates highlighted in green, weak candidates highlighted in red.  Step 5: Define the direction function One more thing before we can finally start tracing the boundary, we need to define a function that defines the direction of two pixels in the boundary. Let p_i, p_i&#43;1 be two pixels in K, then we can visualize the following directions as &amp;ldquo;to travel from p_i to p_i&#43;1 we need to go one pixel x&amp;rdquo; where x is the direction. So for x = &amp;ldquo;south west&amp;rdquo;, we need to travel one pixel to the bottom and one pixel to the right in image space. We get 8 direction like this and we assign each direction a number: 0 = west, 1 = south west, 2 = south and so on in clockwise direction (the boundary is traced in counterclockwise direction while this direction mapping function assigns values clockwise). We then end up with 7 = north west after which we loop back to 0 = west. To do ths looping we take the modulo of the direction before returning the appropriate next pixel.
auto translate_in_direction = [&amp;amp;](Vector2ui point, uint8_t direction) -&amp;gt; Vector2ui { direction = direction % 8; int x_offset, y_offset; switch (direction) { case 0: // WEST: p_i&#43;1 is right of p_i  x_offset = -1; y_offset = 0; break; case 1: // SOUTH WEST: p_i is  x_offset = -1; y_offset = &#43;1; break; case 2: // SOUTH  x_offset = 0; y_offset = &#43;1; break; case 3: // SOUTH EAST  x_offset = &#43;1; y_offset = &#43;1; break; case 4: // EAST  x_offset = &#43;1; y_offset = 0; break; case 5: // NORTH EAST  x_offset = &#43;1; y_offset = -1; break; case 6: // NORTH  x_offset = 0; y_offset = -1; break; case 7: // NORTH_WEST  x_offset = -1; y_offset = -1; break; } return Vector2ui(point.x() &#43; x_offset, point.y() &#43; y_offset); }; It is important to really understand this function, for tracing to work properly we need to know what direction we just went to get from p_i-1 to p_i, let&amp;rsquo;s say &amp;ldquo;south&amp;rdquo; (which is the value 2) then when looking for the next pixel around p_i we need to start looking first at south - 1 = south west, if no match was found look south next, south east afterwards, etc.. Doing it like this will assure that our tracing can&amp;rsquo;t get stuck in loops.
Step 6: Tracing We first need to initialize the algorithm, we open a vector that holds confirmed boundary points and a vector of directions that holds the corresponding direction. So when the boundary point is at position j in the vector then the direction vector will have the direction travele from p_j-1 to p_j at position j.
(the following algorithm is missing certain parts of the full algorithm above, this is for ease of explaining, we will get to what other things we need to do later).
auto&amp;amp; boundary = std::vector&amp;lt;Vector2ui&amp;gt;(); auto&amp;amp; direction = std::vector&amp;lt;uin8_t&amp;gt;(); auto top_left = *strong_pixels.begin(); boundary.push_back(top_left); strong_pixels.erase(top_left); direction.push_back(0); We also initialize the first boundary point which is the top-most left-most strong pixel. Because our custom PixelSet orders points in just this way, the first pixel in the set is also the top-most, left-most one. We then initialize the directions with 0 (west). We can now start tracing.
size_t current_i = 0; do { // current pixel and direction  auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); // boolean to track if a proper candidate was found yet  bool found = false; // check strong candidates  // we start at the last direction -1 (counterclockwise before the other)  // and go through all possible directions, here counted with n  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { // we check the next pixel as directed by the direction function mentioned above  auto to_check = translate_in_direction(current, dir); // skip if out of bounds  if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // this will be important for convergene, ignore it for now  //if (to_check == top_left)  // finished_maybe = true;  //if a strong pixel was found  if (strong_pixels.find(to_check) != strong_pixels.end()) { // add it to confirmed boundary points  boundary.push_back(to_check); // add it corresponding direction  direction.push_back(dir); // scrub it from the set of possible boundary points  strong_pixels.erase(to_check); // say that we found a candidate  found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { // reset current_i to the pixel last pushed  current_i = boundary.size() - 1; // and skip the rest of the loop  continue; } (...) Again, some of the loop was left out for clarity for now. We start at the top left and then check if any of the pixels in it&amp;rsquo;s 8-connected neighborhood are also in M. If yes, we found a boundary pixel and can push it and now search the 8-connected neighborhood around it. As mentioned above keeping track of the direction is important to enforce a strong order, if we arrived via direction d at pixel p_i (from p_i-1) then we should start checking pixels at direction d-1 next. d-1 is left of d because the directions are numerate clockwise. If we don&amp;rsquo;t find a pixel there, try the next direction and so on.
Now you might think we are done. We just go through all the pixels and since all strong pixels are linked, it&amp;rsquo;ll just finish, right? Well not quite. Consider our region from earlier:
 Figure 6: State of the tracing algorithm after 44 iterations. The compass wheel on the top shows the directions where ``mint = east``, ``red = west``, ``pink = north``, ``cyan = south``, etc.. The corresponding direction for each point p_i is shown in color. The yellow point is the starting point. On the right the boundary so far is highlighted in white.  We&amp;rsquo;re stuck! We&amp;rsquo;ve only been using strong pixels so far and we&amp;rsquo;ve gotten to a point where there&amp;rsquo;s no other strong pixel in the 8-neighborhood. We kept track of the weak pixels for just this occassion, because:
(the previous code is reposts here for convenience)
do { auto current = boundary.at(current_i); auto current_direction = direction.at(current_i); bool found = false; // check strong candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; // check for convergence by looping back to the starting pixel  if (to_check == top_left) finished_maybe = true; if (strong_pixels.find(to_check) != strong_pixels.end()) { // push new pixel from set to boundary  boundary.push_back(to_check); direction.push_back(dir); strong_pixels.erase(to_check); found = true; break; } } // if we already found a strong candidate we can just jump to the next  if (found) { current_i = boundary.size() - 1; continue; } // if no strong candidate was found, check weak candidates  for (int dir = current_direction - 1, n = 0; n &amp;lt; 8; &#43;&#43;dir, &#43;&#43;n) { // same thing as with strong candidates, just with the weak_pixels set  auto to_check = translate_in_direction(current, dir); if (to_check.x() &amp;lt; _x_bounds.x() or to_check.x() &amp;gt; _x_bounds.y() or to_check.y() &amp;lt; _y_bounds.x() or to_check.y() &amp;gt; _y_bounds.y()) continue; if (weak_pixels.find(to_check) != weak_pixels.end()) { boundary.push_back(to_check); direction.push_back(dir); weak_pixels.erase(to_check); found = true; break; } } // if we found a weak pixel we can continue  if (found) { current_i = boundary.size() - 1; continue; } } while (current_i != 0); We now do just the same thing for weak pixels but we want to prioritize the strong ones always but now that we would be stuck without them we fall back and add 1 weak pixel:
 Figure 6: State of the tracing algorithm after 44 iterations. The compass wheel on the top shows the directions where ``mint = east``, ``red = west``, ``pink = north``, ``cyan = south``, etc.. The corresponding direction for each point p_i is shown in color. The yellow point is the starting point. On the right the boundary so far is highlighted in white.  ]]></content:encoded>
    </item>
    <item>
      <title>[WIP] Choosing initial Cluster Centers for use in RGB Image Segmentation using k-means clustering</title>
      <link>http://clemens-cords.com/post/k-means_heuristic/</link>
      <pubDate>Thu, 29 Jul 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/k-means_heuristic/</guid>
      <description>Abstract TODO
The Algorithm First we initialize the cluster centers (how exactly this is done will be explained later)
using namespace crisp; struct Cluster { Color color_sum; size_t n; Color mean_color; } std::vector&amp;lt;Cluster&amp;gt; clusters; for (size_t i = 0; i &amp;lt; n_clusters; &#43;&#43;i) { Color color = // heuristically choosen, see below  clusters.push_back(Cluster{color, 1, color}); } Here n is the number of pixels currently in the cluster, color_sum is the sum of the rgb color and mean_color is the current color assigned to the entire cluster.</description>
      <category domain="http://clemens-cords.com/categories/programming">Programming</category>
      <category domain="http://clemens-cords.com/categories/crisp">Crisp</category>
      <content:encoded><![CDATA[Abstract TODO
The Algorithm First we initialize the cluster centers (how exactly this is done will be explained later)
using namespace crisp; struct Cluster { Color color_sum; size_t n; Color mean_color; } std::vector&amp;lt;Cluster&amp;gt; clusters; for (size_t i = 0; i &amp;lt; n_clusters; &#43;&#43;i) { Color color = // heuristically choosen, see below  clusters.push_back(Cluster{color, 1, color}); } Here n is the number of pixels currently in the cluster, color_sum is the sum of the rgb color and mean_color is the current color assigned to the entire cluster.
We then specify a distance measure, for this variant the euclidian distance in rgb space is sufficient. Quantizing the possible values (remember, our Colors are floating point vectors which each element in the range [0, 1]) aids in performance
auto distance = [](Color a, Color b) -&amp;gt; int { int score = abs(int(a.red() * 255) - int(b.red() * 255)) &#43; abs(int(a.green() * 255) - int(b.green() * 255)) &#43; abs(int(a.blue() * 255) - int(b.blue() * 255)); return score; } We quantize the color components to 8-bit and then compute the euclidian distance ommitting the square root and square operations for improved performance as no normalization is necessary. Note that this function will return values in the range of [0, 3*255].
The algorithm is initialized by first allocating the result image. To save on performance we use the image itself as a way to keep track of which pixel is in which clusters. Since the result image is an RGB image we arbitrarily declare the .red() component to be the cluster index (range {0, 1, 2, &amp;hellip;, n_clusters - 1}) while all other components are set to 0. While color components can only be in the range [0, 1] for display, crisp allows you to freely set the values while the images are still in memory. The range is only enforced when binding an image for rendering.
ColorImage out; // .r is cluster index out.create(image.get_size().x(), image.get_size().y(), Color(-1, 0, 0)); Now for the algorithm proper, we can think of it in two parts, the first part is the initial iteration. Here we assign each pixel to the cluster nearest to it:
for (long x = 0; x &amp;lt; image.get_size().x(); &#43;&#43;x) for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { auto&amp;amp; pixel = image(x, y); // find cluster with minimum distance  int min_distance = std::numeric_limits&amp;lt;int&amp;gt;::max(); size_t min_cluster_i = -1; for (size_t i = 0; i &amp;lt; clusters.size(); &#43;&#43;i) { // compute mean color of all cluster pixels  auto current_distance = distance(pixel, clusters.at(i).mean_color); if (current_distance &amp;lt; min_distance) { min_distance = current_distance; min_cluster_i = i; } } // assign pixel to cluster, .red() component is cluster index  pixel.red() = min_cluster_i; } We could end the algorithm right now and we would have a decent segmentation however this relies entirely on the heuristically chosen cluster centers and there is no guruantee the clusters would trend towards optimality. We achieve a decent approximation of this behavior (achieving proper optimality is NP-hard) by iteratively checking each pixel again and if there is a cluster that&amp;rsquo;s a better fit for it, we reassign it. After each iteration, we recompute the current mean cluster color meaning the clusters will change a little each iteration until convergence is achieved.
size_t n_changed = -1; // number of pixels that changed cluster this iteration  while (n_changed &amp;gt; 0) { n_changed = 0; for (long y = 0; y &amp;lt; image.get_size().y(); &#43;&#43;y) { auto&amp;amp; pixel = image(x, y); // find cluster with minimum distance  int min_distance = std::numeric_limits&amp;lt;int&amp;gt;::max(); size_t min_cluster_i = -1; for (size_t i = 0; i &amp;lt; clusters.size(); &#43;&#43;i) { // compute mean color of all cluster pixels  auto current_distance = distance(pixel, clusters.at(i).mean_color); if (current_distance &amp;lt; min_distance) { min_distance = current_distance; min_cluster_i = i; } } // get cluster pixel is currently assigned to  int old_i = pixel.red(); // reassign  if (old_i != min_cluster_i) { auto&amp;amp; old_cluster = clusters.at(old_i); auto&amp;amp; new_cluster = clusters.at(new_i); old_cluster.n -= 1; old_cluster.color_sum -= pixel; new_cluster.n &#43;= 1; new_cluster.color_sum &#43;= pixel; } // else do nothing  } // update clusters  for (auto&amp;amp; cluster : clusters) cluster.mean_color = cluster.color_sum / cluster.n; } It can be shown [TODO] that if ties (that is two clusters have the exact same distance to a single pixel) are resolved arbitrarily but each pixel is only assigned to one cluster this algorithm will always converge.
[TODO: convergence with images after each cluster]
The Heuristic In proper statistics it is common to run the algorithm multiple times with different number of cluster centers. Some even randomize the cluster centers completely and the solution is achieved by testing convergence between different entire runs rather than iterations within the same run. In image segmentation this is not available to us, usually the viewer will a) know how many principal color clusters there are in an image and b) will not have the memory nor time to do multiple executions. This means we basically need to nail it on the first try.
]]></content:encoded>
    </item>
    <item>
      <title>A (personal) modern C&#43;&#43; Styleguide</title>
      <link>http://clemens-cords.com/post/c&#43;&#43;_style_guide/</link>
      <pubDate>Mon, 17 May 2021 22:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/c&#43;&#43;_style_guide/</guid>
      <description>Quickstart  skip to here for a fully syntax highlighted code example that has all the possible cases Introduction  People, especially outside of CS, often think of code as purely utilitarian. It&amp;rsquo;s like a machine or a tool, it doesn&amp;rsquo;t matter how dirty it is inside, as long as it works and if it breaks you can call someone to fix it, it&amp;rsquo;s the best it can be. While this may be the case for a pneumatic borer I would urge everyone to consider source code asthetically equivalent to a dissertation, a pamphlet or an instruction guide and any piece of code should have the same care and effort put into it&amp;rsquo;s formatting and presentation as into more traditional documents.</description>
      <category domain="http://clemens-cords.com/categories/programming">Programming</category>
      <content:encoded><![CDATA[Quickstart  skip to here for a fully syntax highlighted code example that has all the possible cases Introduction  People, especially outside of CS, often think of code as purely utilitarian. It&amp;rsquo;s like a machine or a tool, it doesn&amp;rsquo;t matter how dirty it is inside, as long as it works and if it breaks you can call someone to fix it, it&amp;rsquo;s the best it can be. While this may be the case for a pneumatic borer I would urge everyone to consider source code asthetically equivalent to a dissertation, a pamphlet or an instruction guide and any piece of code should have the same care and effort put into it&amp;rsquo;s formatting and presentation as into more traditional documents. The main reason for this is to allow for cooperation, it&amp;rsquo;s always nice to have good documentation but if someone new joins the team knowing nothing about how things are done and they&amp;rsquo;re tasked to fix something rather than just use it their success and speed of accomplishing this mainly depends on how well maintained the code is. This way pretty code not only shows that you take pride and care in your work but also increases productiveness far, far into the future.
I would like to quickly break down my philosophy of what makes good practice and code style. The most important things are (in order starting with most important):
 Consistency: If you format something a certain way and you think it&amp;rsquo;s the best way to do it, format it that way every single time Clarity: Assume your audience has a very hard time understanding any code and try to make it easy to understand for them, not your well-trained doctorated senior dev Adaptability: Try to stay with the most up-to-date-way of doing things and if someone shows you a better way to do things, adopt it without question Elegance: Assuming maximum clarity and correctness the most elegant way to do something is the right way.  Table of Contents
0. Quickstart
1. Loops &amp; Brackets
2. Variables
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp2.1 Naming
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp2.2 Declaration
3. Operators
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp3.1 References and Pointers
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp3.2 Logical Operators
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp3.3 Numerical Operators
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp3.4 Pre- and Postfix Increment
4. Functions
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp4.1 Member Functions
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp4.2 Lambdas
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp4.3 Auto-deducing Return Types
5. Classes &amp; Enums
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.1 Naming
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.2 Class vs. Struct
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.3 Order of access-specified members: .hpp vs .cpp
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.3.1 Filenames
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.3.2 Template for Header-only Libraries
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp5.3.3 Template for Non-Header-Only
6. Comments &amp; Documentation
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp6.1 In-File Documentation
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp6.2 Comments
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp6.3 Choice of Variable/Function Names
7. Closing Remarks
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp7.1 Version Control
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp7.2 Testing and Profiling
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp7.3 When Optimization is not Appropriate
&amp;nbsp&amp;nbsp&amp;nbsp&amp;nbsp7.4 The Golden Rule: Always Refactor Once
8. Thanks and References
Loops &amp; Brackets Let&amp;rsquo;s get the omnipresent stuff out of the way first:
If present at all, brackets after if-else, while, for, try-catch and similar loops as well as blocks, non-lambda functions, classes, structs and enums should always have a newline before the first bracket as such:
// wrong enum { MEMBER_1, MEMBER_2 }; class foo { inline Foo() { for (auto&amp;amp; i : unmentioned_member) { initialize(i); }; } inline void bar() { return 1; } inline void empty_function() { } } // correct enum { MEMBER_1, MEMBER_2 }; class foo { inline Foo() { for (auto&amp;amp; i : unmentioned_member) { initialize(i); }; } inline void bar() { return 1; } }  The only exceptions to this are namespaces and lambdas with only one line of code inside of them as empty blocks. These should be opened and closed in the same line:
namespace style_guide::inside { Foo::Foo() {} // newline before the first bracket but closed in the same line  Foo::Foo(ArgumentType arg) : _member_a(//...),  _member_b(//...),  _other_member([&amp;amp;]() -&amp;gt; MemberType&amp;amp;&amp;amp; // newline because lambda body has more than one line of code  { do_something_else(); return std::move(MemberType(arg)); }()), _member_c(//...)  { std::sort(member_a.begin(), member_a.end() [](auto a, auto b) { return a &amp;gt; b; }); // no newline because it is one line  } }  Note how the multi-line lambda in the constructor is indented the same as the other _members in the trailing initializer list.
Now this decision is pretty arbitrary and it&amp;rsquo;s kind of a toincoss wether an individual dev will do newline-before or newline-after for curved brackets but I&amp;rsquo;d still like to give some context: I think the main reason to do after is to force devs to put a basically empty line after the first statement. This helps distinguish between function/class definitions and regular statements and it also makes it easier to find the start of the loop. Basically what I&amp;rsquo;m trying to say is:
// messy namespace space { ClassName : public Mother { ClassName() = default; ClassName(ArgType arg) : ClassName() { } } } // clearer namespace space { ClassName : public Mother { ClassName() = default; ClassName(ArgType arg) : ClassName() {} } } // equally clear though: namespace space { ClassName : public Mother { ClassName() = default; ClassName(ArgType arg) : ClassName() { } } }  What&amp;rsquo;s important isn&amp;rsquo;t the bracket but the empty or almost empty line after the declaration. The reason I pick newline-before is because for constructors like in the above example I like the opening bracket for the CTOR definition to not be inline with one of the initializer list statements, with normal functions it can&amp;rsquo;t get lost but with CTORs it can in all the brackets of the initializer list:
Foo::Foo() : LambdaMember2([&amp;amp;]() { auto res = multiline_lambda(); res &#43;= multiline_lambda(); return res; }(), LambdaMember([&amp;amp;](){ return oneline_lambda();}()) { }  Is very hard to parse. With the bracket starting the ctor definition preceded by a newline it is much easier and if you&amp;rsquo;re doing the newline-before in one situation you should do it always because as I said, consistency is the most important thing.
Variables The following variable naming scheme should be adhered to at all times in most C-based languages unless language specifics prohibit:
 Scope Qualifier Naming Example   local class-, function- or internal namespace scope non-const or const  lower-case snake_case
  size_t not_a_member;
   global scope non-const or const all-caps snake_case  static InputHandler INPUT_HANDLER;
#define PI 3.14159
   public member non-const lower-case snake_case  Vector2f top, left;
   public member const all-caps snake_case  const size_t MAX_THREAD_COUNT = 64;
   private or protected member non-const or const lower-case snake_case with prefix &#34;_&#34;  std::string _id;
static Color _default_color;    Some variables gain a pre- or postfix depending on their type as such (the above naming scheme still applies):
 Type Name   unsigned int index i, j, ... or foo_i, foo_j, ...   unsigned int counter n_foo   bool is_foo, should_foo, can_foo, etc.   Type* foo_ptr   Type::Iterator foo_it   Color foo_rgb, foo_rgba, foo_hsv, foo_hsva, etc.   Angle in [0, 360] or [0, 2&amp;#960] foo_dg, foo_rad   Percent in [0%, 100%] foo_pct   typename or using/typedef declaration name  Foo_t    Declaration The order of type qualifier should always adhere to the following priorites (where 0 is the left most qualifier) 0: static 1: inline, extern 2: mutable 4: volatile 5: constexpr, consteval, const 6: Type of the variable &amp;lt;no space&amp;gt; 7: &amp;amp; or * or &amp;amp;&amp;amp; &amp;lt;space&amp;gt; 8: Name of the variable // for example static inline const foo* const ptr = //... extern mutable volatile const auto* var = //... 
In easier to remember terms: always put static fist, always group const as right as possible but before the variable type and name. It&amp;rsquo;s rare for this many qualifiers to pile up but it&amp;rsquo;s good to have a solid idea of where stuff goes should the time come.
Any variable that explicitely calls a constructor or other function on the right hand-side of the declaration should have the type auto.
// wrong: Vector2f vec = Vector2f(20, 50); Vector2f vec = create_vector(20, 50); Vector2f vec(20, 50); // correct: auto vec = Vector2f(20, 50); auto vec = create_vector(20, 50);  This is to avoid confusion with C&#43;&#43;20s new aggregate initialization with round brackets as it is not immediately obvious which of these the following statement tries to invoke:
Vector2f foo();  I could be either calling the default constructor of Vector2f with c&#43;&#43;20 aggregrate initialization with round brackets and no arguments or forward declaring a function with return type Vector2f and no arguments. Using
auto foo = Vector2f();  makes it immediatly obvious.
For trivial numeric types, always use float for floating points. 64-bit numbers like double should be a conscious decision on the developers part when they recognize that they need the extra precision, not the default. This is to avoid unnecessary overhead on certain cpu architectures or in bleeding-edge-performance environment.
Be aware of potential overhead this may cause by casting up or down the precision hirarchy when interacting with other libraries. clang-tidy usually issues a warning for this and it should not be ignored.
On the topic of casting, static_cast should always be preferred over c-style casts, even for trivial numeric type:
double high_precision = 2.2153165231; // wrong float low_precision = float(high_precision); float low_precision = (float) high_precision; auto low_precision = float(high_precision); float low_precision = static_cast&amp;lt;float&amp;gt;(high_precision); // correct auto low_precision = static_cast&amp;lt;float&amp;gt;(high_precision);  If multiple variables of the same type and type-qualifier are declared one after another, always use the init-declarator-list form as such:
std::string first_name, last_name, adress; Foo *ptr_one, *ptr_two; Bar &amp;amp;ref; Bar not_ref;  There should always be a newline after the first declaration. If one of the variables is a pointer, reference, r-value, etc. then all of them should have the same type qualifier. If this is not the case, the type should be explicitely redeclared in a new line.
This form reduces redundant words be redeclaring the type over and over and makes it easy to group members or variables by type on first glance.
I would make an exception if all variables names are very short words, consider: // version 1 struct Color { float r, g, b, a; } // version 2 struct Color { float r, g, b, a; } 
I would not reject a pull request with version 1 however I would also not ask someone using version 2 for consistency to change the formatting to version 1. Make of that what you will.
Operators Reference, Pointer, R-Value-Reference, etc. In function arguments and any other variable delcaration there should be a space following the &amp;, *, &amp;&amp; qualifiers but not preceding it: Foo&amp;amp; wrap_foo(const Foo* const foo_ptr, Foo&amp;amp;&amp;amp; foo_rvalue, const Foo&amp;amp; foo_ref) { // ... } std::vector&amp;lt;Foo&amp;gt;* foo_vec_ptr = // ... for (auto&amp;amp; foo : *foo_vec_ptr) { // ... }  I am aware this is an onpupolar choice so let me explain: When we read out code in real life meaning actually read it out loud to someone in the same room, when someone asks us: &amp;ldquo;What is the type of const Foo* bar?&amp;rdquo; we answer &amp;ldquo;constant foo pointer&amp;rdquo; or &amp;ldquo;a pointer to a constant foo&amp;rdquo;. It&amp;rsquo;s the same the same with references, r-values, etc.. I consider the type qualifier to be part of the type and it should thus be grouped with the type in the declaration.
I will admit this falls apart when using the above mentioned init-declarator-list:
const Foo&amp;amp; first, // type Reference to a const Foo  second, // type const Foo  *third, // type Pointer to a const Foo  *fourth const; // type constant Pointer to a constant Foo  which is why I stated that when using this form, all variables should have the same type and type qualifier.
Logical Operators  Always use and, or, not instead of &amp;amp;&amp;amp;, ||, ! for boolean operands.
Always use &amp;amp;, |, ^, ~ instead of bitand, bitor, xor, compl for numerical operands. Similarly always use &amp;amp;=, |=, !=, ^= over and_eq, or_eq, not_eq, xor_eq for these operands.
It may be my lua background but I&amp;rsquo;m fond of the and, or, not in boolean expressions, many people from non-cs fields that primarily work in matlab or another very high level scripting language will have an easier time reading complex boolean expressions and using the keywords eliminates typos where the bit-wise logical operators are used accidentally triggering not error but performance overhead from non-short-circuiting expressions.
Numerical Operators There should be a space preceding and following the binary operators &#43;, -, /, %. The unary - should be used on variables as well as number constants in favor of -1 * 
int a, b; // wrong: c = 2*(a&#43;b); c = a*b; c = -1*a; c = std::modulus(a, a%b); // correct: c = 2 * (a &#43; b); c = a * b; c = -a; c = a % (a % b)  Unless in performance critical code conditions where every operation counts it is sometimes preferable to not mathematically simplify formulas for something if it aids in clarity.
Pre- and Postfix Increment Prefix incremenet / decrement should always be preferred over the postfix option unless specifically necessary to produce desired behavior:
size_t n = 0; // wrong: for (auto it = vec.begin(); it != vec.end() or n &amp;lt; 15; it &#43;= 1, n&#43;&#43;) // ...  for (auto it = vec.begin(); it != vec.end() or n &amp;lt; 15; it.operator&#43;&#43;(1), n&#43;&#43;) // ...  // correct: for (auto it = vec.begin(); it != vec.end() or n &amp;lt; 15; &#43;&#43;it, &#43;&#43;n) // ...   In many scenarios, post- and prefix operators are functionally equivalent but in that 1% of cases in which they aren&amp;rsquo;t using the wrong one may cause hard-to-catch bugs. Always defaulting to the prefix version tends to avoid this as much as possible.
Functions All function names should be lower-case snake_case. This includes lambdas however functors should be named according to the class naming specification (c.f. below). On declaration without definition, function arguments should be unnamed unless specifically necessary to give context or if there are multiple arguments of the same type:
// wrong void set_position(Vector2f position); void add_percentage(float); // expects [0, 1], not [0, 100] void set_name(std::string, std::string); // correct void set_position(Vector2f); void add_percentage(float zero_to_one); void set_name(std::string first_name, std::string last_name);  Member Functions Members functions that set one or more member variables should always be named set_* where * is the same name used when declaring the member in the .hpp source code.
class Color { private: float _r, _g, _b, _alpha; public: void set_rgb(float, float, float); void set_alpha(); // wrong:  void set_red(float); // correct:  void set_r(float; }  Similarly, getters should always be named get_*, however if the function returns a single bool, if that bool is a member variable named _is_foo it&amp;rsquo;s getter should be named get_is_foo(), however if that value is not a member it&amp;rsquo;s getter should be phrased like a yes-or-no question, for example:
class OsWindowHandler; class RenderWindow { private: std::string _id; bool _is_closed = false; public: void set_closed(bool b) { _closed = b; } bool get_is_closed() const // member bool  { return _closed; } bool is_focused() const // non-member bool  { return OsWindowHandler::is_window_focused(_id); } bool should_notify_on_close() const // non-member bool  { return OsWindowHandler::get_context_settings(__id).should_notify_on_close; } }  Lambdas Lambdas are very powerful in modern C&#43;&#43; and it can be very attractive to not add a new member function and instead just open a local lambda function inside another function. Lambdas should be used only where necessary, if you would call the potential member function from 2 or more places it&amp;rsquo;s almost never worth it to not just create the new function.
If you do have to use a lambda, if the lambda function itself is only called once inside it&amp;rsquo;s scope, consider making it anonymous, the most common place would be inside a set or sorting algorithm with a custom comparison. If more than 1 routine needs it and it needs to be a lambda, consider making it a member and binding it to an std::function;
A place I like to employ lambdas is for inline static initialization. Sometimes you need to call functions that do not return the object you&amp;rsquo;re trying to initialize like so:
class TexturedFoo : public Drawable { private: const Texture _texture; static inline const _window_resolution = []() { if (not RenderWindow::is_initialized()) RenderWindow::initialize_from_config(); return RenderWindow::get_resolution(); }(); public: TexturedFoo(std::string texture_id) : _texture([&amp;amp;]() -&amp;gt; Texture&amp;amp;&amp;amp; { auto texture = new_texture(); if (not texture.load_from_file(texture_id)) throw /... return std::move(texture); }()) {} }  I think this can be a very elegant way to initiate things that need a more complex routine than just calling it&amp;rsquo;s constructor, however thought should be put into wether it really is necessary to make this a lambda.
For the above example both initialization functions could just be a static member function of RenderWindow and Texture respectively and I would again argue that the lambda is only justified if that behavior is only called exactly once in the entirety of the library.
When using lambdas it&amp;rsquo;s almost always best to delcare them as [&amp;amp;](auto arg1, auto arg2) { //&amp;hellip;  nowadays. This way the compiler will decide which variables to capture themself and the argument type is automatically deduced. The capture should only be manually specificed if for example the invocation of a copy constructor is necessary. Similarly the trailing return-type  [&amp;amp;](auto arg1, auto arg2) -&amp;gt; foo { //&amp;hellip; should be ommitted unless specifically necessary.
I would advise you to check on the lambda docs page every year or so, it feels like the new C&#43;&#43; versions, C&#43;&#43;20 just recently and C&#43;&#43;23 recently, are very impactful on what is possible with lambdas and I would encourange everyone to try to stay up to date with the most recent way of doing things, most recently templated lambdas and constraints/concepts support.
Auto-deducing return types  It&amp;rsquo;s tempting to always declare the return-types as auto if possible, even if it&amp;rsquo;s not strictly necessary. It may often seem like the most modern way to do things but I would advise caution. If proper documentation isn&amp;rsquo;t done yet it can send potential collborators on the hunt through your function definition to find out what exactly your function returns anyway. Without auto one look at the function declaration would&amp;rsquo;ve sufficed. In my opinion, if auto is possible the following form should be used:
// wrong: template&amp;lt;typename Return_t&amp;gt; auto create_function() { return std::move(std::function&amp;lt;Return_t()&amp;gt;()); } // better: template&amp;lt;typename Return_t&amp;gt; std::function&amp;lt;Return_t()&amp;gt;&amp;amp;&amp;amp; create_function { return std::move(std::function&amp;lt;Return_t()&amp;gt;()); } // best: // @brief dummy function // @returns rvalue of type function&amp;lt;Return_t()&amp;gt; template&amp;lt;typename Return_t&amp;gt; auto create_function() { return std::move(std::function&amp;lt;Return_t()&amp;gt;()); }  By employing good in-code documentation the original dev gets to use the convenience of auto and the collaborators only have to take one look at the function declaration to know what type the function returns. In cases were returning auto is absolutely necessary (such as when the return type depends on template parameters outside the developers control) other than proper documentation there is no way of getting around needing to see the definition to truly understand what&amp;rsquo;s happening.
Classes &amp; Enums Naming Classes, Enums and other class-like entities are always named upper-case CamelCase. This includes structs, unions, enums, typenames and typedefs as well as using-declarations for any of these entities.
Enums As all Enum &#34;members&#34; are inherently scoped static constexpr constants, they should be in all-caps SNAKE_CASE. When each enumerator is manually defined with an expression, the enums type should also be manually defined as such: // wrong: enum Manual : int {ZERO, ONE, TWO} enum Manual {FIRST = 1, SECOND = 2, THIRD = -2} // correct: enum Manual {ZERO, ONE, TWO} // auto deduces int enum Manual : int {FIRST = 1, SECOND = 2, THIRD = -2}  An enum that&amp;rsquo;s inside the global namespace should be declared a scope enum enum class Foo. This is to avoid name-collisions which can be very annoying when an unrelated library you&amp;rsquo;re using already reserved an enum constant name you want but they didn&amp;rsquo;t scope their enum properly. If an enum is only used inside a very limited scope it is fine to omit the class for the convenience of not having to specify 3 scopes before using an enum constant.
namespace MyLibrary { // use enum class  enum class PublicEnum { //...  namespace detail { // no enum class okay  enum DetailEnum { //...  } class Class { private: // also okay  enum PrivateEnum { //...  } } // never put an enum in global namespace  Class vs. Struct If all member variables and functions of a user-defined type are public, struct should be used an no access specifiers (public, protected, private) should be stated.
In all other cases, class should be used and all members should be manually access specified.
// wrong: struct HSV { public: float _h, _s, _v; } struct Color { HSV to_hsv() const; private: float _r, _g, _b, _alpha; } // correct: struct HSV { float h, s, v; // no _ because members are public } class Color { public: HSV to_hsv() const; private: float _r, _g, _b, _alpha; }  File Organization: .hpp and .cpp Filenames  All programming language files should be lower-case snake_case. If a header holds a class named FooClass, the header should be named foo_class.hpp and similarly the source file should be foo_class.cpp.
If FooClass contains and internal class FooInternal declared in the same header then there should be a seperate second .cpp named foo_internal.cpp. This avoids having to recompile both classes when only changing something in one of them.
.h should not be used unless the entire header is exclusively written in C, not C&#43;&#43;. For files in other programming languages the most common programming language file-extension should be used, for example if your data representation for a video-setting file is in lua and you agreed on a .cfg file extension for configs, the file should be name video_settings.cfg.lua. Many programming and data representation languages do not care what their file is name but you should still take care to make it immediatly obvious what language a file is written in, collaborators should not have to open the file in notepad and try to figure out the language. Here is a non-exhaustive list of languages and file extensions I personally use:
 Language Extension   C&#43;&#43;  .hpp for headers
.cpp for source files    Python  .py    Java  .java    JavaScript  .js    Lua  .lua    Ruby  .rb    Perl  .pl    Go / Golang  .go    R  .R    GLSL  .glsl
.frag for Fragment Shaders
.vert for Vertex Shaders
   Data Representation: CSV  .csv    Data Representation: XML  .xml    Data Representation: Lua  .sav.lua for savefiles
.cfg.lua for config
etc.
   Data Representation: JSON  .json    Data Representation: Matlab  .MAT    I think a lot of these are standard and there&amp;rsquo;s surely many I&amp;rsquo;ve missed like all the html-related things like .md and .css but the point of this list was more to instill in you a point of consistent flexibility. If it doesn&amp;rsquo;t matter to the computer the extension is technically arbitrary, but the human part of your company should decide one exactly one answer anyway and stick to that at all times. If that answer overrides the script languages default extension like I do with .lua, it should be .yourextension.lua to allow people from outside your team to easily classify your files too.
Classes: Declaration and Definition I might be wrong on this but afaik in modern C&#43;&#43; the only reason to use the .hpp for declaration and .cpp for definition for a non-templated class is to reduce compile time. I&amp;rsquo;ve asked this question to multiple people and nobody was able to give me a solid answer, for cyclic linkage and multiple definition errors you can inline functions in the .hpp and static members you can also inline to allocate them without a .cpp. Furthermore templated classes require the definition to be in a .hpp anyway so it&amp;rsquo;s very confusing. This is how I&amp;rsquo;ve been doing it so far and it kinda depends on a decision made at the very beginning of development:
Header-only library If the library is header-only there should not be a single .cpp anywhere. The following form should be used:
Inside the same access-specifier (private, protected, public) region, the declarations immediately followed by the definition should be in the following order:
 using / friend class declaration static member variables non-static member variables Constructor, Destructor, Copy- and Move-Constructors Assignment operators other operators static member functions non-static member functions  Inside the class the access-specified regions should be in the following order:
 private protected public  All functions should be defined inline, for non template files the inline keyboard should be used. Similarly all static members, both in templated and non-templated classes, should be initialized inline. This may cause static initilizations to be sensitive to the order they&amp;rsquo;re declared in a file so be wary of that. If you&amp;rsquo;re not sure you can always use the construct-on-first-use idiom.
// template_foo.hpp template&amp;lt;typename First_t, typename... Args_t&amp;gt; class TemplateFoo : protected ParentFoo { private: static inline MemberType _static_private_member = //...  Function_t _private_member; void private_func(Args_t... members) const { // full definition here  } protected: using Function_t = std::function&amp;lt;First_t(Args_t...)&amp;gt;; static inline protected MemberType _protected_member = //..  MemberType2 _protected_member; template&amp;lt;typename T&amp;gt; First_t protected_func(T) { // full definition here  } public: static inline constexpr float PUBLIC_CONST_FOO = 42; float x, y; Foo() = default; Foo(Args_t...) = default; // &#43; dtor, move-assignment, copy-constructor, etc.  static float static_function() { // full definition here  } auto get_private_member() const { return _private_member; } } // non_template_foo.hpp class NonTemplateFoo : public TemplateFoo&amp;lt;int, int&amp;gt; { private: static inline int member = TemplateFoo&amp;lt;int, int&amp;gt;::static_function(); public: inline void NonTemplateFoo() { // full definition here  } }  Advantages of this form include an easier time linking things, being able to access the source code directly by just following back the header you included and less duplicated elements because you do not have to redeclare the template for every external definition. Disadvantages include far larger compile time and the fact an IDE is needed so collaborators can collabls longer functions or internal classes to rapidly access all declared functions signatures without having to scroll through thousands of lines. If that last point doesn&amp;rsquo;t fully make sense, let&amp;rsquo;s compare with the non-header only format:
Not Header-only Library If the library has at least one .cpp it is not header only and the following form should be used for all interal files from your library:
Inside the same access-specifier members should be declared in the following order:
 using / friend class declaration static member variables non-static member variables Constructor, Destructor, Copy- and Move-Constructors Assignment operators other operators static member functions non-static member functions  Inside the class the access-specified regions should be in the following order:
 public protected private  Note that this is inverted when compared to header-only libraries. This is because in non-header-only libraries the class body should exclusively contain declarations. All functions and static members should be defined in the .cpp or if that is not possible (for example because the class is templated) they should still only be declared in the class definition and then defined later in the .hpp as such:
// template_foo.hpp template&amp;lt;typename T&amp;gt; class TemplateFoo : public ParentFoo { public: TemplateFoo(); float get_member(); void set_member(float) const; protected: static Type _protected_member; private: void private_function(); float _private_member; } // ###################################################################  template&amp;lt;typename T&amp;gt; Type TemplateFoo&amp;lt;T&amp;gt;::_protected_member = // ...  template&amp;lt;typename T&amp;gt; TemplateFoo&amp;lt;T&amp;gt;::TemplateFoo() { // full definition } template&amp;lt;typename T&amp;gt; float TemplateFoo&amp;lt;T&amp;gt;::get_member() const { return _private_member; } template&amp;lt;typename T&amp;gt; void TemplateFoo&amp;lt;T&amp;gt;::set_member(float f) { _private_member = f; }  // non_template_foo.hpp class NonTemplateFoo : public ParentFoo { public: NonTemplateFoo(); float get_member(); void set_member(float) const; protected: static Type _protected_member; private: void private_function(); float _private_member; } // non_template_foo.cpp Type NonTemplateFoo::_protected_member = // ...  NonTemplateFoo::NonTemplateFoo() { // ... } float NonTemplateFoo::get_member() { return _private_member; } void NonTemplateFoo::set_member(float f) { _private_member = f; } void NonTemplateFoo::private_function() { // .. }  Advantages of this form include improved readability as with just one look a collaborator can get all the relevant external information about a class. Further compile-time is vastly improved which I&amp;rsquo;d like to put into context here because this point is honestly mostly dependent on personality.
Compile time is usually just seen as a necessary evil and since the user will most often not have to compile your product it&amp;rsquo;s somewhat irrelevant for the economical process, right? Well anytime your machine is compiling you&amp;rsquo;re not doing anything else. If this is just for a small app you only loose seconds which is irrelevant but if it&amp;rsquo;s for a big highly interlinked app and especially if either your machine is slow and/or you use a lot of compile-time execution (which you should nowadays) things can add up fast. For my biggest project recompiling all modules on my somewhat-decent laptop can take upwards of 90s. In an environment where rapid iteration is needed this can add up fast and I can imagine is a big reason people tend to gravity towards scripting language that don&amp;rsquo;t need to compile nowadays. No matter how you stand on the issue of compile time, wether you split the .hpp or you don&amp;rsquo;t is up to preference but you do have to decide early on stick to it. Again consistency is the most important thing, if you do something badly but the same everytime it&amp;rsquo;s easier understood than using 20 different elegant solutions that don&amp;rsquo;t look alike.
Comments &amp; Documentation In-File Documentation Unless otherwise dictated by the documentation API your project uses, the following form should be employed for all public or protected functions:  Tag Is Optional Description   @brief Never Optional  Short description of what the function does. Should be one sentence    @param Optional if function has no parameters or functions exists in private context  Description of function argument follower by &amp;ltname_of_the_parameter:. If parameter is unnamed in delcaration, the name is ommitted and replaced with a number instead (see example below    @returns  Optional if function returns void or function exists in private context  Description of the returned value. If functions returns auto, explicit mention of the function return type    @author Always Optional  Handle of developer, this may be the real name or for example a github user name. Ask collborators if they are okay with having their full name distributed and always give credit in highly collaborative scenarios such as crowd-sourcing    (no tag) always optional  Verbose description of the functions. Always put last after all the tagged content right before the actual function declaration    // @brief mix two foos using the ratio // @param 1: First foo to merge // @param 2: Second foo to merge // @param ratio: ratio of first to second foo, [0, 1] // @returns rvalue reference to newly merge foo // // Functions takes two foos and applies (...) // (...) // (...) // returning the now coagulated foos as an rvalue reference Foo&amp;amp;&amp;amp; merge_foos(Foo&amp;amp;, Foo&amp;amp;, float ratio);  For private functions or functions and classes in namespace detail the documentation can be substituted with a single line like this: // @brief merge using ratio Foo&amp;amp;&amp;amp; merge_foos(Foo&amp;amp;, Foo&amp;amp;, float ratio); 
While it would of course be best to document everything it&amp;rsquo;s a reality of software development that proper documentation will happen last. That&amp;rsquo;s why some tags are optional, it&amp;rsquo;s an incentive for developers to at least do the one-line version however it is not an excuse to then never extend the one-line version. It&amp;rsquo;s a temporary solution but a better compromise than not documenting anything.
Comments In case you haven&amp;rsquo;t noticed so far, comments should be inline-comments of the form // this is a comment concerning the next line auto next_line = ...  Notice the space after //. All comments should start at the smallest indent of the block meaning there should never be a piece of code left of the comment and the comment should be appropriately indented as such:
// wrong: for (size_t x = 0; x &amp;lt; n_x; &#43;&#43;x) { for (size_t y = 0; y &amp;lt; 0.75 * n_x; &#43;&#43;y) // x:y ratio should be 4:3  { doo_foo(x, y); // ...  } } // correct: for (size_t x = 0; x &amp;lt; n_x; &#43;&#43;x) { // x:y ratio should be 4:3  for (size_t y = 0; y &amp;lt; 0.75 * n_x; &#43;&#43;y) { doo_foo(x, y); // ...  } }  All comments should be in-line comments. This is mostly a matter of prefrence but I do have a valid reason: When debugging you often want to disable a piece of the program by &amp;ldquo;commenting it out&amp;rdquo;, i.e. putting a multi-line comment around it temporarly. If there are already multiline comments in that section this will break the commented-out section up which means you have to add multiple comments just to comment something out for 10mins. An exception to this rule are comments at the very beginning of the file such as copy right disclaimers or similar legal-related things. As they are at the start of the file they can never interrupt multi-line comments. If you do decide to include multi-line comments in the release they should be formatted as such:
/* * There should be an empty line above the first line * An each line should start with an asterisk * (...) * (...) */  Choice of Variable/Function Names Let&amp;rsquo;s do a little hypothecial, first I&amp;rsquo;d like you to look at this code completely out of context. I intentionally choose variable names poorly but without malice, maybe I&amp;rsquo;m new or I really didn&amp;rsquo;t have time to do this so I just submitted my first draft as a pull request
class Base {}; class Wrapper : public WrapperBase { // ... }; Pool { void start(size_t); std::condition_variable _var; std::mutex _mutex; std::queue&amp;lt;std::unique_ptr&amp;lt;TaskWrapperBase&amp;gt;&amp;gt; _q; std::vector&amp;lt;std::thread&amp;gt; _w; bool _res, _shutdown, }; void Pool::start(size_t n) { assert(_t.empty()); for (size_t i = 0; i &amp;lt; n; &#43;&#43;i) { _t.emplace_back([&amp;amp;]() { auto lock = std::unique_lock&amp;lt;std::mutex&amp;gt;(_mutex, std::defer_lock); while (true) { lock.lock(); _cv.wait(lock, [&amp;amp;]() -&amp;gt; bool { return _res || !_queue.empty() || _shutdown; }); if (_res || (_shutdown &amp;amp;&amp;amp; _queue.empty())) { return; } auto t = std::move(_queue.front()); _queue.pop(); lock.unlock(); t-&amp;gt;operator()(); } }); } }  What happens in this code? I&amp;rsquo;m sure many of you will be able to figure it out eventually but this isn&amp;rsquo;t a pop-quiz, it&amp;rsquo;s simply a demonstration of what a difference code-style can make to a third party. Let&amp;rsquo;s look at the same code again but overly commented in a vein effort to explain what is happening:
// empty base needed for unique pointer class Base {}; // Wraps tasks class Wrapper : public WrapperBase { // ... }; // variable size thread pool Pool { public: // ...  private: // initialize threads  void start(size_t); std::condition_variable _var; std::mutex _mutex; std::queue&amp;lt;std::unique_ptr&amp;lt;TaskWrapperBase&amp;gt;&amp;gt; _q; // task queue  std::vector&amp;lt;std::thread&amp;gt; _w; // worker threads  bool _res, // is threapool paused for resizing?  _shutdown, // is threapool shutting down? }; void Pool::start(size_t n) { for (size_t i = 0; i &amp;lt; n; &#43;&#43;i) { // add thread with routine  _t.emplace_back([&amp;amp;]() { // create lock for queue  auto lock = std::unique_lock&amp;lt;std::mutex&amp;gt;(_mutex, std::defer_lock); // keep checking if new task for taskqueue is ready  while (true) { lock.lock(); // wait at conditional variable until notified  _cv.wait(lock, [&amp;amp;]() -&amp;gt; bool { return _res || !_queue.empty() || _shutdown; }); // finish routine when resizing or shutting down  if (_res || (_shutdown &amp;amp;&amp;amp; _queue.empty())) { return; } // then get task  auto t = std::move(_queue.front()); _queue.pop(); lock.unlock(); // and run it  t-&amp;gt;operator()(); } }); } }  This is prefectly understandable, I think most people will immediately know whats going on and it could be argued that&amp;rsquo;s the only thing that counts but I would like you to remind of one of the goals of this style guide: elegance. Comments are not elegant, because operating under the paradigm of &amp;ldquo;only comment when absolutely necessary&amp;rdquo; we can rewrite the code without sacrificing clarity like so:
class TaskWrapperBase {}; class TaskWrapper : public TaskWrapperBase { // ... }; ThreadPool { public: // ...  private: void initialize_worker_threads(size_t); std::queue&amp;lt;std::unique_ptr&amp;lt;TaskWrapperBase&amp;gt;&amp;gt; _task_queue; std::condition_variable _queue_conditional_variable; std::mutex _queue_mutex; std::vector&amp;lt;std::thread&amp;gt; _worker_threads; bool _is_paused_for_resizing, _is_shutting_down; }; void ThreadPool::initialize_worker_threads(size_t n_threads) { for (size_t i = 0; i &amp;lt; n_threads; &#43;&#43;i) { _worker_threads.emplace_back([&amp;amp;]() { auto queue_lock = std::unique_lock&amp;lt;std::mutex&amp;gt;(_queue_mutex, std::defer_lock); while (true) { queue_lock.lock(); _queue_conditional_variable.wait(queue_lock, [&amp;amp;]() -&amp;gt; bool { return not _task_queue.empty() or _is_paused_for_resizing or _is_shutting_down; }); if (_is_paused_for_resizing or _is_shutting_down and _task_queue.empty()) return; auto new_task = std::move(_task_queue.front()); _task_queue.pop(); queue_lock.unlock(); new_task-&amp;gt;operator()(); } }); } }  I think it is appropriate to say that this version is just as clear and easy to understand as the commented version except this time the reader actually looks at the code. Someone trying to debug the first version will ignore the code completely on their first read because they don&amp;rsquo;t need it to understand what&amp;rsquo;s even going on but when they do they still have to deal with obtuse variable names. Using the more elegant minimally commented version allows for less fluff at no cost of clarity.
Closing Comments Here are some things I didn&#39;t know where to put structurally but that I still think are important to say: Version Control Commit as often as possible. Anytime you change file or continue to the next step of implementing/fixing something, create a new commit. The actual commit message should be what was just done, past-tense, not what you&#39;re about to do. The more granular the commits are the easier it will be to rollback when something goes wrong and similar to documentation well formatted messages will help you understand things instantly even 2 years later. Consider coming up with a labeling scheme each message begins with, I often see things like [FIX], [POLISH], [TYPO], [ISSUE#1234] etc.. Some teams even uses emotes which I do kinda like but not all shells support this so maybe alphanumerical would be better for now.
You&#39;d think having a million commits would get hard to manage but that&#39;s what pull-requests are for. They collapse all the tiny commits into one big package and by grouping them like this you both get the convenience of having the granularity on your machine but once it goes to the team or users they don&#39;t have to sift through pages and pages of messages to rebase. Test and Profile Frequently Testing modules frequently is non-negotiable. I recommend google test but an internal testing framework can also be used. Testing assures correctness and it can be used later to see if modules would interact properly were this new feature merge with the master.
Test small pieces of modules, you don&#39;t need to write a test for every function but a test should take no more than 5mins to finish. If you do need testing in general to run for 30mins should should create 6 smaller tests and run them sequentially. That way if you only change one part you don&#39;t need to run the giant test again to see if it works now. Profiling and Benchmarking are a more sublte issue because they aren&amp;rsquo;t always necessary in my opinion. For certain application yes, you should absolutely benchmark and especially if you&amp;rsquo;re currently trying to optimize already existing code the only way you can do that properly is to actually benchmark the code by running it to see if it actually got faster. There are best-practice ways for this such as using a steady machine or running benchmarks multiple times and using statistical analysis on the results but I won&amp;rsquo;t go into that here.
Don&amp;rsquo;t think you can spot performance problems just by re-reading code, always run it even if you have three doctorates and 40 years of experience humans make errors or overlook things no matter how good they are at what they do. Having solid proof of how things are better now in forms of data is also a better way to convince customers or collaborators that what you did was actually successfull.
I do realize that this can be overkill, you don&amp;rsquo;t need to squeeze out that last 5% performance increase for your app that starts a washing machine in your house. A good middle-ground I found is to have a profiling tool run in the background. For example for my game I have a seperate process that monitors how much time of the 1/60ths of a second (the duration of one frame) the rendering and simulation needs and how much time the engine has just waiting for the monitor to refresh. If nothing happens that index is at 0%, if the engine is so slow that it lags the screen the tool will notify me that the percentage is above 100%. I always have that number on the side of my screen, that way when something doesn&amp;rsquo;t quite break but behaves unexpectedly non-performant I know to investigate. When Optimization is Appropriate On the topic of optimization, don&amp;rsquo;t always go for the optimal way of doing things unless it&amp;rsquo;s necessary. Again, some for some environments it&amp;rsquo;s true, always do it the fastest way that&amp;rsquo;s what counts but be realistic about it. If you for example want to allocate user made objects and access them later by ID and you expect the user to at most ever make 6 objects then writing a specially-hashed map that objects them in sub-logarithmic time is really not worth it vs just putting them in an array and iterating over all indices everytime you need to access them. Similarly I would always think first before parallelizing something. Anytime that decision is made, the reason should be &amp;ldquo;we can&amp;rsquo;t make it sequential because x&amp;rdquo;. Sequential should always always be default, parallelization is something that introduce so much complexity and hard-to-catch bugs that I would honestly recommend not doing anything in paralell until you&amp;rsquo;re read to release and then paralellizing thing to make everything snappier though this requires a good softwaredesigner because things have to be designed in a way that would work both sequential and in paralell from the beginning
Always Refactor at least once When you realize you accomplished your taks or fixed a bug, take some time to go through your code again to pretty it up. I often set myself goals like &#34;today I will implement x&#34; and then at the end of the day when I run my test and it goes through I&#39;ll go &#34;done for today&#34; and leave and then I never touch that file again until something goes wrong. This is not good, just because something works doesn&#39;t mean it&#39;s done. It&#39;s not even about adding documentation, sometimes redundant stuff will be left over or an if-else branch has duplicate branches or you forgot to delete a commented out section or I you slacked off and didn&#39;t 100% keep to the style guide rules detailed above. Always doing at least one refactor will do wonders to keep your code clean and most importantly the more you do this the more you will just do things properly the first time. I&#39;m still in this learning process myself but actually challenging what I do through force of habit will get me closer to doing it right the first time over time. Epilogue If you read this far through in one go then I would like to thank you for your time. The main point of this piece was less to inform (nobody is reading these anyway) but to investigate my own way of doing things. Writing everything out forces me to argue my points to the imagined reader and it helps me structure and maybe questions my ways of doing things and like I stated at the beginning, adaptability and a willingless to discard tradition in favor of better things are the 3rd most important thing you can do with your coding style. C.
 As I evolve I&#39;ll probably change and keep this update so here is a list of edits: First published, Mai 18th 2021 Added Bracket Chapter, Mai 28th 2021 Added &#34;don&#39;t use else&#34;, Mai 30th 2021]]></content:encoded>
    </item>
    <item>
      <title>Defining Earthbound-Likes: A Genealogy and Personal Examination</title>
      <link>http://clemens-cords.com/post/earthbound-like/</link>
      <pubDate>Sat, 06 Mar 2021 23:00:00 UT</pubDate>
      <dc:creator>C. Cords</dc:creator>
      <guid>http://clemens-cords.com/post/earthbound-like/</guid>
      <description>Abstract   Earthbound (1994, SNES) has been hugely influential on both niche indie games and large mainstream AAA successes. This article proposes that a lineage of games, inherently linked by the specific design decision and artistics ideas they share, form a new genre: &#34;earthbound-likes&#34;. As Earthbound, Yume Nikki, various commercial RPG-Maker games such as OFF and LISA, the Mario RPG series and lastly Undertale are examined, properties that went on to be inherited by their successors and thus became part of the lineage are identified and collected to form a definition, a checklist of what makes a game an &#34;</description>
      <category domain="http://clemens-cords.com/categories/game-design">Game Design</category>
      <content:encoded><![CDATA[  Abstract   Earthbound (1994, SNES) has been hugely influential on both niche indie games and large mainstream AAA successes. This article proposes that a lineage of games, inherently linked by the specific design decision and artistics ideas they share, form a new genre: &#34;earthbound-likes&#34;. As Earthbound, Yume Nikki, various commercial RPG-Maker games such as OFF and LISA, the Mario RPG series and lastly Undertale are examined, properties that went on to be inherited by their successors and thus became part of the lineage are identified and collected to form a definition, a checklist of what makes a game an &#34;earthbound-like&#34;. The exact nature of the definition is stated and argued to be specific enough to warrant to be considered it&#39;s own genre. This article aims to give future generations a way to appreciate these often somewhat niche games for what they contributed to the genres successes and what they will surely go on to contribute to many games to come.   Table of Contents  Motivation
1.1 Some Examples
 Earthbound
2.1 Earthbounds Combat
2.2 Giygas
 RPG-Maker
3.1 Yume Nikki
3.2 OFF
 Super Mario RPG and it&amp;rsquo;s Sequels
 LISA
 And then came many Others
 Undertale
 An actual Definition
 Footnotes   1 Motivation I want to start this piece on a personal note, I was casually talking to a friend and I opened the conversation by showing them the following screenshot of a &amp;ldquo;Omori&amp;rdquo; (which is by all accounts a very good game) completely out of context:
 Figure 1.0: Battle Scene from Omori that I showed to my friend.  I just thought it looks nice, right? Their reaction was a lot stronger than I anticipated: &amp;ldquo;Looks neat but man I just get the vibe it&amp;rsquo;s a generic western jRPG in a cool package&amp;rdquo;. I poked further and they went on to elaborate that they feel like it&amp;rsquo;s just another one of &amp;ldquo;those&amp;rdquo; games, midly disturbing subject matter, quirky world, shallow jRPG combat and an amazing visual presentation. I do not necessarily agree with any of these and obviously my friend hasn&amp;rsquo;t played the game so I wouldn&amp;rsquo;t even consider it their opinion, really, but even after the conversation went on I kept thinking about what they said. It implies that Omori is part of a lineage, that an out-of-context screenshot of it&amp;rsquo;s battle system is enough evidence to not only know what very specific type of game it is but to also judge it as so alike every other such game that it becomes generic.
In this piece I will not argue for or against Omori or talk about it much at all, I won&amp;rsquo;t yet disclose if I agree with my friend or if they changed their opinion after doing more research, rather I would like to examine what exactly my friend was intuitively comparing Omori to and how we got to that point.
1.1 Some Examples  Figure 1.1: The equivalent of a Battle in Dujanah. (source: Rock, Paper, Shotgun)  As you may have guessed from the title, I will call the genre I think Omori is part of &amp;ldquo;earthbound-likes&amp;rdquo;. I think most will be familiar with the term &amp;ldquo;souls-like&amp;rdquo; which does have a set of fairly clearly formulated properties [1] but for the sake of argument I will give a single line definition: &amp;ldquo;A souls-like game is a game that was inspired by Dark Souls&amp;rdquo;. An earthbound-like, then, is a game inspired by Mother 2: Earthbound. Though much like with souls-likes, it goes far deeper than that. As I will aim to illustrate, earthbound-likes follow a clear timeline of sorts, as another game joined their lineage it based it&amp;rsquo;s ideas on those that game before it and over time, the genre and the set of ideas of all the games contained in it grew. Similarly to how souls-like incorporated ideas from Bloodborn or how metroidvania incorporated ideas not only from Super Metroid but the PS1 and GBA Castlevania titles. While all of these started with one game, today the genres identity can be traced to far more than just it&amp;rsquo;s originator.
For now here is a non-exhaustive, non-ordered list of games I consider earthbound-likes:
  The Mother Series (Mother 1, Mother 2: Earthbound, Mother 3)
  Mario RPG (SNES), Paper Mario 64, Paper Mario: The Thousand Year Door, Mario &amp;amp; Luigi: Super Star Saga, Mario &amp;amp; Luigi: Bowsers Inside Story, Marioa &amp;amp; Luigi: Partners in Time, etc.
  Most if not all of the commercially released RPG-Maker games, including but not limited to:
  Yume-Nikki,  Yume 2kki,  LISA: The Painful,  To the Moon,  OFF,  One Shot,  Escaped Chasms,  .flow,  Ib,  Dreaming Mary,  Suits: A Business RPG  The Witch&amp;#39;s House,  Jimmy &amp;amp; The Pulsating Mass,  Dujanah,  Hylics     Undertale and it&amp;#39;s sequels
   (Note: please visit any of these links in order to buy and/or download any game mentioned here. I whole-heartedly recommend them all and some of them are free!) 
I think most people that are familiar with these games can already form a somewhat clear picture of what I mean when I say earthbound-like but let&amp;rsquo;s build my definition step by step, with the first step being, of course:
2 Earthbound Earthbound is the Dark-Souls of ... When Earthbound released it tanked in sales, became relatively obscure almost immediately and didn&#39;t even get a PAL import because Nintendo didn&#39;t want to bother. To this day Mother 3 (which is objectively a great game and whos Protagonist is in Smash Bros) still has not received official localization, that&#39;s how traumatic Earthbounds lack of success was to them. Many blamed this on the stink-based advertising but I personally believe it was mostly due to the fact that the game was non-optionally bundled with a strategy guide, raising the price above what similar games would&#39;ve costed off the same shelf. Most children in 1994 weren&#39;t willing to take a risk and spend their entires years gaming-budget on a game actively marketed as super weird and unlike any other game they knew.
That last point is not technically true however, Earthbound was very much alike it&#39;s prequel: Mother 1 for the NES (later released as Earthbound Beginnings on the Nintendo Eshop. I don&#39;t want to dwell to deep on Earthbound history and Mother 1, there are entire websites dedicated to it and this piece will already be long enough as it is but what I do want to note is that Earthbound wasn&#39;t iconic when it came out, no, it took some time to become iconic. I personally played the game for the first time in 2014 on an emulator, however I (probably becuase of being based in europe) was relatively late to the party. Most people who would eventually get really into Earthbound played it around the start of the 2000s.  Figure 2.0: Saturn Valley in Mother 1. (source: Earthboundcentral)  So what made Earthbound special? It was a good game but I think what made it stand out is how unlike traditional jRPGs [2] it really was. It was set in a facsimile of suburban america, you had to actually put your money into an ATM, enemies were rabid dogs, angry ducks, homeless people but then there were also three psychicly connected moles, full-sized dinosaurs, a guy that lives inside a dungeon and the dungeon is also a statue that walks around, you get the idea. It&amp;rsquo;s story was almost nonsensical, an episodic fever dream of walking around an increasingly surreal world erasing pencil-shaped roadblocks made out of metal and waiting behind a waterfall for 1h in real time to fight a literal pile of vomit. Earthbound was odd, quirky and - unlike it&amp;rsquo;s advertising - incredibly charming. Pairing that with decent graphics, sound and controls even by todays standards and one of the best soundtracks in all of gaming and you get a cult classic. However while all of these things are important they only make up half of Earthbound gameplay.
2.1 Earthbounds Combat  Figure 2.1: Battle against a Moonside &#34;Crazed [Street] Sign&#34; in Earthbound.  Like all traditional jRPGs gameplay in Earthbound exists in one of two states: in the &amp;ldquo;overworld&amp;rdquo; you walk around talking to people, maybe solving some light puzzles. There you can walk into (or otherwise interact with) an enemy (who may or may not be also trying to interact with you) to start a&amp;hellip; let&amp;rsquo;s say &amp;ldquo;abstract representation&amp;rdquo; of a battle. Instead of having a sense of place Earthbound simply has static sprites floating in front of a 1994 SNES SFX-Chip-facilitated shader fever dream. Pairing this with the weird and unique enemy designs (c.f. Fig. 2.1 above) made for an extremely unusally looking battle system. What took place underneath that asthetic however was very much like a traditional jRPG; one might maybe even call it generic. People usually don&amp;rsquo;t feel that way when thinking about their memories with Earthbound though. This is for multiple reason: firstly the fact you&amp;rsquo;re fighting all these crazy creatures really does inherently bring a lot of flavor and memorability into battles. Along the Pokémon-esque rock-paper-scissor weakness-means-more-damage system Earthbound also includes a lot (and I mean &amp;ldquo;a lot&amp;rdquo; as in &amp;ldquo;too many&amp;rdquo;) esoteric status ailments in a somewhat vain effort to add complexity. While there&amp;rsquo;s traditional statuses like being asleep, poisoned and burning there&amp;rsquo;s also: having fleas, having a runny nose, feeling homesick, having a mushroom grow on you, feeling &amp;ldquo;strange&amp;rdquo;, having a sunstroke. The actual mechanical purpose of these is fairly opaque and I think most player could not name the actual gameplay impact most of these have but that didn&amp;rsquo;t really matter, much like the choice of setting and enemies the quirky statuses added flavor rather than depth - which isn&amp;rsquo;t a bad thing. It is fun to play a game on your SNES and find out mid-battle that your character can have a runny nose and miss a turn because they&amp;rsquo;re sniffling and if for no other reason than this, all the statuses were worth the inclusion in my opinion.
 Figure 2.1.1: Animated .gif showing the rolling health meter. (source: lparchive)  Another often remembered feature is the &amp;ldquo;rolling&amp;rdquo; health bar (c.f. Fig. 2.1.1). When loosing or gaining health, rather than instantly adding the given amount to the party-members healthbar it instead adds to it slowly and in real time. &amp;ldquo;Real time&amp;rdquo; meaning that while you menu or wait for attack animations to play out the health slowly &amp;ldquo;rolls&amp;rdquo; up or down. The intended emergent gameplay moment of this is for you take a huge hit that would ordinarily instantly kill your character and instead having the health roll down at rate that now incentivizes fast action, if you&amp;rsquo;re quick enough (and you get lucky enough that the other animations aren&amp;rsquo;t too long) you may be able to select a healing item at which point your health immediately starts rolling up instead, avoiding death. I think most player will have at least one moment like this and for that alone this system was worth the inclusion but I also think that much like the numerous weird status ailments it only provides the illusion of depth. If you were to replace the system with the traditional die-instantly way of taking damage you would would only need to redesign a single-digit number of encounters[3] over an entire playthrough and everything else would go the exact same way mechanically. All that really changes is the potential emotional impact. I don&amp;rsquo;t want to critique this system too much, I think it is a good system but much like the status ailments it&amp;rsquo;s inclusion did not result in adding significant depth, rather it added flavor. I think it&amp;rsquo;s important to recognize this and (spoiler alert) it will be a fundamental principle in games that followed in Earthbound footsteps.
2.2 Giygas  Figure 2.2: Animated .gif showing the background texture and shader during the battle against Giygas. Note the vague shape of a fetus with a brain and umbilical cord in the pattern.  Giygas is the penultimate villain and final boss of Earthbound and it&amp;rsquo;s honestly hard to describe what it even is in-lore. It&amp;rsquo;s an abstract, non-corporeal personification of evilness in general. You get to the final fight by first traveling into the future where all life on earth and even the atmosphere is completely destroyed, you enter a womb-like structure and when fighting against it it&amp;rsquo;s the first thing you notice is that unlike any other enemy in Earthbound is not just a static sprite in front of a shader background (except for pokey at during the first phase). Rather the shader background is the Giygas resulting in it feeling ethereal and omnipresent. Shigesato Itoi, Earthbounds designer, credited the experience of walking into the wrong cinema and seeing a very violent scene as a young child as inspiration for Giygas presentation and I think it is one of the most memorable villains in all of gaming, not because of writing or mechanics but because of it&amp;rsquo;s position and the tone of the encounter relative to the game it resides in. Up until this fight Earthbound was really light-hearted, there were some midly disturbing concepts like being in a cult and zombies but at five-seconds-before-midnight hour of the plot Earthbound goes from Pokémon Villains level of disturbing to full Lovecraft out of nowhere. I personally think that this and only this is why it Giygas had such an impact. The things that make Earthbound special and memorable aren&amp;rsquo;t inherent to it&amp;rsquo;s actual properties, rather it&amp;rsquo;s those properties in relation to the established norms of traditional jRPGs, of kids games in general and even of Earthbounds own plots tone. Earthbound is a traditional jRPG in every way except in affect, it tries very hard to be unlike them and for most people it succeeds. Rather than adding to the gameplay in a mechanically meaningful way it adds flavor, so much flavor in fact that instead of seeming try-hard it blew past that and ascended to cult status.
I may have been coming off as negative so far but I do want to explicitely state here that Earthbounds is an amazing game, amazing despite it&amp;rsquo;s gameplay and frankly that gameplay barely matters if all other parts are this compelling and unique.
&amp;hellip; that is until it wasn&amp;rsquo;t unique anymore like 10 years later.
 Let&amp;rsquo;s quickly take inventory of some of Earthbounds properties. Trust me, I&amp;rsquo;m going somewhere with this:
 quirky and light-hearted world
 surreal, character-introduction-driven plot (each chapter corresponds to one party member)
 very literal plot (there are no big themes or metaphors tackled and everything that happens happens diagetically)
 shallow jRPG battle system with great presentation
 mechanics are added for flavor, not for depth
 plot suddenly shifts to very dark subject matter during the final moments
  3 RPG-Maker   Figure 3: UI for creating a battle in the first english version of RPG-Maker. (source: steam)  So you really liked Earthbound and you want to make your own Earthbound, what do you do? Well you could get a degree in computer sciencr and learn to program and spend 3 years in your bedroom creating a game from scratch or you could just take Earthbound and&amp;hellip; make it yours by modifying the actual game for the SNES with hacking tools. This is called romhacking and I only want to mention Earthbounds romhacking scene because of one very important actor that will have their start here, nevertheless even in 2000 people wanted to make their own Earthbound and while they didn&amp;rsquo;t technically need to be able to code it was still kinda hard for the non tech-literate.
That was until in late 2000 RPG-Maker was created. Intended as 45% Engine, 45% development tool, 10% publishing platform, RPG-Maker boiled traditional jRPGs down to their purest form. It was literally designed to be the tropiest of jRPGs, turn-based combat, high-fantasy setting, tile-based movement, 2d-sprites, the works. It was also designed to be incredibly easy to use, much like what Flash did for animators, with RPG-Maker every person with a small amount of determination and even smaller amount of money can create their own game. These games were inherently limited though. Modifying the RPG-Maker engine is hard, not only do the Terms of Service not really like that you try but it just doesn&amp;rsquo;t give the tools and absolute freedom some games need. For example (and this is specific mostly for earlier versions of the engine) games had to run in a tiny window at native resolution, the engine was terribly optimized, displaying fonts broke constantly, features were slowly getting rolled out with limited backwards-compatibility, that is to say it wasn&amp;rsquo;t a very good game engine back then (I heard it&amp;rsquo;s pretty decent nowadays though) but what it was very good at was giving players who were really into Earthbound but not at all into computer science a tool to make their own. And so they did.
3.1 Yume Nikki  Figure 3.1: KyūKyū-kun, a non-interactable NPC in Yume Nikki who rubs a staricase, emitting a scrubbing noise similar to plastic gloves on glass. (source: giantbomb)  Yume Nikki was released by the highly enigmatic &amp;ldquo;Kikiyama&amp;rdquo;, the games name in japanese translates to &amp;ldquo;dream diaries&amp;rdquo; and fitting of it&amp;rsquo;s title it&amp;rsquo;s incomprehensible yet incredibly emotionally affecting. It was made with a very old version of RPG-Maker and I can&amp;rsquo;t help but think that that is why Yume Nikki chose to completely forgo one of the defining features of jRPGs: turn-based combat. Yume Nikki is a jRPG that&amp;rsquo;s all overworld. There is no strategic gameplay and the game consists of just kinda walking around, looking at things. Today some would call Yume Nikki a walking simulator, you do explore a set of vast worlds but I&amp;rsquo;d as the term &amp;ldquo;walking simulator&amp;rdquo; has some connotations I&amp;rsquo;d rather call Yume Nikki an RPG-Maker powered interactive art piece, mostly focused on illustrations and athmospheric music.
It has multiple worlds to explore, visually distinct from each other but very same-y within themself owing to RPG-Makers tile-based maps. What limited Interaction with the environment there is happens through &amp;ldquo;costumes&amp;rdquo;, collectable items you can pick up and equip that may just change the sprite of the player character or may interact with other objects like for example brandishing a knife to attack certain enemies. Others have done great work describing this game and what is appealing about it far better than I ever could so instead I would like to focus on a few very specific thing about it. In Yume Nikki you play as a girl that is implied to be a Hikikomori, a young adult that basically refuses to leave their room instead exclusively engaging in solitude and media. In-game you start in this room and to access the different worlds you use the titular dream journal. This way of accessing the actual games worlds frames them as inherently dream-like and thus explicitly a product of the main characters psyche. For example in one world you can see a car crash victims body lying on the street bleeding out and while there is neither dialogue nor much in the way of interaction the framing of the game cements this imagery as a representation of trauma of the main character. As Madotsuki (that&amp;rsquo;s her name) is basically a blank slate and the project feels so much like outsider-art made by one person far away in japan it is reasonable to see people project that trauma onto the dev themself. The car crash scene moves the goalpost in a way where now every single element of the mostly non-sensical world can possibly be interpreted as a metaphor and projected onto the main character like this.
I think that this. along with the trance-like experience of walking around surreal, beautiful but highly repeptitious worlds with athmospheric music and no goal or purpose other than to see more is why this game is has such an impact on people. There is a giant, very passionate fan-community around this game both in Japan and the US. I genuinely cannot overemphasize how huge this game is in certain circles, every character in the game even if they literally have no purpose other than being a visual element has their own lore and fandom. Most notably is the almost main-stream famous Uboa (c.f. Fig. 3.1.1 below), but no matter how purposeless the visual elements are to the gameplay, they all become functionally equivalent in the audiences mind because all of them become a piece of expressionism, seemingly aiding in the exploration of Madotsukis mind.
Yume Nikki is a very strange game, it has no relation to traditional jRPGs other than those inherited by RPG-Maker but by being so strange yet endearing it has itself become an influence on a whole new generation of indie-devs. Maybe it is because of the RPG-Maker community being a breeding ground for indie devs but over the years most games inspired by Earthbound were made by people who were also in turn inspired by Yume Nikki and I think seeing this game as the Castlevania of Metroidvanias, the Bloodborne of Souls-like, one of the ur-members of the earthbound-like lineage sheds light on not only it&amp;rsquo;s influence but on what made the game so interesting even in 2003.
 Figure 3.1.1: There is a very low chance that operating the light switch in this house in Yume Nikki leads to a non-interactable, jump-scare-like encounter with Uboa. [4]   Similar to the chapter on Earthbound I would like to take inventory of Yume Nikkis properties:
 surreal world that is a metaphorical exploration of the main character damaged psyche
 made in RPG-Maker by a single person overseas
 no mechanical or strategic challenge
  Let&amp;rsquo;s look at another early-gen RPG-Maker game made by an anonymous developer in a non-english speaking country:
3.2 OFF OFF may be a bad game. OFF is also an important game. I don&amp;rsquo;t actually want to judge the games quality but unlike Yume Nikki I don&amp;rsquo;t consider it a pillar of the genre, rather I would like to use OFF as a case-study to stand in for many, many other RPG-Maker games that are in the same vein. I&amp;rsquo;d like to show you two screenshots from OFF, hopefully without context. Try to only look at the first, read the paragraph below it and then look at the second
 Figure 3.2: OFFs overworld  I think most would recognize that this is a RPG-Maker game. It&amp;rsquo;s very basic, just solid colors and an overworld sprite so bland you could be forgiven if you think it&amp;rsquo;s as a placeholder. Movement feels terrible, the puzzles are terrible, the dialogue is horrendously translated and it runs in a tiny 400px x 300px window that cannot be maximized. Now let me show you the second glimpse of the game:
 
Figure 3.2.1: Timestamped video of the battle against one of the chapter bosses in OFF.
It&#39;s enough to watched ~25s of the video, note the music and arstyle of the enemy sprites. (source: Kairin on youtube)   Now, I don&amp;rsquo;t actually know how long you&amp;rsquo;ve kept watching but hopefully you experience at least some of the spark this game emits. It&amp;rsquo;s still rough, the sprites for the party are literally just circles and a pixelated .png, it&amp;rsquo;s still a vanilla RPG-Maker game with terrible balancing and very little in terms of strategy but despite all of this it looks and sounds completely unique. The hand-drawn (some may call it doodled) look was achieved by literally drawing the sprites on paper and scanning the pages, down-res them and put them in the game without further work. Now this might not be totally true but what I&amp;rsquo;m trying to make note of here is that the games graphic do give you that feeling. Once you&amp;rsquo;re confronted with realizing this it&amp;rsquo;s the opposite of immersize, instead the process of creating the game was made obvious and is instantly recognizable. If you actually play the game you&amp;rsquo;ll notice that not just the battles do this, cutscenes will have scanned-in prints from old real-world library books, the dialogue reads like someone with a decent knowledge of french and a&amp;hellip; suboptimal knowledge of english spend 2 evenings google-translating all the files in the RPG-Maker repo but none of that matters for the actual emotional experience of the game. It&amp;rsquo;s process over product, form over meaning and much like Yume Nikki or many contemporary art installations it confronts the audience directly with who made it and how they did it. In OFFs case one might say a bored french musician who just needed an outlet and in Yume Nikkis case opinions vary from a traumatized japanese hikikomori who wanted to translate their dispair into a game to just a talented artists with a strong sense of how to do expressionism well but what is the truth in both of these cases is that the game becomes an examination of the person who created it. RPG-Maker games are rarely made by studios, they&amp;rsquo;re rarely made by experienced devs, most often they are made by a single person with no budget and no coding experience who wanted to make something important enough for them to pour hundreds of hours of effort into creating it and I think if you peak behind the curtain even slightly, most every RPG-Maker game regardless of quality carries this connotation.
Let&amp;rsquo;s do our inventory for OFF:
 surreal world that is used as a framework to exhibit the developers visual and musical art
 made in RPG-Maker by a single person overseas
 no mechanical or strategic challenge
  4 Super Mario RPG, Super Star Saga and Paper Mario 64 Taking a break from no-budget indie games I&amp;rsquo;d like to circle back to a first-party game that had all the funding in the world and (unlike Earthbound) matching success. In 1996 Nintendo joined forces with the at that point incredibly critically acclaimed square who was just fresh of Final Fantasy 6 (SNES), one of the best jRPG of all time, to use the lovable plumber known for jumping when you press A in a&amp;hellip;turn-based jRPG? I think the devs were well aware of the discrepancy inherent in that concept and proceeded to solve the issue with a fresh and ingenious idea that went on to carry an entire franchise worth of sequels to this day.
 Figure 4.0: A basic battle against a flying koopa enemy in Super Mario RPG: Legend of the Seven Stars (SNES). Note that in order to execute the shell kick at maximum damage the player has to time the A press with the animation of the shell hitting the floor in front of mario.  They created in my opinion one of the best mechanics ever implemented in a turn-based rpg: action commands. Instead of just menu&amp;rsquo;ing and hitting A to select a move you have to also time an additional A-press in what may be described as a quick time event (QTE) nowadays. In Super Mario RPG (SNES) said QTE was communicated purely through animation, there&amp;rsquo;s no &amp;ldquo;hit A now&amp;rdquo; prompt, instead you have to time the press based on what&amp;rsquo;s happening on screen. This system is fuzzy, much like a rhythm game the closer to perfect you do the input the more damage the move will do. As the game went on, action commands got more and more complex, from simply pressing A once when attacking or defending to pressing A frame-perfectly 100 times in order to execute the highest damaging combo which is used by speed runners to obliterate every boss in no time. Later games in the series would incorporate more than just one button and with better technology better animations were made to accompany them. I would like to quickly distinguish between traditional QTEs (as seen in Resident Evil 4 for example) and action commands. In my opinion it comes down to this: a) does failing the button press result in a fail-state and b) does perfectly executing the command result in an additional effect on top of the default outcome. In RE4 if you don&amp;rsquo;t press the button fast enough you get eaten by the big fish and have to reload a save, hitting it perfectly is the default state, you don&amp;rsquo;t get any additional effect, it&amp;rsquo;s simply a challenge to overcome so the game can continue. In action games like Bayonetta, failing the QTE will result in a fail-state however doing the QTE well (for example hitting a button really fast) will not only let you continue with the game but get a bonus reward of extra currency or points. In the Mario RPG series failing the QTE does not result in a fail-state, for some moves it does not result in any punishment compared to the default of not doing anything like it was Final Fantasy at all. You can play Mother 3 without ever hitting a single QTE and it&amp;rsquo;ll play just like Earthbound but if you do hit that games rhythm based action command your attack will deal extra damage. That is the difference: failing a traditional QTE results in a fail-state, winning continues the game. Failing an action command doesn&amp;rsquo;t do anything, completing it result in a bonus. This classification is on a spectrum with RE4 on the other end and something like bayonetta square in the middle. Either way, the point of action commands was to replicate another more mainstream genres gameplay during a game state that originally did not have any gameplay whatsoever (like during a cutscene or while watching your attack animation play out) and even in 1996 it succeeds at that, replicating what is very similar to timing a jump in a traditional Mario game.
 Figure 4.1: Action command in Mario &amp; Luigi: Superstar Saga (GBA) (spanish localization pictured here). At higher difficulties the explicit button prompts will disappear and the player is tasked to memorize when to press A or B for maximum damage.  I often talk with people about jRPGs who have never liked them and their main reason for doing so is that they want &amp;ldquo;actual gameplay instead of just pressing A on a menu&amp;rdquo;. While the validity of this claim is questionable I think what is meant by this is that traditional jRPGs do not offer mechanical challenge like many of the other most profitable genres in gaming. Action games, fps, platformers, even casual mobile games like angry birds are by their very core about tasking the player with completing a mechanical challenge first and foremost. Games about strategy and simulation tend to be more niche and I think herein lies the problem, it&amp;rsquo;s not that many traditional jRPGs lack mechanical challenge, it&amp;rsquo;s that that mechanical challenge is rarely (especially in earthbound-likes) substituted with strategic gameplay. Few would call a Paradox a game about clicking on menus because it offers strategic and intellectual depth instead. Earthbound-likes - at least those we&amp;rsquo;ve looked at here so far - do not offer strategic depth. That doesn&amp;rsquo;t make them bad games, it just means it leaves their mass-market appeal below those of more traditional games. That is until Mario RPG and later (and probably most succesfully) Paper Mario 64 presented the same earthbound-like package except this time fights stimulate your hands as well. It worked, big-time, I know many people who would never finish an RPG-Maker game or even call themself interested in RPGs but who love Paper Mario to death and unlike Earthbound, Nintendo to this day keeps some of their first party studios busy developing Mario RPG games for their consoles.
I don&amp;rsquo;t think that &amp;ldquo;Final Fantasy but with mario and QTEs&amp;rdquo; fully represents the true appeal of the Mario RPG series. Unlike most other Nintendo properties the writers of the games and especially Super Mario RPG and Super Star Saga were given complete freedom. Both games do share enemies and elements with other Mario Games but they might as well take place in a paralell dimension, completely new art styles, original characters, powers, universes (super star saga&amp;rsquo;s world is based on beans instead of mushrooms). Instead of the personality-less version of their platformer-self, characters like Bowser or Mario himself are fleshed out so much it&amp;rsquo;s fair to call them wholly original with a light Mario paint-job.
 Figure 4.2: Battle against Culex, a hidden boss battle in Super Mario RPG (SNES) that vastly differs in visual presentation and difficulty from the base game and any Mario game.  What I&amp;rsquo;m trying to say is that the Mario RPG series aren&amp;rsquo;t really like other Mario games, instead they are alike arthbound-likes: A surreal yet quirky setting and visual design, biting wit and meta-commentary on the genre of jRPGs, the works. Mario &amp;amp; Luigi: Super Star Saga even has the sudden unexpectedly disturbing final boss fight (c.f. Figure 4.3 below). I can&amp;rsquo;t say if the devs were directly inspired by Earthbound but what I can state with confidence is that future earthbound-likes were very much inspired by the Mario RPG series. Suddenly, non-strategic jRPG combat felt less tolerable, you don&amp;rsquo;t have to make it complicated but you do have to make it unique or at least mechanically &amp;ldquo;more&amp;rdquo; than just pressing A in a menu. Not all games used the same system as Mario RPG, they may insert a rhythm mini game, a shmup or platformer, a fighting game, a dating sim, anything really. What genre to pull gameplay from is truly arbitrary and I think the wide variety of what devs would translate Mario RPGs action commands into shows how earthbound-like combat isn&amp;rsquo;t able to stand on it&amp;rsquo;s own in a post-Mario RPG world.
I want to explicitely state again: jRPG combat doesn&amp;rsquo;t have to be hard or complex, games don&amp;rsquo;t have to have good combat to be good games at all. I want to instead just neutrally remark that adding mechanically interesting gameplay on top of the quintessential and thus generic jRPG gameplay without modifying that base is a very common trait for earthbound-likes. That is what Mario RPG went on to bring to the lineage, action commands are genius and it turns out they do especially well at making vanilla jRPG combat more exciting and appealing which happened to be exactly what Earthbound-likes needed to get big.
 Figure 4.3: Mario &amp; Luigi: Super Star Sagas (GBA) last boss. During this attack animation the player has to time the A and B presses independently to make Mario and Luigi jump respectively.  Instead of listing multiple things for this chapters inventory I&amp;rsquo;d instead just like to note a single property of the Mario RPG Series again: The battle system was made interesting not through strategic or conceptual complexity but through insertion of out-of-genre, mechanically interesting gameplay.
5 LISA (Content warning: pixelart depictions of body horror and gore, descriptions of sexual violence and domestic abuse towards children. Skip to here to avoid them)
LISA: The Painful (2014) is another RPG-Maker game, it was made in RPG Make VX Ace which unlike the version Yume Nikki and OFF were made in does offer developers a lot more freedom. LISA breaks with the trend of top-down 45° angle graphics and instead migrates to a 2D side-view, also including light platforming elements along with it. Just like Mario RPG, LISA also slotted another genre into battles: this time more akin to fighting game combos or maybe comparing it to DDR would be more correct. Instead of selecting moves from a menu you enter button combinations and depending on which permutation was inputted, the character will do a certain special attack.
 Figure 5.0 : The player inputs a series of button presses to trigger a special attack &#34;dropkick&#34; and deal additional damage in LISA: The Painful (PC). (source: comic vine)  While at first glance both the platforming and combo system make LISA a fairly unique RPG-Maker game I don&amp;rsquo;t actually want to bring LISA up just because of this. What makes LISA unique, especially in 2014, was the subject matter and how dark LISA was truly willing to be. Without going into story details, LISA has explicit scene of the Protagonist torturing or murdering someone, of being gunned down and riddled with arrows, slowly and agonizingly bleeding out. Enemy factions commiting essentially genocide and through out the game there are flashbacks to the protagonists live which shows (not alludes to) domestic abuse towards a child that was never loved and ravagely beaten by their friends and family, you get the idea. LISA is dark, I don&amp;rsquo;t consider LISA a very profound game, it explicitely contrasts these dark themes with levity and jokes about someone pooping in the open on a spider and homoerotic barbers construction a middle finger out of their combined hair but underneath all the edgyness is a core that many players will find emotionally exposing and very real.
 Figure 5.1 : &#34;Joy Mutant&#34; throne for the Trumpeter exhibiting Akira-esque body horror.  Like OFF, Lisa was made by essentially one person, also like OFF not very much is known about them. They intentionally used fictional characters from the game to release their music and to my knowledge their real name isn&amp;rsquo;t known, yet the sincerity and realism of how domestic abuse was portrayed in LISA let many people form a parasocial relationship with the main character and maybe even the perceived identity of the dev. I&amp;rsquo;d like to clarify here that I am not saying that either the dev or someone close to them has experienced domestic abuse or any of the atrocities in LISA, rather I&amp;rsquo;m pointing out that through death of the author the devs persona, the entity that seemingly made a game about all these intensely personal things, will have some audience members project those experience onto them just like in Yume Nikki. Yume Nikki dealt with loneliness and to some extend trauma but LISA not only went more towards the trauma side but showed it in explicit, disturbing detail.
As we&amp;rsquo;ve gone through the other games so far every game had a design decision or idea that went on to be inherited. LISA does have many original design decision but what I think LISA contributed most importantly is taking the lid of the how dark you&amp;rsquo;re allowed to go. Giygas was dark for what Earthbound was game but does not come close to themes tackled in LISA. LISA took away the stigma of RPG-Maker games having to imitate Earthbound in a way where most of it is quirky, light and child-friendly. LISA proved commercially and politically that you can do pretty much whatever you want but I do want to note that LISA isn&amp;rsquo;t tasteless, it doesn&amp;rsquo;t depict graphic rape scenes or realistic gore but for what is achievable in RPG-Maker, LISA went farther than any big successfull earthbound-like has gone to up to that point.
  Figure 5.2 : Scene from LISA: The Joyful (2015) showing a mutant cradling a child with the desecrated and deformed corpses of two seminal characters nearby   Much like LISA does itself I&#39;d like to insert somewhat arbitrarily placed levity here by noting that LISA: The Painful has a hidden area in which there is a school of fish with tiny feet living on land. One of the fish is a lawyer who you recruit and if you use his special attacks in battle he damages the enemy with paper cuts and the force of the justice system.   Just as before I&amp;rsquo;d like to take inventory of LISA properties before we continue:
 profoundly dark and disturbing subject matter that is thematically motivated by the main characters psyche
 quirky and sometimes funny plot
 little strategic challenge
 inserts an out-of-genre system to make battle more mechanically interesting
 made in RPG-Maker by a single person
  6 And then came many others With dev-tools improving and earthbound-likes finding widespread indie success, RPG-Maker games went from being an underground thing for devs who didn&amp;rsquo;t feel up to creating their game from scratch to a valid engine, delivery tool and genre. As a result many such games followed suit, from simple made-by-one-teenager games that don&amp;rsquo;t really have anything good to offer but still had enough love and effort poured into them to have been worth the time and purchase to multi-person teams with tens of thousands worth of budget and capital to be spend on artists and writers. RPG-Maker stopped being exclusively for amateurs and much like Yume Nikki all this time ago many games wouldn&amp;rsquo;t really be classified by many of their fans as traditional jRPGs at all.
Titles like To the Moon are more of a 2D interactive story than an RPG, yet it saw mainstream success and is regarded as one of the most emotionally impactful games from that decade. Here&amp;rsquo;s a non-ordered list with a quick blurb to highlight many of the games that came out during this period:
i) Oneshot: a RPG-Maker game where you can only die once, the game locks up afterwards so you really do only have &amp;ldquo;one shot&amp;rdquo; ii) Yume 2kki, an official sequel sanctioned by the original dev iii) Mother4, a direct sequel to Mother3 made by a team of talented non-nintendo devs. Sadly while nintendo refuses to support the mother series they do still seize and desist anything that tries to do it for them (#freemelee) so the project was canceled and is currently transitioning to become it&amp;rsquo;s own IP iv)&amp;ldquo;The Witches House&amp;rdquo;, &amp;ldquo;Mad Father&amp;rdquo;, &amp;ldquo;Angels of Death&amp;rdquo;, three game whos devs are to my knowledge completely unrelated yet all had the same idea of bringing a resident evil style pure horror game into the earthbound-like RPG-Maker format
Lastly &amp;ldquo;Omori&amp;rdquo; - despite my friends initial opinion of it earned - 200 000$ on kickstarter and was not only delivired to critically acclaim but is now being ported to non-PC consoles. It is by all measures a huge success. By todays poinbt in time earthbound-likes have seized to be niche whatsoever, they are a full industry and a pillar of gaming and for every obscure bland direct Earthbound copy there is a wholely original (if strongly inspired) game that enriched many peoples lifes. One title however stands far above them all though, so ubiquitously succesfull that many see it as the culmination and potential implosion of the earthbound-like genre.
7 Undertale Arguably one of the most influential indie games of all time and the supposed rags-to-riches story of one guy (along with his artist) making a game he cared about in his bedroom and selling over a million copies, Undertales reputation precedes it. As mentioned earlier in this piece the Developer of Undertale did not, in fact, came out of nowhere, he found his footing in the Earthbound romhack community and amassed a sizable audience there. That audience led him to try to get the snowball rolling on kickstarter and as lightning in a bottle it grew into an mixed-metaphor avalanche. Undertale is one of those things that by itself is so small but grew so big that fans had to extract every tiny element from it, every single character of even the lowest significance has a sub-fandom around them, the game blew up in the 2016 tumblr-scene so much that when Night in the Woods released I saw countless of reviews saying &#34;play this before the fandom ruins it like Undertale&#34;. I don&#39;t share the opinion that the reaction to Undertale in anyway colored the game and I&#39;m not really here to examine that. I&#39;m in a very priviledged position: I played Undertale completely out of context. I bought it the day it came out because it was on sale and I literally read no reviews or checked the internet before I finished it. It wasn&#39;t even intentional I just thought it&#39;s another obscure RPG-Maker (it was actually made in game maker) title that a few thousand people will play and as demonstrated in this piece I kinda like those games. So what was Undertale: the game, not Undertale: the phenomenon? Undertale is the quintessential earthbound-like, it&amp;rsquo;s creator literally went directly from working on Earthbound to working on it and the influence is obvious. Undertale features strategically shallow combat that is again spiced up not with funny status effects this time but with two Mario RPG-esque out-of-genre elements. The most obvious is the shoot-em-up mode, when you get attacked instead of just taking the damage you enter a small window where you control a heart dodging incoming projectiles. This is very much like Gradius or Touhou-games but I woudln&amp;rsquo;t say it tries to appeal to any of their audience. It&amp;rsquo;s less about being a compelling shoot-em-up (and it really doesn&amp;rsquo;t have to be) but about not being a bog-standard jRPG. The shump window is Undertales equivalent to the action-commands and it even used the infamously hard nature of those shmup games as a way to create one of the most iconic bossfights in all of gaming. While the shmup section happens when you&amp;rsquo;re taking damage, when attacking Undertale takes inspiration from two other genres to advance earthbounds battle system: The first and arguably blandest one is that if you attack to deal damage you get a typical action-command, press the button at the right time to do more damage. The more other intersting one is that each battle has an entirely seperate mode in which enemies aren&amp;rsquo;t meant to be defeated by killing them but by having social interactions with them. I don&amp;rsquo;t consider the moral component of this very notable, it plainly displays the two options of killing them or sparing them in literal text on screen at all times; what the spare system offers however is a situation in which players are actively incentivized to form para-social relationship with these characters. Much like Earthbound, each chapter of Undertale is based around a certain character and as each chapter culminates in a bossfight with them that also means it culminates in a chance to get to know that chapters characters and the game runs with this, you can visit their homes, go on dates, become friends, etc.. I think this is why Undertale spawned such a dedicated fanbase, taking the mechanics of the game on it&amp;rsquo;s own merits it already literally tasks players to become a fan of Undertales characters as gameplay so of course that will result in players doing that in real life.
Undertale came out in 2015 which means that right now in 2021 we&amp;rsquo;re well past the 2-3 year development cycle of most videogames. There are and will be many games inspired specifically by Undertale and I&amp;rsquo;d like to examine what this means for the earthbound-like genre in general. Undertale was the moment where the genre exploded, LISA was succesful but did not have the reality warping success of Undertale so we&amp;rsquo;re presented with a problem: There will be games inspired by Undertale who&amp;rsquo;s devs have not played LISA or any of the games that preceeded it simply because there&amp;rsquo;s just so many more people who got into Undertale (some at a young age) exclusively. As stated above Undertale does exhibit most of the properties of earthbound-likes but I think it&amp;rsquo;s more appropriate to say it exhibits the properties of Earthbound specifically. Undertale isn&amp;rsquo;t an exploration of someones psyche, it isn&amp;rsquo;t particularly surreal or high-concept, it&amp;rsquo;s very child-friendly, has no disturbing themes or content and does a very good job at appealing to a mainstream audience much unlike most of the games (except Mario RPG) discussed here. None of these things make Undertale a worse game, I&amp;rsquo;d like to explicitely state here tha Undertale is, in fact, quite good but it means that with Undertales success the earthbound-like genre branched.
We&amp;rsquo;re now in a situation where we have games or other works of art that are &amp;lt;href=&amp;quot;https://indreams.me/search/results/?term=undertale&amp;rdquo; target=&amp;rdquo;_blank&amp;quot;&amp;gt;Undertale-clones, games that, just like Undertale did with Earthbound, exclusively try to follow Undertale and only Undertale and this may contribute to somewhat of an erasure of the genres history. We&amp;rsquo;re already feeling that shift manifesting in less than positive ways but most importantly it asks the question: are games only based on Undertale earthbound-likes? I can&amp;rsquo;t answer that question but what I can do is remind you of what my friend said when seeing a screenshot of Omori. I think what my friend meant when saying Omori looks generic is that it looks like it&amp;rsquo;s made in reaction to Undertale. Being just another Earthbound has grown stale, that&amp;rsquo;s why the genre kept evolving and adding new exciting things and games that followed the course of that lineage have managed to not be stale at all because of this. Omori is part of that lineage, it isn&amp;rsquo;t stale but people who don&amp;rsquo;t know what this somewhat obscure genre of games are capable of won&amp;rsquo;t realize this and disregard it as just another product of the explosion that was started by Undertale.
Omori is a good game, Undertale is a good game, many of Undertale successors are good games but what may not necessarily be true is that many of Undertales successors used the insight gained over decades by titles of the earthbound-genre to their full potential. Let&amp;rsquo;s do our inventory one last time:
 quirky and light-hearted world
 surreal, character-introduction-driven plot (each chapter corresponds to one party member)
 very literal plot (there are no big themes or metaphors tackled and everything that happens happens diagetically)
 shallow jRPG battle system with great presentation
 mechanics are added for flavor, not for depth
 plot suddenly shifts to very dark subject matter during the final moments
 inserts an out-of-genre system to make battle more mechanically interesting
  Hopefully you see my point.
8 An actual Definition Throughout this article we have not only looked at important games that got us to the point where we are today by inspiring each other and subsequent games but I have tried to systematically highlight things each game passed on, their &amp;ldquo;genes&amp;rdquo; so to say. If we pool these together in a set and try to find the least common denominator we can assume that this is as close to a proper definition of what I call &amp;ldquo;earthbound-likes&amp;rdquo; as we can get. So let&amp;rsquo;s do that:
A game can be considered an earthbound-like if it has enough of the following features:
a) &amp;hellip; it usually has 2D, sprite-based, low resolution, pixelart graphics
b) It is build around two distinct states, in the &amp;ldquo;overworld&amp;rdquo; the player moves through a diagetic world, usually on 2 axis, solves light puzzles and interacts with other characters by walking into them or for example talking to them or triggering the second state, &amp;ldquo;battle&amp;rdquo;, in which the player engages in an abstract representation of what diagetically is a violent fight or another form of confrontation suchs as social interactions. Note that the &amp;ldquo;battle&amp;rdquo; state can be absent completey in certain games (like Yume Nikki), the overworld part however is not optional.
c) &amp;hellip; the movement on the overworld is mechanically basic
d) &amp;hellip; there is very little strategic depth in the battle system d1) However these systems are enriched by an arbitrary out-of-genre mechanic from shoot-em-ups, fighting games, dating sims, etc. d2) The vanilla version of the battle system (the battle system without the out-of-genre gameplay) is similar to those found in early Final Fantasy, D&amp;amp;D, Dragon Quest or RPG-Maker games
e) &amp;hellip; it is directly or indirectly iterating on Mother 2: Earthbound and Notable RPG-Maker games, meaning it&amp;hellip; e1) has a world that is considered unusual compared to traditional fantasy jRPGs e2) Is very character driven, chapters or entire plothreads may revolve around meeting or introducing certain character instead instead of a driving external goal or item e3) have a quirky world that is both comedic and dramatic in writing e4) intends to be an emotionally impactful and intensely personal5 experience e5) has moments of overt horror and dark or surprisingly disturbing subject matter
f) &amp;hellip; it focuses a lot of it&amp;rsquo;s presentation on static illustration and music rather than elaborate animation or high fidelity graphics
g) &amp;hellip; it is made by a small team of either indie developers or if made by a first-party studio, developers that are usually not established for their work in other AAA successes g1) many of these developer found their footing in the Earthbound-, RPG-maker- or Undertale-community
There we have, case closed, right? You might have noticed that I used a lot of law- and math-definition tricks by using weasel words and not commiting to stating if all or even how many of these properties have to be present in a game for it to be designated as an &amp;ldquo;earthbound-like&amp;rdquo;. I consider this valid, genres aren&amp;rsquo;t a label intended to classify a piece and put it in a box and only that box, they are meant to give context to the pieces place in history and it&amp;rsquo;s inspirations and relation to other similar art. Nontheless, every single game mentioned in this article either fullfills all of these properties or came out before a time where one of it&amp;rsquo;s to-be sucessoors would introduce the missing property into the genre and have all subsequent games inherit it. There wasn&amp;rsquo;t out-of-genre mechanics before Mario RPG (there probably were games I don&amp;rsquo;t know about but you get my point), before LISA Earhtbound-likes weren&amp;rsquo;t willing to dabble with themes this explicitely disturbing (again there probably were some), you get the idea. As the genre evolved and more amazing games were made those games in turn inspired future games and the list of properties grew and grew to a point where we are today. I consider the above definition not only well-established but incredibly specific. Specific enough that some (including me) may think it should be enough to call it it&amp;rsquo;s own genre.
Either way my intention isn&amp;rsquo;t to establish a new term, what is actually important to me is for young audience members who for example got really into Undertale but none of the games mentioned here to maybe go back and try them. Don&amp;rsquo;t just emulate Earthbound and think you have the full context of even just Undertale. I&amp;rsquo;m positive anyone who liked Undertale will like the Mario RPG games, if they&amp;rsquo;re old enough I would also warmly recommend LISA. After all the actual point of putting a label on a game is to make it so, if someone likes that game, they know they should try out games in the same genre. Hopefully this article made that a little bit easier to do and I again recommend every single game mentioned by name here. I do consider all of these games at least worth your time to try and I am hopeful that in the future developers who did try them will bring us even more games that advance the genre just like all these classics.
Thank you for reading,
C.
(if you got this far I would appreciate feedback in the comment section below)
 Footnotes: [1] Properties include: punishing and methodical gameplay, a rechargable health item that has a limited amount of uses per death (estus flask), strategically placed save points (bonfires), loosing a currency upon death but being given the chance to visit the place of demise and collect most or all of the lost currency, etc.
[2] Games with turn-based combat and 2d sprite-based graphics such as the Final Fantasy (1, 2, 3, 6, 7, 8) and Dragon Quest (1, 2, 3, 4, 5) Series, Chrono Trigger, Suikoden, Tales of Phantasia, Xenogears and most recently Octopath Traveler. Unlike my list of earthbound-likes I don&#39;t think I have to elaborate more on what I mean when I say &#34;traditional jRPG&#34;
[3] If I&#39;m not mistaken only the end-game round robot that explodes when you kill it is an enemy that was designed specifically with the rolling health in mind
[4] Note how the style of room layout and trapezoid perspective is very similar to Earthbound:   [5] &#34;personal&#34; in terms of a para-social relationship between the player and their perceived idea of an entity that is either diagetically present inside the game (such as Alphys in Undertale) or their idea of the dev themself (such as the creator of Yume Nikki, LISA, &#34;Temmy&#34; from Undertale) even if that persona has no actual relation to the real-life person that developed the game will not attribute this quote to anyone directly but a general sentiment about why jRPGs have become niche is that players   Comments (via Twitter)  I wrote an article about the #gamedesign of a seies of games that started with #EarthBound and #yumenikki and culminated in #undertale. Maybe someone here finds it interesting:https://t.co/6bRQAlBzlq#gamedev
&amp;mdash; Clems (@not_birb) March 7, 2021  ]]></content:encoded>
    </item>
  </channel>
</rss>
